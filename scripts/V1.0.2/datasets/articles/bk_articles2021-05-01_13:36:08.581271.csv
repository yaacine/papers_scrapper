,title,pub_year,author,volume,journal,number,pages,publisher,abstract,filled,author_pub_id,num_citations,pub_url,cites_id,citedby_url,gsrank,author_id,eprint_url,got_citations,got_author_ids,author_ids
1274599,Deep learning,2015,Yann LeCun and Yoshua Bengio and Geoffrey Hinton,521,,7553,436-444,Nature Publishing Group,Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition. visual object recognition. object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images. video. speech and audio. whereas recurrent nets have shone light on sequential data such as text and speech.,True,kukA0LcAAAAJ:naSTrk-c4S8C,38098,https://www.nature.com/articles/nature14539,5362332738201102290,/scholar?cites=5362332738201102290,,,https://s3.us-east-2.amazonaws.com/hkg-website-assets/static/pages/files/DeepLearning.pdf,0,0,0
1274600,Gradient-based learning applied to document recognition,1998,Yann LeCun and Léon Bottou and Yoshua Bengio and Patrick Haffner,86,,11,2278-2324,Ieee,Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture. gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns. such as handwritten characters. with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks. which are specifically designed to deal with the variability of 2D shapes. are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction. segmentation recognition. and language modeling. A new learning paradigm. called graph transformer networks (GTN). allows …,True,kukA0LcAAAAJ:u5HHmVD_uO8C,36052,https://ieeexplore.ieee.org/abstract/document/726791/,1909057046224785356,/scholar?cites=1909057046224785356,,,http://www.iro.umontreal.ca/~lisa/bib/pub_subject/finance/pointeurs/lecun-98.pdf,0,0,0
1274601,Generative adversarial networks,2014,Ian J Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio,,arXiv preprint arXiv:1406.2661,,,,We propose a new framework for estimating generative models via an adversarial process. in which we simultaneously train two models: a generative model G that captures the data distribution. and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D. a unique solution exists. with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons. the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.,True,kukA0LcAAAAJ:i8eIfGGcn98C,30386,https://arxiv.org/abs/1406.2661,8618380841735941249,/scholar?cites=8618380841735941249,,,https://arxiv.org/pdf/1406.2661.pdf%3Enh%C6%B0ng%C3%A2%E2%82%AC%20%20k%20qu%20kh%20nh%3E%20h%C3%83%C2%ACnh%20%C3%A1%C2%BA%C2%A3nh%20th%C3%A1%C2%BB%C2%B1c%20t%C3%A1%C2%BA%C2%BF.%20%20C%C3%83%C2%B3%20th%C3%A1%C2%BB%C6%92%20m%C3%A1%C2%BB%E2%84%A2t%20ng%C3%83%C2%A0y%20n%C3%83%C2%A0o%20%C3%84%E2%80%98%C3%83%C2%B3%20c%C3%83%C2%B4ng%20ngh%C3%A1%C2%BB%E2%80%A1%20nh%C3%86%C2%B0%20th%C3%A1%C2%BA%C2%BF%20n%C3%83%C2%A0y%20c%C3%83%C2%B3%20th%C3%A1%C2%BB%C6%92%20%C3%84%E2%80%98%C3%86%C2%B0%C3%A1%C2%BB%C2%A3c%20cung%20c%C3%A1%C2%BA%C2%A5p%20d%C3%86%C2%B0%C3%A1%C2%BB%E2%80%BAi%20d%C3%A1%C2%BA%C2%A1ng%20s%C3%A1%C2%BA%C2%A3n%20ph%C3%A1%C2%BA%C2%A9m%20ti%C3%83%C2%AAu%20d%C3%83%C2%B9ng%20ho%C3%A1%C2%BA%C2%B7c%20doanh%20nghi%C3%A1%C2%BB%E2%80%A1p%20%C3%84%E2%80%98%C3%A1%C2%BB%C6%92%20t%C3%A1%C2%BA%C2%A1o%20ra%20h%C3%83%C2%ACnh%20%C3%A1%C2%BA%C2%A3nh%20gi%C3%A1%C2%BB%E2%80%98ng%20nh%C3%86%C2%B0%20cu%C3%A1%C2%BB%E2%84%A2c%20s%C3%A1%C2%BB%E2%80%98ng%20theo%20y%C3%83%C2%AAu%20c%C3%A1%C2%BA%C2%A7u.%20%3C/a%3E%3C/p%3E%3C/div%3E%20%20%20%20%20%20%20%20%3C/div%3E%20%20%20%20%20%20%20%20%3Cfooter%3E%20%20%20%20%20%20%20%20%20%20%20%20%3C!--%20post%20pagination%20--%3E%20%20%20%20%20%20%20%20%20%20%20%20%3C!--%20review%20--%3E%20%20%20%20%20%20%20%20%20%20%20%20%3Cdiv%20class=,0,0,0
1274602,Deep learning,2016,Ian Goodfellow and Yoshua Bengio and Aaron Courville and Yoshua Bengio,1,,2,,MIT press,Kwang Gi Kim https://doi. org/10.4258/hir. 2016.22. 4.351 ing those who are beginning their careers in deep learning and artificial intelligence research. The other target audience consists of software engineers who may not have a background in machine learning or statistics but who nonetheless want to acquire this knowledge rapidly and begin using deep learning in their fields. Deep learning has already proven useful in many software disciplines. including computer vision. speech and audio processing. natural language processing. robotics. bioinformatics and chemistry. video games. search engines. online advertising and finance. This book has been organized into three parts so as to best accommodate a variety of readers. In Part I. the author intro duces basic mathematical tools and machine learning concepts. Part II describes the most established deep learning algorithms that are essentially solved technologies. Part III describes more speculative ideas that are widely believed to be important for future research in deep learning. In this book. certain areas assume that all readers have a computer science background. The authors assume familiarity with programming and a basic understanding of computational performance issues. complexity theory. introductory-level calculus and some of the terminology of graph theory.,True,kukA0LcAAAAJ:orDZ08hpP44C,25580,https://synapse.koreamed.org/upload/SynapseData/PDFData/1088HIR/hir-22-351.pdf,16766804411681372720,/scholar?cites=16766804411681372720,,,https://synapse.koreamed.org/upload/SynapseData/PDFData/1088HIR/hir-22-351.pdf,0,0,0
1274603,Neural machine translation by jointly learning to align and translate,2014,Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio,,arXiv preprint arXiv:1409.0473,,,,Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation. the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper. we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture. and propose to extend this by allowing a model to automatically (soft-) search for parts of a source sentence that are relevant to predicting a target word. without having to form these parts as a hard segment explicitly. With this new approach. we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore. qualitative analysis reveals that the (soft-) alignments found by the model agree well with our intuition.,True,kukA0LcAAAAJ:__bU50VfleQC,17666,https://arxiv.org/abs/1409.0473,9430221802571417838,/scholar?cites=9430221802571417838,,,https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel,0,0,0
1274604,Learning phrase representations using RNN encoder-decoder for statistical machine translation,2014,Kyunghyun Cho and Bart Van Merriënboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio,,arXiv preprint arXiv:1406.1078,,,,In this paper. we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation. and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively. we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.,True,kukA0LcAAAAJ:kJDgFkosVoMC,13671,https://arxiv.org/abs/1406.1078,9119975171114587835,/scholar?cites=9119975171114587835,,,https://arxiv.org/pdf/1406.1078.pdf?source=post_page---------------------------,0,0,0
1274605,Understanding the difficulty of training deep feedforward neural networks,2010,Xavier Glorot and Yoshua Bengio,,,,249-256,JMLR Workshop and Conference Proceedings,Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained. since then several algorithms have been shown to successfully train them. with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks. to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value. which can drive especially the top hidden layer into saturation. Surprisingly. we find that saturated units can move out of saturation by themselves. albeit slowly. and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally. we study how activations and gradients vary across layers and during training. with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations. we propose a new initialization scheme that brings substantially faster convergence.,True,kukA0LcAAAAJ:D_sINldO8mEC,12348,http://proceedings.mlr.press/v9/glorot10a,17889055433985220047,/scholar?cites=17889055433985220047,,,http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf,0,0,0
1274606,Learning deep architectures for AI,2009,Yoshua Bengio,,,,,Now Publishers Inc,Can machine learning deliver AI? Theoretical results. inspiration from the brain and cognition. as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (eg in vision. language. and other AI-level tasks). one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations. such as in neural nets with many hidden layers. graphical models with many levels of latent variables. or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction. defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task. but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006. following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures. yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures. explanations for their success are proposed and discussed. highlighting challenges and suggesting avenues for future explorations in this area.,True,kukA0LcAAAAJ:YsMSGLbcyi4C,9482,http://books.google.com/books?hl=en&lr=&id=cq5ewg7FniMC&oi=fnd&pg=PA1&dq=info:pYyS8T9g_kkJ:scholar.google.com&ots=KpfdSZmmJz&sig=RZRTfPZpPU3gRAlSD9_MFxa8lyc,5331804836605365413,/scholar?cites=5331804836605365413,,,https://dl.acm.org/doi/abs/10.1561/2200000006,0,0,0
1274607,Representation learning: A review and new perspectives,2013,Yoshua Bengio and Aaron Courville and Pascal Vincent,35,,8,1798-1828,IEEE,The success of machine learning algorithms generally depends on data representation. and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations. learning with generic priors can also be used. and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning. covering advances in probabilistic models. autoencoders. manifold learning. and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations. for computing representations (i.e.. inference). and the geometrical connections between representation …,True,kukA0LcAAAAJ:jFemdcug13IC,8850,https://ieeexplore.ieee.org/abstract/document/6472238/,559463397382443088,/scholar?cites=559463397382443088,,,https://arxiv.org/pdf/1206.5538),0,0,0
1274608,A Neural probabilistic language model,2003,Yoshua Bengio and Rejean Ducharme and Pascal Vincent,3,Journal of Machine Learning Research,,1137-1155,,A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences. expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function. showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models. and that the proposed approach allows to take advantage of longer contexts.,True,kukA0LcAAAAJ:qjMakFHDy7sC,7383,https://www.jmlr.org/papers/volume3/tmp/bengio03a.pdf,11929840984668291800,/scholar?cites=11929840984668291800,,,https://www.jmlr.org/papers/volume3/tmp/bengio03a.pdf,0,0,0
1274609,Show. attend and tell: Neural image caption generation with visual attention,2015,Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhudinov and Rich Zemel and Yoshua Bengio,,,,2048-2057,PMLR,Inspired by recent work in machine translation and object detection. we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k. Flickr30k and MS COCO.,True,kukA0LcAAAAJ:X0DADzN9RKwC,6942,http://proceedings.mlr.press/v37/xuc15,9471583366007765258,/scholar?cites=9471583366007765258,,,http://proceedings.mlr.press/v37/xuc15.pdf,0,0,0
1274610,Imagenet classification with deep convolutional neural networks,2012,Alex Krizhevsky and Ilya Sutskever and Geoffrey E Hinton,25,Advances in neural information processing systems,,1097-1105,,We trained a large. deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data. we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network. which has 60 million parameters and 650.000 neurons. consists of five convolutional layers. some of which are followed by max-pooling layers. and three fully-connected layers with a final 1000-way softmax. To make training faster. we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%. compared to 26.2% achieved by the second-best entry.,True,x04W_mMAAAAJ:UebtZRa9Y70C,85699,http://kr.nvidia.com/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf,2071317309766942398,/scholar?cites=2071317309766942398,,,http://kr.nvidia.com/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf,0,0,0
1274611,Tensorflow: Large-scale machine learning on heterogeneous distributed systems,2016,Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Józefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng,,arXiv preprint arXiv:1603.04467,,,,TensorFlow is an interface for expressing machine learning algorithms. and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems. ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms. including training and inference algorithms for deep neural network models. and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields. including speech recognition. computer vision. robotics. information retrieval. natural language processing. geographic information extraction. and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November. 2015 and are available at this http URL.,True,x04W_mMAAAAJ:4aZ_i-5WJEQC,28177,https://arxiv.org/abs/1603.04467,13312035063239472247,/scholar?cites=13312035063239472247,,,https://arxiv.org/pdf/1603.04467.pdf%20http://arxiv.org/abs/1603.04467,0,0,0
1274612,Dropout: a simple way to prevent neural networks from overfitting,2014,Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov,15,The journal of machine learning research,1,1929-1958,JMLR. org,Deep neural nets with a large number of parameters are very powerful machine learning systems. However. overfitting is a serious problem in such networks. Large networks are also slow to use. making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training. dropout samples from an exponential number of different “thinned” networks. At test time. it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision. speech recognition. document classification and computational biology. obtaining state-of-the-art results on many benchmark data sets.,True,x04W_mMAAAAJ:1yWc8FF-_SYC,27659,https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_campaign=buffer&utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com,17092600409158696067,/scholar?cites=17092600409158696067,,,https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_campaign=buffer&utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com,0,0,0
1274613,Distributed representations of words and phrases and their compositionality,2013,Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean,,arXiv preprint arXiv:1310.4546,,,,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example. the meanings of"" Canada"" and"" Air"" cannot be easily combined to obtain"" Air Canada"". Motivated by this example. we present a simple method for finding phrases in text. and show that learning good vector representations for millions of phrases is possible.",True,x04W_mMAAAAJ:kNdYIx-mwKoC,27086,https://arxiv.org/abs/1310.4546,2410615501856807729,/scholar?cites=2410615501856807729,,,"https://arxiv.org/pdf/1310.4546.pdf),",0,0,0
1274614,Sequence to sequence learning with neural networks,2014,Ilya Sutskever and Oriol Vinyals and Quoc V Le,,arXiv preprint arXiv:1409.3215,,,,Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available. they cannot be used to map sequences to sequences. In this paper. we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality. and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset. the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set. where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally. the LSTM did not have difficulty on long sentences. For comparison. a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system. its BLEU score increases to 36.5. which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally. we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly. because doing so introduced many short term dependencies between the source and the target sentence which made the optimization …,True,x04W_mMAAAAJ:-DxkuPiZhfEC,14712,https://arxiv.org/abs/1409.3215,13133880703797056141,/scholar?cites=13133880703797056141,,,https://arxiv.org/pdf/1409.3215,0,0,0
1274615,Mastering the game of Go with deep neural networks and tree search,2016,David Silver and Aja Huang and Chris J Maddison and Arthur Guez and Laurent Sifre and George Van Den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis,529,nature,7587,484-489,Nature Publishing Group,The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games. and reinforcement learning from games of self-play. Without any lookahead search. the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm. our program AlphaGo achieved a 99.8% winning rate against other Go programs. and …,True,x04W_mMAAAAJ:PyEswDtIyv0C,10122,https://www.nature.com/articles/nature16961?MRK_CMPG_SOURCE=sm_tw_pp,300412370207407505,/scholar?cites=300412370207407505,,,https://web.iitd.ac.in/~sumeet/Silver16.pdf,0,0,0
1274616,Intriguing properties of neural networks,2013,Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus,,arXiv preprint arXiv:1312.6199,,,,Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed. it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.First. we find that there is no distinction between individual high level units and random linear combinations of high level units. according to various methods of unit analysis. It suggests that it is the space. rather than the individual units. that contains of the semantic information in the high layers of neural networks.,True,x04W_mMAAAAJ:Zph67rFs4hoC,6953,https://arxiv.org/abs/1312.6199,2835128024326609853,/scholar?cites=2835128024326609853,,,https://arxiv.org/pdf/1312.6199.pdf?source=post_page---------------------------,0,0,0
1274617,Improving neural networks by preventing co-adaptation of feature detectors,2012,Geoffrey E Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R Salakhutdinov,,arXiv preprint arXiv:1207.0580,,,,"When a large feedforward neural network is trained on a small training set. it typically performs poorly on held-out test data. This"" overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead. each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random"" dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",True,x04W_mMAAAAJ:LkGwnXOMwfcC,6400,https://arxiv.org/abs/1207.0580,250385030036514850,/scholar?cites=250385030036514850,,,https://arxiv.org/pdf/1207.0580.pdf),0,0,0
1274618,On the importance of initialization and momentum in deep learning,2013,Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton,,,,1139-1147,PMLR,Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper. we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter. it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore. carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.,True,x04W_mMAAAAJ:MXK_kJrjxJIC,3497,http://proceedings.mlr.press/v28/sutskever13.html,7449004388220998591,/scholar?cites=7449004388220998591,,,http://proceedings.mlr.press/v28/sutskever13.pdf,0,0,0
1274619,Infogan: Interpretable representation learning by information maximizing generative adversarial nets,2016,Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel,,arXiv preprint arXiv:1606.03657,,,,This paper describes InfoGAN. an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently. and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically. InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset. pose from lighting of 3D rendered images. and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles. presence/absence of eyeglasses. and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.,True,x04W_mMAAAAJ:v6i8RKmR8ToC,2906,https://arxiv.org/abs/1606.03657,14881367722116467754,/scholar?cites=14881367722116467754,,,https://arxiv.org/pdf/1606.03657.pdf?source=post_page---------------------------,0,0,0
1274620,Improving language understanding by generative pre-training,2018,Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever,12,,,,,Natural language understanding comprises a wide range of diverse tasks such as textual entailment. question answering. semantic similarity assessment. and document classification. Although large unlabeled text corpora are abundant. labeled data for learning these specific tasks is scarce. making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text. followed by discriminative fine-tuning on each specific task. In contrast to previous approaches. we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task. significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance. we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test). 5.7% on question answering (RACE). and 1.5% on textual entailment (MultiNLI).,True,x04W_mMAAAAJ:zdjWy_NXXwUC,2012,https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf,11028265058903086746,/scholar?cites=11028265058903086746,,,https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf,0,0,0
1274621,Deep residual learning for image recognition,2016,Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun,,,,770-778,,Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs. instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize. and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations. we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions. where we also won the 1st places on the tasks of ImageNet detection. ImageNet localization. COCO detection. and COCO segmentation.,True,ALVSZAYAAAAJ:b0M2c_1WBrUC,76628,http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html,9281510746729853742,/scholar?cites=9281510746729853742,,,https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf,0,0,0
1274622,Faster r-cnn: Towards real-time object detection with region proposal networks,2015,Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun,,arXiv preprint arXiv:1506.01497,,,,State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks. exposing region proposal computation as a bottleneck. In this work. we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network. thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with'attention'mechanisms. the RPN component tells the unified network where to look. For the very deep VGG-16 model. our detection system has a frame rate of 5fps (including all steps) on a GPU. while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007. 2012. and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions. Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.,True,ALVSZAYAAAAJ:abG-DnoFyZgC,21058,https://arxiv.org/abs/1506.01497,16436232259506318906,/scholar?cites=16436232259506318906,,,https://arxiv.org/pdf/1506.01497.pdf;,0,0,0
1274623,Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,2015,Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun,,,,1026-1034,,Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work. we study rectifier neural networks for image classification from two aspects. First. we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second. we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization. we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet. 6.66%). To our knowledge. our result is the first to surpass the reported human-level performance (5.1%) on this dataset.,True,ALVSZAYAAAAJ:u_35RYKgDlwC,11499,http://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html,6243061688889140249,/scholar?cites=6243061688889140249,,,https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf,0,0,0
1274624,Faster R-CNN: towards real-time object detection with region proposal networks,2016,Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun,39,IEEE transactions on pattern analysis and machine intelligence,6,1137-1149,IEEE,State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks. exposing region proposal computation as a bottleneck. In this work. we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network. thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms. the RPN component tells the unified …,True,ALVSZAYAAAAJ:AXkvAH5U_nMC,8467,https://ieeexplore.ieee.org/abstract/document/7485869/,10104828340132865674,/scholar?cites=10104828340132865674,,,,0,0,0
1274625,Spatial pyramid pooling in deep convolutional networks for visual recognition,2015,Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun,37,IEEE transactions on pattern analysis and machine intelligence,9,1904-1916,IEEE,Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g.. 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work. we equip the networks with another pooling strategy. “spatial pyramid pooling”. to eliminate the above requirement. The new network structure. called SPP-net. can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages. SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset. we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets. SPP-net achieves state-of-the-art classification results using a single full-image …,True,ALVSZAYAAAAJ:ldfaerwXgEUC,6021,https://ieeexplore.ieee.org/abstract/document/7005506/,15950430662967917123,/scholar?cites=15950430662967917123,,,https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf,0,0,0
1274626,Identity mappings in deep residual networks,2016,Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun,,,,630-645,Springer. Cham,Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper. we analyze the propagation formulations behind the residual building blocks. which suggest that the forward and backward signals can be directly propagated from one block to any other block. when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit. which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100. and a 200-layer ResNet on ImageNet. Code is available at:                      https://github.com/KaimingHe/resnet-1k-layers                                        .,True,ALVSZAYAAAAJ:nb7KW1ujOQ8C,5455,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,14035416619237709781,/scholar?cites=14035416619237709781,,,https://arxiv.org/pdf/1603.05027.pdf).,0,0,0
1274627,Single image haze removal using dark channel prior,2010,Kaiming He and Jian Sun and Xiaoou Tang,33,IEEE transactions on pattern analysis and machine intelligence,12,2341-2353,IEEE,In this paper. we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model. we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover. a high-quality depth map can also be obtained as a byproduct of haze removal.,True,ALVSZAYAAAAJ:W7OEmFMy1HYC,5006,https://ieeexplore.ieee.org/abstract/document/5567108/,11452018541640230539,/scholar?cites=11452018541640230539,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.672.3815&rep=rep1&type=pdf,0,0,0
1274628,R-fcn: Object detection via region-based fully convolutional networks,2016,Jifeng Dai and Yi Li and Kaiming He and Jian Sun,,arXiv preprint arXiv:1605.06409,,,,We present region-based. fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times. our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal. we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones. such as the latest Residual Networks (ResNets). for object detection. We show competitive results on the PASCAL VOC datasets (eg. 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile. our result is achieved at a test-time speed of 170ms per image. 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL,True,ALVSZAYAAAAJ:KxtntwgDAa4C,3787,https://arxiv.org/abs/1605.06409,14880935744314366653,/scholar?cites=14880935744314366653,,,https://arxiv.org/pdf/1605.06409.pdf?fbclid=IwAR3CG5C1gZQehIVFuU_MYa5ah4SkAECWmyP17Ftu5VLdvAhh-EssMyLWdKM,0,0,0
1274629,Guided image filtering,2012,Kaiming He and Jian Sun and Xiaoou Tang,35,IEEE transactions on pattern analysis and machine intelligence,6,1397-1409,IEEE,In this paper. we propose a novel explicit image filter called guided filter. Derived from a local linear model. the guided filter computes the filtering output by considering the content of a guidance image. which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1]. but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output. enabling new filtering applications like dehazing and guided feathering. Moreover. the guided filter naturally has a fast and nonapproximate linear time algorithm. regardless of the kernel size and the intensity range. Currently. it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer …,True,ALVSZAYAAAAJ:1sJd4Hv_s6UC,3704,https://ieeexplore.ieee.org/abstract/document/6319316/,11575470123647071266,/scholar?cites=11575470123647071266,,,https://www.researchgate.net/profile/Xiaoou-Tang/publication/236228168_Guided_Image_Filtering/links/54e999640cf27a6de1103c64/Guided-Image-Filtering.pdf,0,0,0
1274630,Learning to detect a salient object,2007,Tie Liu and Jian Sun and Nan-Ning Zheng and Xiaoou Tang and Heung-Yeung Shum,,,,1-8,IEEE,In this paper. we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features. including multiscale contrast. center-surround histogram. and color spatial distribution. to describe a salient object locally. regionally. and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further. we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database. and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.,True,ALVSZAYAAAAJ:QIV2ME_5wuYC,2764,https://ieeexplore.ieee.org/abstract/document/5432215/,2328220750626159535,/scholar?cites=2328220750626159535,,,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.4387&rep=rep1&type=pdf,0,0,0
1274631,Shufflenet: An extremely efficient convolutional neural network for mobile devices,2018,Xiangyu Zhang and Xinyu Zhou and Mengxiao Lin and Jian Sun,,,,6848-6856,,We introduce an extremely computation-efficient CNN architecture named ShuffleNet. which is designed specially for mobile devices with very limited computing power (eg. 10-150 MFLOPs). The new architecture utilizes two new operations. pointwise group convolution and channel shuffle. to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures. eg lower top-1 error (absolute 7.8%) than recent MobileNet~ cite {howard2017mobilenets} on ImageNet classification task. under the computation budget of 40 MFLOPs. On an ARM-based mobile device. ShuffleNet achieves $ sim $13 $ imes $ actual speedup over AlexNet while maintaining comparable accuracy.,True,ALVSZAYAAAAJ:xtRiw3GOFMkC,2421,http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html,6469340744827368429,/scholar?cites=6469340744827368429,,,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf,0,0,0
1274632,Introduction to Information Retrieval,2008,Christopher D Manning and Prabhakar Raghavan and Hinrich Schütze,,,1,496,Cambridge: Cambridge University Press,,True,1zmDOdwAAAAJ:VqEBHV59gIcC,35004,http://scholar.google.com/scholar?cluster=16805160410401272073&hl=en&oi=scholarr,7205603893148316555,/scholar?cites=7205603893148316555,,,,0,0,0
1274633,Introduction to information retrieval,2008,Christopher D Manning and Prabhakar Raghavan and Hinrich Schütze,,,,,Cambridge University Press. Cambridge,,True,1zmDOdwAAAAJ:AYV8G9fXOwEC,22478,http://scholar.google.com/scholar?cluster=16805160410401272073&hl=en&oi=scholarr,7205603893148316555,/scholar?cites=7205603893148316555,,,,0,0,0
1274634,Introduction to Information Retrieval,2008,Christopher D Manning and Prabhakar Raghavan and Hinrich Schütze,,,,,,,True,1zmDOdwAAAAJ:PcID3NGdOhUC,20709,,7205603893148316555,/scholar?cites=7205603893148316555,,,,0,0,0
1274635,Introduction to Information Retrieval,2008,Christopher D Manning and Prabhakar Raghavan and Schütze,,,,,Cambridge. England: Cambridge University Press,,True,1zmDOdwAAAAJ:rqo51bwTRb4C,20695,,7205603893148316555,/scholar?cites=7205603893148316555,,,,0,0,0
1274636,Glove: Global vectors for word representation,2014,Jeffrey Pennington and Richard Socher and Christopher D Manning,,,,1532-1543,,Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix. rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure. as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.,True,1zmDOdwAAAAJ:WAzi4Gm8nLoC,20681,https://www.aclweb.org/anthology/D14-1162.pdf,15824805022753088965,/scholar?cites=15824805022753088965,,,https://www.aclweb.org/anthology/D14-1162.pdf,0,0,0
1274637,Foundations of statical natural language processing,1999,Christopher D Manning and H Schütze,,,,,MIT Press. Cambridge. MA,Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations. as well as detailed discussion of statistical methods. allowing students and researchers to construct their own implementations. The book covers collocation finding. word sense disambiguation. probabilistic parsing. information retrieval. and other applications.,True,1zmDOdwAAAAJ:52Op3ZsrznwC,14718,http://books.google.com/books?hl=en&lr=&id=3qnuDwAAQBAJ&oi=fnd&pg=PT12&dq=info:0hfgkn0S6ZAJ:scholar.google.com&ots=ysG4t3AuL1&sig=6OGMfFLT6Ihwl9KymiEo0jinKp4,10441897541579577298,/scholar?cites=10441897541579577298,,,,0,0,0
1274638,Foundations of Statistical Natural Language Processing,1999,Christopher Manning and Hinrich Schütze,,,,,Cambridge. MA: The MIT Press,Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations. as well as detailed discussion of statistical methods. allowing students and researchers to construct their own implementations. The book covers collocation finding. word sense disambiguation. probabilistic parsing. information retrieval. and other applications.,True,1zmDOdwAAAAJ:YI35Bd-2PncC,14551,http://books.google.com/books?hl=en&lr=&id=3qnuDwAAQBAJ&oi=fnd&pg=PT12&dq=info:0hfgkn0S6ZAJ:scholar.google.com&ots=ysG4t3AuMV&sig=7jffRskSNYwS7gpUoL_yOwpdEgM,10441897541579577298,/scholar?cites=10441897541579577298,,,,0,0,0
1274639,The Stanford CoreNLP Natural Language Processing Toolkit,2014,Christopher D Manning and Mihai Surdeanu and John Bauer and Jenny Finkel and Steven J Bethard and David McClosky,,,,,,We describe the design and use of the Stanford CoreNLP toolkit. an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used. both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple. approachable design. straightforward interfaces. the inclusion of robust and good quality analysis components. and not requiring use of a large amount of associated baggage.,True,1zmDOdwAAAAJ:MAUkC_7iAq8C,6323,https://www.aclweb.org/anthology/P14-5010.pdf,18438307512677181244,/scholar?cites=18438307512677181244,,,https://www.aclweb.org/anthology/P14-5010.pdf,0,0,0
1274640,Effective approaches to attention-based neural machine translation,2015,Minh-Thang Luong and Hieu Pham and Christopher D Manning,,,,1412-1421,arXiv preprint arXiv:1508.04025,An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However. there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention. we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points. an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.,True,1zmDOdwAAAAJ:scjTk0LcRdsC,5545,https://arxiv.org/abs/1508.04025,12347446836257434866,/scholar?cites=12347446836257434866,,,https://arxiv.org/pdf/1508.04025),0,0,0
1274641,Recursive deep models for semantic compositionality over a sentiment treebank,2013,Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher D Manning and Andrew Y Ng and Christopher Potts,,,,1631-1642,,Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this. we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215.154 phrases in the parse trees of 11.855 sentences and presents new challenges for sentiment compositionality. To address them. we introduce the Recursive Neural Tensor Network. When trained on the new treebank. this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%. an improvement of 9.7% over bag of features baselines. Lastly. it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.,True,1zmDOdwAAAAJ:Ehil0879vHcC,5044,https://www.aclweb.org/anthology/D13-1170.pdf,15964830791795922151,/scholar?cites=15964830791795922151,,,https://www.aclweb.org/anthology/D13-1170.pdf,0,0,0
1274642,Feature-rich part-of-speech tagging with a cyclic dependency network,2003,Kristina Toutanova and Dan Klein and Christopher D Manning and Yoram Singer,,,,252-259,,We present a new part-of-speech tagger that demonstrates the following ideas:(i) explicit use of both preceding and following tag contexts via a dependency network representation.(ii) broad use of lexical features. including jointly conditioning on multiple consecutive words.(iii) effective use of priors in conditional loglinear models. and (iv) fine-grained modeling of unknown word features. Using these ideas together. the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ. an error reduction of 4.4% on the best previous single automatically learned tagging result.,True,1zmDOdwAAAAJ:4DMP91E08xMC,3853,https://www.aclweb.org/anthology/N03-1033.pdf,15400239283834381573,/scholar?cites=15400239283834381573,,,https://www.aclweb.org/anthology/N03-1033.pdf,0,0,0
1274643,Latent dirichlet allocation,2003,David M Blei and Andrew Y Ng and Michael I Jordan,3,the Journal of machine Learning research,,993-1022,JMLR. org,We describe latent Dirichlet allocation (LDA). a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model. in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is. in turn. modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling. the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling. text classification. and collaborative filtering. comparing to a mixture of unigrams model and the probabilistic LSI model.,True,mG4imMEAAAAJ:IUKN3-7HHlwC,37325,https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8,17756175773309118945,/scholar?cites=17756175773309118945,,,https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8,0,0,0
1274644,On spectral clustering: Analysis and an algorithm,2001,Andrew Ng and Michael Jordan and Yair Weiss,14,Advances in neural information processing systems,,849-856,,Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second. many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper. we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory. we analyze the algorithm. and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.,True,mG4imMEAAAAJ:2KloaMYe4IUC,9600,https://papers.nips.cc/paper/2092-on-spectral-clusteringanalysis-and-an-algorithm.pdf,11592423042323210811,/scholar?cites=11592423042323210811,,,https://papers.nips.cc/paper/2092-on-spectral-clusteringanalysis-and-an-algorithm.pdf,0,0,0
1274645,ROS: an open-source Robot Operating System,2009,Morgan Quigley and Ken Conley and Brian Gerkey and Josh Faust and Tully Foote and Jeremy Leibs and Rob Wheeler and Andrew Y Ng,3,ICRA workshop on open source software,3.2,5,,This paper gives an overview of ROS. an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather. it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper. we discuss how ROS relates to existing robot software frameworks. and briefly overview some of the available application software which uses ROS.,True,mG4imMEAAAAJ:u-x6o8ySG0sC,8367,http://www.cim.mcgill.ca/~dudek/417/Papers/quigley-icra2009-ros.pdf,143767492575573826,/scholar?cites=143767492575573826,,,http://www.cim.mcgill.ca/~dudek/417/Papers/quigley-icra2009-ros.pdf,0,0,0
1274646,Recursive deep models for semantic compositionality over a sentiment treebank,2013,Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher D Manning and Andrew Y Ng and Christopher Potts,,,,1631-1642,,Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this. we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215.154 phrases in the parse trees of 11.855 sentences and presents new challenges for sentiment compositionality. To address them. we introduce the Recursive Neural Tensor Network. When trained on the new treebank. this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%. an improvement of 9.7% over bag of features baselines. Lastly. it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.,True,mG4imMEAAAAJ:_axFR9aDTf0C,5042,https://www.aclweb.org/anthology/D13-1170.pdf,15964830791795922151,/scholar?cites=15964830791795922151,,,https://www.aclweb.org/anthology/D13-1170.pdf,0,0,0
1274647,Rectifier nonlinearities improve neural network acoustic models,2013,Andrew L Maas and Awni Y Hannun and Andrew Y Ng,30,Proc. icml,1,3,,Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work. we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining. networks with rectifier nonlinearities produce 2% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally. we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.,True,mG4imMEAAAAJ:gsN89kCJA0AC,4541,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.1422&rep=rep1&type=pdf,13874212620286738798,/scholar?cites=13874212620286738798,,,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.1422&rep=rep1&type=pdf,0,0,0
1274648,Distance metric learning with application to clustering with side-information,2002,Eric P Xing and Andrew Y Ng and Michael I Jordan and Stuart Russell,15,,505–512,12,,Many algorithms rely critically on being given a good metric over their inputs. For instance. data can often be clustered in many “plausible” ways. and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user. the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics. it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance. we may ask them to provide examples. In this paper. we present an algorithm that. given examples of similar (and. if desired. dissimilar) pairs of points in ŹŅ. learns a distance metric over ŹŅ that respects these relationships. Our method is based on posing metric learning as a convex optimization problem. which allows us to give efficient. local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.,True,mG4imMEAAAAJ:W5xh706n7nkC,3478,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.1936&rep=rep1&type=pdf,6588442176337540489,/scholar?cites=6588442176337540489,,,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.1936&rep=rep1&type=pdf,0,0,0
1274649,Large scale distributed deep networks,2012,Jeffrey Dean and Greg S Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Quoc V Le and Mark Z Mao and Marc’Aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Andrew Y Ng,,,,,,Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper. we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework. we have developed two algorithms for large-scale distributed training:(i) Downpour SGD. an asynchronous stochastic gradient descent procedure supporting a large number of model replicas. and (ii) Sandblaster. a framework that supports a variety of distributed batch optimization procedures. including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature. and achieves state-of-the-art performance on ImageNet. a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks. the underlying algorithms are applicable to any gradient-based machine learning algorithm.,True,mG4imMEAAAAJ:VOx2b1Wkg3QC,3243,http://research.google/pubs/pub40565/,9220704513857531974,/scholar?cites=9220704513857531974,,,http://research.google/pubs/pub40565.pdf,0,0,0
1274650,Efficient sparse coding algorithms,2007,Honglak Lee and Alexis Battle and Rajat Raina and Andrew Y Ng,,,,801-808,,Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data. it discovers basis functions that capture higher-level features in the data. However. finding sparse codes remains a very difficult computational problem. In this paper. we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding. allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and. therefore. may provide a partial explanation for these two phenomena in V1 neurons.,True,mG4imMEAAAAJ:sNmaIFBj_lkC,3150,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.7544&rep=rep1&type=pdf,17369200716676083828,/scholar?cites=17369200716676083828,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.7544&rep=rep1&type=pdf,0,0,0
1274651,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,2009,Honglak Lee and Roger Grosse and Rajesh Ranganath and Andrew Y Ng,,,,609-616,,There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized. high-dimensional images remains a difficult problem. To address this problem. we present the convolutional deep belief network. a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling. a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features. such as object parts. from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down …,True,mG4imMEAAAAJ:VaXvl8Fpj5cC,2911,https://dl.acm.org/doi/abs/10.1145/1553374.1553453,14272331216913182998,/scholar?cites=14272331216913182998,,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.8314&rep=rep1&type=pdf,0,0,0
1274652,On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes,2002,Andrew Y Ng and Michael I Jordan,,,,841-848,,We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show. contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred. that there can often be two distinct regimes of performance as the training set size is increased. one in which each algorithm does better. This stems from the observation-which is borne out in repeated experiments-that while discriminative learning has lower asymptotic error. a generative classifier may also approach its (higher) asymptotic error much faster.,True,mG4imMEAAAAJ:T_ojBgVMvoEC,2767,http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classi%20fiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf,7998621100817377612,/scholar?cites=7998621100817377612,,,http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classi%20fiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf,0,0,0
1274653,Learning word vectors for sentiment analysis,2011,Andrew Maas and Raymond E Daly and Peter T Pham and Dan Huang and Andrew Y Ng and Christopher Potts,,,,142-150,,Unsupervised vector-based approaches to semantics can model rich lexical meanings. but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (eg star ratings). We evaluate the model using small. widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.,True,mG4imMEAAAAJ:cFHS6HbyZ2cC,2729,https://www.aclweb.org/anthology/P11-1015.pdf,5506141124337583723,/scholar?cites=5506141124337583723,,,https://www.aclweb.org/anthology/P11-1015.pdf,0,0,0
1274654,The ATLAS experiment at the CERN large hadron collider,2008,Georges Aad and E Abat and J Abdallah and AA Abdelalim and A Abdesselam and O Abdinov and BA Abi and M Abolins and H Abramowicz and E Acerbi and BS Acharya and R Achenbach and M Ackers and DL Adams and F Adamyan and TN Addy and M Aderholz and C Adorisio and P Adragna and M Aharrouche and SP Ahlen and F Ahles and A Ahmad and H Ahmed and G Aielli and PF Akesson and Torsten Åkesson and SM Alam and J Albert and S Albrand and M Aleksa and IN Aleksandrov and M Aleppo and F Alessandria and C Alexa and G Alexander and T Alexopoulos and G Alimonti and M Aliyev and PP Allport and SE Allwood-Spiers and A Aloisio and J Alonso and R Alves and MG Alviggi and K Amako and P Amaral and SP Amaral and G Ambrosini and G Ambrosio,3,Journal of Instrumentation,S08003,,IOP Publishing,"Aad. G; Bentvelsen. S; Bobbink. GJ; Bos. K; Boterenbrood. H; Brouwer. G; Buis. EJ; Buskop. JJF;
Colijn. AP; Dankers. R; Daum. C; de Boer. R; de Jong. P; Ennes. P; Gosselink. M; Groenstege.
H; Hart. RGG; Hartjes. F; Hendriks. PJ; Hessey. NP; Jansweijer. PPM; Kieft. G; Klous. S; Kluit. P;
Koffeman. E; Koutsman. A; Liebig. W; Limper. M; Linde. F; Luijckx. G; Massaro. G; Muijs. A; Peeters.
SJM; Reichold. A; Rewiersma. P; Rijpstra. M; Scholte. RC; Schuijlenburg. HW; Snuverink. J; van
der Graaf. H; van der Kraaij. E; Van Eijk. B; van Kesteren. Z; van Vulpen. I; Verkerke. W; Vermeulen.
JC; Vreeswijk. M; Werneke. P; Cakir. O; Ciftci. AK; Duran Yildiz. H; Sultanov. S; Turk Cakir. I; Yilmaz.
M; Aubert. B; Bazan. A; Beaugiraud. B; Bellachia. F; Berger. N; Blaising. JJ; Colas. J; Consonni.
M; Delebecque. P; Delsart. PA; Di Ciaccio. L; Dayot. N Dumont>; Elles. S; Ghez. Philippe>; Girard.
CG; Gouanere. M; Goy. C; Guillemin. T; Ionescu. G … 
",True,EZjSxgwAAAAJ:9yKSN-GCB0IC,15401,https://cds.cern.ch/record/1129811,4583933401305623738,/scholar?cites=4583933401305623738,,,https://cds.cern.ch/record/1129811,0,0,0
1274655,Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC,2012,Georges Aad and Tatevik Abajyan and B Abbott and J Abdallah and S Abdel Khalek and Ahmed Ali Abdelalim and R Aben and B Abi and M Abolins and OS AbouZeid and H Abramowicz and H Abreu and BS Acharya and L Adamczyk and DL Adams and TN Addy and J Adelman and S Adomeit and P Adragna and T Adye and S Aefsky and JA Aguilar-Saavedra and M Agustoni and M Aharrouche and SP Ahlen and F Ahles and A Ahmad and M Ahsan and G Aielli and T Akdogan and TPA Åkesson and G Akimoto and AV Akimov and MS Alam and MA Alam and J Albert and S Albrand and M Aleksa and IN Aleksandrov and F Alessandria and C Alexa and G Alexander and G Alexandre and T Alexopoulos and M Alhroob and M Aliev and G Alimonti and J Alison and BMM Allbrooke and PP Allport and SE Allwood-Spiers and J Almond and A Aloisio and R Alon and A Alonso and F Alonso and A Altheimer and B Alvarez Gonzalez and MG Alviggi and K Amako and C Amelung and VV Ammosov and SP Amor Dos Santos and A Amorim and N Amram and C Anastopoulos and LS Ancu and N Andari and T Andeen and CF Anders and G Anders and KJ Anderson and A Andreazza and V Andrei and M-L Andrieux and XS Anduaga and S Angelidakis and P Anger and A Angerami and F Anghinolfi and A Anisenkov and N Anjos and A Annovi and A Antonaki and M Antonelli and A Antonov and J Antos and F Anulli and M Aoki and S Aoun and L Aperio Bella and R Apolle and G Arabidze and I Aracena and Y Arai and ATH Arce and S Arfaoui and J-F Arguin and E Arik and M Arik and AJ Armbruster and O Arnaez and V Arnal and C Arnault and A Artamonov and G Artoni and D Arutinov and S Asai and S Ask and B Åsman and L Asquith and K Assamagan and A Astbury and M Atkinson and B Aubert and E Auge and K Augsten and M Aurousseau and G Avolio and R Avramidou and D Axen and G Azuelos and Y Azuma and MA Baak and G Baccaglioni and C Bacci and AM Bach and H Bachacou and K Bachas and M Backes and M Backhaus and J Backus Mayes and E Badescu and P Bagnaia and S Bahinipati and Y Bai and DC Bailey and T Bain and JT Baines and OK Baker and MD Baker and S Baker and P Balek and E Banas and P Banerjee and Sw Banerjee and D Banfi and A Bangert and V Bansal and HS Bansil,716,Physics Letters B,1,1-29,North-Holland,A search for the Standard Model Higgs boson in proton–proton collisions with the ATLAS detector at the LHC is presented. The datasets used correspond to integrated luminosities of approximately 4.8 fb− 1 collected at s= 7 TeV in 2011 and 5.8 fb− 1 at s= 8 TeV in 2012. Individual searches in the channels H→ Z Z (⁎)→ 4 ℓ. H→ γ γ and H→ W W (⁎)→ e ν μ ν in the 8 TeV data are combined with previously published results of searches for H→ Z Z (⁎). W W (⁎). b b¯ and τ+ τ− in the 7 TeV data and results from improved analyses of the H→ Z Z (⁎)→ 4 ℓ and H→ γ γ channels in the 7 TeV data. Clear evidence for the production of a neutral boson with a measured mass of 126.0±0.4 (stat)±0.4 (sys) GeV is presented. This observation. which has a significance of 5.9 standard deviations. corresponding to a background fluctuation probability of 1.7× 10− 9. is compatible with the production and decay of the Standard Model …,True,EZjSxgwAAAAJ:pNwUjlbgJN8C,10122,https://www.sciencedirect.com/science/article/pii/S037026931200857X,1650605866782918575,/scholar?cites=1650605866782918575,,,https://www.sciencedirect.com/science/article/pii/S037026931200857X,0,0,0
1274656,The ATLAS simulation infrastructure,2010,Georges Aad and B Abbott and J Abdallah and AA Abdelalim and Abdelmalek Abdesselam and B Abi and M Abolins and H Abramowicz and H Abreu and BS Acharya and DL Adams and TN Addy and J Adelman and C Adorisio and P Adragna and T Adye and S Aefsky and JA Aguilar-Saavedra and M Aharrouche and SP Ahlen and F Ahles and A Ahmad and H Ahmed and M Ahsan and G Aielli and T Akdogan and TPA Åkesson and G Akimoto and AV Akimov and A Aktas and MS Alam and MA Alam and S Albrand and M Aleksa and IN Aleksandrov and C Alexa and G Alexander and G Alexandre and T Alexopoulos and M Alhroob and M Aliev and G Alimonti and J Alison and M Aliyev and PP Allport and SE Allwood-Spiers and J Almond and A Aloisio and R Alon and A Alonso and MG Alviggi and K Amako and C Amelung and A Amorim and G Amoros and N Amram and C Anastopoulos and T Andeen and CF Anders and KJ Anderson and A Andreazza and V Andrei and XS Anduaga and A Angerami and F Anghinolfi and N Anjos and A Annovi and A Antonaki and M Antonelli and S Antonelli and J Antos and B Antunovic and F Anulli and S Aoun and G Arabidze and I Aracena and Y Arai and ATH Arce and JP Archambault and S Arfaoui and J-F Arguin and T Argyropoulos and M Arik and AJ Armbruster and O Arnaez and C Arnault and A Artamonov and D Arutinov and M Asai and S Asai and R Asfandiyarov and S Ask and B Åsman and D Asner and L Asquith and K Assamagan and A Astbury and A Astvatsatourov and G Atoian and B Auerbach and K Augsten and M Aurousseau and N Austin and G Avolio and R Avramidou and D Axen and C Ay and G Azuelos and Y Azuma and MA Baak and AM Bach and H Bachacou and K Bachas and M Backes and E Badescu and P Bagnaia and Y Bai and T Bain and JT Baines and OK Baker and MD Baker and S Baker and F Baltasar Dos Santos Pedrosa and E Banas and P Banerjee and S Banerjee and D Banfi and A Bangert and V Bansal and SP Baranov and S Baranov and A Barashkou and T Barber and EL Barberio and D Barberis and M Barbero and DY Bardin and T Barillari and M Barisonzi and T Barklow and N Barlow and BM Barnett and RM Barnett and A Baroncelli and AJ Barr and F Barreiro and J Barreiro Guimaraes da Costa and P Barrillon and R Bartoldus and D Bartsch,70,The European Physical Journal C,3,823-874,Springer-Verlag,The simulation software for the ATLAS Experiment at the Large Hadron Collider is being used for large-scale production of events on the LHC Computing Grid. This simulation requires many components. from the generators that simulate particle collisions. through packages simulating the response of the various detectors and triggers. All of these components come together under the ATLAS simulation infrastructure. In this paper. that infrastructure is discussed. including that supporting the detector description. interfacing the event generation. and combining the GEANT4 simulation of the response of the individual detectors. Also described are the tools allowing the software validation. performance testing. and the validation of the simulated output against known physics processes.,True,EZjSxgwAAAAJ:2osOgNQ5qMEC,6343,https://link.springer.com/article/10.1140/epjc/s10052-010-1429-9,10614936945083796742,/scholar?cites=10614936945083796742,,,https://link.springer.com/article/10.1140/epjc/s10052-010-1429-9,0,0,0
1274657,Asymptotic formulae for likelihood-based tests of new physics,2011,Glen Cowan and Kyle Cranmer and Eilam Gross and Ofer Vitells,71,The European Physical Journal C,2,1-19,Springer-Verlag,We describe likelihood-based statistical tests for use in high energy physics for the discovery of new phenomena and for construction of confidence intervals on model parameters. We focus on the properties of the test procedures that allow one to account for systematic uncertainties. Explicit formulae for the asymptotic distributions of test statistics are derived using results of Wilks and Wald. We motivate and justify the use of a representative data set. called the “Asimov data set”. which provides a simple method to obtain the median experimental sensitivity of a search or measurement as well as fluctuations about this expectation.,True,EZjSxgwAAAAJ:eQOLeE2rZwMC,6332,https://link.springer.com/article/10.1140/epjc/s10052-011-1554-0,15686529226357257581,/scholar?cites=15686529226357257581,,,https://link.springer.com/content/pdf/10.1140/epjc/s10052-011-1554-0.pdf,0,0,0
1274658,Luminosity Determination in Pp Collisions at S√ S= 8 TeV Using the ATLAS Detector at the LHC,2016,Masahiro Morii and SK Chan and B Clark and Paolo Giromini and Melissa Franklin and John Huth and Victoria Ippolito and Tomo Lazovich and D Lopez Mateos and Charlotte Rogan,,The European Physical Journal C,,,,The luminosity determination for the ATLAS detector at the LHC during pp collisions at s√= 8 TeV in 2012 is presented. The evaluation of the luminosity scale is performed using several luminometers. and comparisons between these luminosity detectors are made to assess the accuracy. consistency and long-term stability of the results. A luminosity uncertainty of δℒ/ℒ = ±1.9% is obtained for the 22.7 fb-1 of pp collision data delivered to ATLAS at s√= 8 TeV in 2012.,True,EZjSxgwAAAAJ:6UW509z96lEC,5394,https://dash.harvard.edu/handle/1/42659223,13028960326374976316,/scholar?cites=13028960326374976316,,,https://dash.harvard.edu/bitstream/handle/1/42659223/Aaboud2016_Article_LuminosityDeterminationInPpCol.pdf?sequence=1,0,0,0
1274659,Jet energy measurement with the ATLAS detector in proton-proton collisions at s√= 7 TeV,2013,Adam Barton and Guennadi Borissov and Eva Bouhova-Thacker and Timothy Brodbeck and James Catmore and Alexandre Chilingarov and Ruth Davidson and Lee de Mora and Harald Fox and Robert Henderson and Gareth Hughes and Roger William Lewis Jones and Vakhtang Kartvelishvili and Robin Long and Peter Love and Peter Ratoff and Maria Smizanska and James Walder,73,European Physical Journal C: Particles and Fields,3,,,The jet energy scale and its systematic uncertainty are determined for jets measured with the ATLAS detector at the LHC in proton-proton collision data at a centre-of-mass energy of s√=7 TeV corresponding to an integrated luminosity of 38 pb−1. Jets are reconstructed with the anti-k t algorithm with distance parameters R=0.4 or R=0.6. Jet energy and angle corrections are determined from Monte Carlo simulations to calibrate jets with transverse momenta p T≥20 GeV and pseudorapidities |η|<4.5. The jet energy systematic uncertainty is estimated using the single isolated hadron response measured in situ and in test-beams. exploiting the transverse momentum balance between central and forward jets in events with dijet topologies and studying systematic variations in Monte Carlo simulations. The jet energy uncertainty is less than 2.5 % in the central calorimeter region (|η|<0.8) for jets with 60≤p T<800 GeV. and is maximally 14 % for p T<30 GeV in the most forward region 3.2≤|η|<4.5. The jet energy is validated for jet transverse momenta up to 1 TeV to the level of a few percent using several in situ techniques by comparing a well-known reference such as the recoiling photon p T. the sum of the transverse momenta of tracks associated to the jet. or a system of low-p T jets recoiling against a high-p T jet. More sophisticated jet calibration schemes are presented based on calorimeter cell energy density weighting or hadronic properties of jets. aiming for an improved jet energy resolution and a reduced flavour dependence of the jet response. The systematic uncertainty of the jet energy determined from a combination of in situ techniques is …,True,EZjSxgwAAAAJ:IEPzPICaOQQC,4461,https://eprints.lancs.ac.uk/id/eprint/67906/,2742424474329200311,/scholar?cites=2742424474329200311,,,https://eprints.lancs.ac.uk/id/eprint/67906/1/art_3A10.1140_2Fepjc_2Fs10052_013_2304_2.pdf,0,0,0
1274660,The ALICE experiment at the CERN LHC,2008,Kenneth Aamodt and A Abrahantes Quintana and R Achenbach and S Acounis and D Adamová and C Adler and M Aggarwal and F Agnese and G Aglieri Rinella and Z Ahammed and A Ahmad and N Ahmad and S Ahmad and A Akindinov and P Akishin and D Aleksandrov and B Alessandro and R Alfaro and G Alfarone and A Alici and J Alme and T Alt and S Altinpinar and W Amend and C Andrei and Yves Andres and A Andronic and G Anelli and M Anfreville and V Angelov and A Anzo and C Anson and T Anticić and V Antonenko and D Antonczyk and F Antinori and S Antinori and P Antonioli and Laurent Aphecetche and H Appelshäuser and V Aprodu and M Arba and S Arcelli and A Argentieri and N Armesto and R Arnaldi and A Arefiev and I Arsene and A Asryan and A Augustinus and TC Awes and J Äysto and M Danish Azmi and S Bablock and A Badalà and SK Badyal and J Baechler and S Bagnasco and R Bailhache and R Bala and A Baldisseri and A Baldit and J Bán and R Barbera and PL Barberis and JM Barbet and G Barnäfoldi and V Barret and J Bartke and D Bartos and M Basile and V Basmanov and N Bastid and Guillaume Batigne and B Batyunya and J Baudot and Cédric Baumann and I Bearden and B Becker and J Belikov and R Bellwied and E Belmont-Moreno and A Belogianni and S Belyaev and A Benato and JL Beney and L Benhabib and F Benotto and Stefania Beole and I Berceanu and A Bercuci and E Berdermann and Y Berdnikov and C Bernard and R Berny and JD Berst and H Bertelsen and L Betev and A Bhasin and P Baskar and A Bhati and N Bianchi and J Bielčik and J Bielčiková and L Bimbot and G Blanchard and F Blanco and D Blau and C Blume and S Blyth and M Boccioli and A Bogdanov and H Bøggild and M Bogolyubsky and L Boldizsár and M Bombara and C Bombonati and M Bondila and D Bonnet and V Bonvicini and H Borel and F Borotto and V Borshchov and Y Bortoli and O Borysov and S Bose and L Bosisio and M Botje and S Böttger and G Bourdaud and O Bourrion and S Bouvier and A Braem and M Braun and P Braun-Munzinger and L Bravina and M Bregant and G Bruckner and R Brun and E Bruna and O Brunasso and GE Bruno and D Bucher and V Budilov and D Budnikov and H Buesching and P Buncic and M Burns and S Burachas and O Busch,3,,08,S08002,IOP Publishing,ALICE (A Large Ion Collider Experiment) is a general-purpose. heavy-ion detector at the CERN LHC which focuses on QCD. the strong-interaction sector of the Standard Model. It is designed to address the physics of strongly interacting matter and the quark-gluon plasma at extreme values of energy density and temperature in nucleus-nucleus collisions. Besides running with Pb ions. the physics programme includes collisions with lighter ions. lower energy running and dedicated proton-nucleus runs. ALICE will also take data with proton beams at the top LHC energy to collect reference data for the heavy-ion programme and to address several QCD topics for which ALICE is complementary to the other LHC detectors. The ALICE detector has been built by a collaboration including currently over 1000 physicists and engineers from 105 Institutes in 30 countries. Its overall dimensions are 16× 16× 26 m3 with a total …,True,EZjSxgwAAAAJ:-Re68m-yPREC,4399,https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08002/meta,15254134629373290316,/scholar?cites=15254134629373290316,,,https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08002/meta,0,0,0
1274661,Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data,2012,Georges Aad and B Abbott and J Abdallah and AA Abdelalim and A Abdesselam and O Abdinov and B Abi and M Abolins and H Abramowicz and H Abreu and E Acerbi and BS Acharya and DL Adams and TN Addy and J Adelman and M Aderholz and S Adomeit and P Adragna and T Adye and S Aefsky and JA Aguilar-Saavedra and M Aharrouche and SP Ahlen and F Ahles and A Ahmad and M Ahsan and G Aielli and T Akdogan and TPA Åkesson and G Akimoto and AV Akimov and A Akiyama and MS Alam and MA Alam and J Albert and S Albrand and M Aleksa and IN Aleksandrov and F Alessandria and C Alexa and G Alexander and G Alexandre and T Alexopoulos and M Alhroob and M Aliev and G Alimonti and J Alison and M Aliyev and PP Allport and SE Allwood-Spiers and J Almond and A Aloisio and R Alon and A Alonso and MG Alviggi and K Amako and P Amaral and C Amelung and VV Ammosov and A Amorim and G Amorós and N Amram and C Anastopoulos and LS Ancu and N Andari and T Andeen and CF Anders and G Anders and KJ Anderson and A Andreazza and V Andrei and ML Andrieux and XS Anduaga and A Angerami and F Anghinolfi and N Anjos and A Annovi and A Antonaki and M Antonelli and A Antonov and J Antos and F Anulli and S Aoun and L Aperio Bella and R Apolle and G Arabidze and I Aracena and Y Arai and ATH Arce and JP Archambault and S Arfaoui and JF Arguin and E Arik and M Arik and AJ Armbruster and O Arnaez and C Arnault and A Artamonov and G Artoni and D Arutinov and S Asai and R Asfandiyarov and S Ask and B Åsman and L Asquith and K Assamagan and A Astbury and A Astvatsatourov and G Atoian and B Aubert and B Auerbach and E Auge and K Augsten and M Aurousseau and N Austin and G Avolio and R Avramidou and D Axen and C Ay and G Azuelos and Y Azuma and MA Baak and G Baccaglioni and C Bacci and AM Bach and H Bachacou and K Bachas and G Bachy and M Backes and M Backhaus and E Badescu and P Bagnaia and S Bahinipati and Y Bai and DC Bailey and T Bain and JT Baines and OK Baker and MD Baker and S Baker and E Banas and P Banerjee and Sw Banerjee and D Banfi and A Bangert and V Bansal and HS Bansil and L Barak and SP Baranov and A Barashkou,72,The European Physical Journal C,3,1-46,Springer-Verlag,Detailed measurements of the electron performance of the ATLAS detector at the LHC are reported. using decays of the Z. W and J/ψ particles. Data collected in 2010 at\(\sqrt {s}= 7\mbox {~ TeV}\) are used. corresponding to an integrated luminosity of almost 40 pb− 1. The inter-alignment of the inner detector and the electromagnetic calorimeter. the determination of the electron energy scale and resolution. and the performance in terms of response uniformity and linearity are discussed. The electron identification. reconstruction and trigger efficiencies. as well as the charge misidentification probability. are also presented.,True,EZjSxgwAAAAJ:_FxGoFyzp5QC,4055,https://link.springer.com/article/10.1140/epjc/s10052-012-1909-1?error=cookies_not_supported&error=cookies_not_supported&code=46e5f81d-328d-49d1-9bf4-20abd8d1d4f3&code=bcf98441-7f60-427a-a793-33233f214686,1762831469858059875,/scholar?cites=1762831469858059875,,,https://link.springer.com/article/10.1140/epjc/s10052-012-1909-1?error=cookies_not_supported&error=cookies_not_supported&code=46e5f81d-328d-49d1-9bf4-20abd8d1d4f3&code=bcf98441-7f60-427a-a793-33233f214686,0,0,0
1274662,Performance of the ATLAS Trigger System in 2010,2012,Georges Aad and B Abbott and J Abdallah and AA Abdelalim and A Abdesselam and O Abdinov and B Abi and M Abolins and H Abramowicz and H Abreu and E Acerbi and BS Acharya and DL Adams and TN Addy and J Adelman and M Aderholz and S Adomeit and P Adragna and T Adye and S Aefsky and JA Aguilar-Saavedra and M Aharrouche and SP Ahlen and F Ahles and A Ahmad and M Ahsan and G Aielli and T Akdogan and TPA Åkesson and G Akimoto and AV Akimov and A Akiyama and MS Alam and MA Alam and S Albrand and M Aleksa and IN Aleksandrov and F Alessandria and C Alexa and G Alexander and G Alexandre and T Alexopoulos and M Alhroob and M Aliev and G Alimonti and J Alison and M Aliyev and PP Allport and SE Allwood-Spiers and J Almond and A Aloisio and R Alon and A Alonso and MG Alviggi and K Amako and P Amaral and C Amelung and VV Ammosov and A Amorim and G Amorós and N Amram and C Anastopoulos and N Andari and T Andeen and CF Anders and KJ Anderson and A Andreazza and V Andrei and ML Andrieux and XS Anduaga and A Angerami and F Anghinolfi and N Anjos and A Annovi and A Antonaki and M Antonelli and S Antonelli and A Antonov and J Antos and F Anulli and S Aoun and L Aperio Bella and R Apolle and G Arabidze and I Aracena and Y Arai and ATH Arce and JP Archambault and S Arfaoui and JF Arguin and E Arik and M Arik and AJ Armbruster and O Arnaez and C Arnault and A Artamonov and G Artoni and D Arutinov and S Asai and R Asfandiyarov and S Ask and B Åsman and L Asquith and K Assamagan and A Astbury and A Astvatsatourov and G Atoian and B Aubert and B Auerbach and E Auge and K Augsten and M Aurousseau and N Austin and G Avolio and R Avramidou and D Axen and Cano Ay and G Azuelos and Y Azuma and MA Baak and G Baccaglioni and C Bacci and AM Bach and H Bachacou and K Bachas and G Bachy and M Backes and M Backhaus and E Badescu and P Bagnaia and S Bahinipati and Y Bai and DC Bailey and T Bain and JT Baines and OK Baker and MD Baker and S Baker and F Baltasar Dos Santos Pedrosa and E Banas and P Banerjee and Sw Banerjee and D Banfi and A Bangert and V Bansal and HS Bansil and L Barak and SP Baranov and A Barashkou and A Barbaro Galtieri,72,The European Physical Journal C,1,1-61,Springer-Verlag,Proton–proton collisions at  TeV and heavy ion collisions at  TeV were produced by the LHC and recorded using the ATLAS experiment’s trigger system in 2010. The LHC is designed with a maximum bunch crossing rate of 40 MHz and the ATLAS trigger system is designed to record approximately 200 of these per second. The trigger system selects events by rapidly identifying signatures of muon. electron. photon. tau lepton. jet. and B meson candidates. as well as using global event signatures. such as missing transverse energy. An overview of the ATLAS trigger system. the evolution of the system during 2010 and the performance of the trigger system components and selections based on the 2010 collision data are shown. A brief outline of plans for the trigger system in 2011 is presented.,True,EZjSxgwAAAAJ:fEOibwPWpKIC,3971,https://link.springer.com/article/10.1140/epjc/s10052-011-1849-1,12841760951602738711,/scholar?cites=12841760951602738711,,,https://link.springer.com/article/10.1140/epjc/s10052-011-1849-1,0,0,0
1274663,Muon recontruction performance of the ATLAS detector in proton–proton colliion data at $$\qrt {} $$= 13 TeV,2016,Georges Aad and B Abbott and J Abdallah and B Abeloos and R Aben and M Abolins and OS AbouZeid and NL Abraham and H Abramowicz and H Abreu and R Abreu and Y Abulaiti and BS Acharya and L Adamczyk and DL Adams and J Adelman and S Adomeit and T Adye and AA Affolder and T Agatonovic-Jovin and JA Aguilar-Saavedra and SP Ahlen and F Ahmadov and G Aielli and H Akerstedt and TPA Åkesson and AV Akimov and GL Alberghi and J Albert and S Albrand and MJ Alconada Verzini and M Aleksa and IN Aleksandrov and C Alexa and G Alexander and T Alexopoulos and M Alhroob and M Aliev and G Alimonti and J Alison and SP Alkire and BMM Allbrooke and BW Allen and PP Allport and A Aloisio and A Alonso and F Alonso and C Alpigiani and M Alstaty and B Alvarez Gonzalez and D Álvarez Piqueras and MG Alviggi and BT Amadio and K Amako and Y Amaral Coutinho and C Amelung and D Amidei and SP Amor Dos Santos and A Amorim and S Amoroso and G Amundsen and C Anastopoulos and LS Ancu and N Andari and T Andeen and CF Anders and G Anders and JK Anders and KJ Anderson and A Andreazza and V Andrei and S Angelidakis and I Angelozzi and P Anger and A Angerami and F Anghinolfi and AV Anisenkov and N Anjos and A Annovi and M Antonelli and A Antonov and J Antos and F Anulli and M Aoki and L Aperio Bella and G Arabidze and Y Arai and JP Araque and ATH Arce and FA Arduh and JF Arguin and S Argyropoulos and M Arik and AJ Armbruster and LJ Armitage and O Arnaez and H Arnold and M Arratia and O Arslan and A Artamonov and G Artoni and S Artz and S Asai and N Asbah and A Ashkenazi and B Åsman and L Asquith and K Assamagan and R Astalos and M Atkinson and NB Atlay and K Augsten and G Avolio and B Axen and MK Ayoub and G Azuelos and MA Baak and AE Baas and MJ Baca and H Bachacou and K Bachas and M Backes and M Backhaus and P Bagiacchi and P Bagnaia and Y Bai and JT Baines and OK Baker and EM Baldin and P Balek and T Balestri and F Balli and WK Balunas and E Banas and Sw Banerjee and AAE Bannoura and L Barak and EL Barberio and D Barberis and M Barbero and T Barillari and T Barklow and N Barlow and SL Barnes and BM Barnett and RM Barnett and Z Barnovska and A Baroncelli and G Barone and AJ Barr,76,The European Physical Journal C,5,292,Springer Berlin Heidelberg,This article documents the performance of the ATLAS muon identification and reconstruction using the LHC dataset recorded at  TeV in 2015. Using a large sample of  and  decays from 3.2 fb of pp collision data. measurements of the reconstruction efficiency. as well as of the momentum scale and resolution. are presented and compared to Monte Carlo simulations. The reconstruction efficiency is measured to be close to  over most of the covered phase space ( and  GeV). The isolation efficiency varies between 93 and  depending on the selection applied and on the momentum of the muon. Both efficiencies are well reproduced in simulation. In the central region of the detector. the momentum resolution is measured to be  () for muons from  () decays. and the momentum scale is known with an uncertainty of . In the region . the  resolution …,True,EZjSxgwAAAAJ:H1FpnsQ9v50C,3970,https://link.springer.com/article/10.1140/epjc/s10052-016-4120-y,7961152465028107010,/scholar?cites=7961152465028107010,,,https://link.springer.com/article/10.1140/epjc/s10052-016-4120-y,0,0,0
1274664,Luminosity Determination in Pp Collisions at S√ S= 8 TeV Using the ATLAS Detector at the LHC,2016,Masahiro Morii and SK Chan and B Clark and Paolo Giromini and Melissa Franklin and John Huth and Victoria Ippolito and Tomo Lazovich and D Lopez Mateos and Charlotte Rogan,,The European Physical Journal C,,,,The luminosity determination for the ATLAS detector at the LHC during pp collisions at s√= 8 TeV in 2012 is presented. The evaluation of the luminosity scale is performed using several luminometers. and comparisons between these luminosity detectors are made to assess the accuracy. consistency and long-term stability of the results. A luminosity uncertainty of δℒ/ℒ = ±1.9% is obtained for the 22.7 fb-1 of pp collision data delivered to ATLAS at s√= 8 TeV in 2012.,True,EZjSxgwAAAAJ:nS6_t4fuB3kC,3957,https://dash.harvard.edu/handle/1/42659223,13028960326374976316,/scholar?cites=13028960326374976316,,,https://dash.harvard.edu/bitstream/handle/1/42659223/Aaboud2016_Article_LuminosityDeterminationInPpCol.pdf?sequence=1,0,0,0
