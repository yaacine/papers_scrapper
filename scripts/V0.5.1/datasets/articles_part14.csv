id,abstract,author,cites,cites_id,journal,number,pages,publisher,title,url,volume,year,citation_link,id_citations
14000,"A comprehensive introduction to Support Vector Machines and related kernel methods. In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs---kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",Bernhard Schölkopf and Alexander J. Smola,19731,3391028632449519147,,,,MIT press,"Learning with kernels: support vector machines, regularization, optimization, and beyond",http://books.google.com/books?hl=en&lr=&id=y8ORL3DWt4sC&oi=fnd&pg=PR13&dq=info:Kw5VJJNaDy8J:scholar.google.com&ots=bMsX7xO4GB&sig=pLb3dsnH5pD8Z6UueDAcSGHlhO0,,2002,/scholar?cites=3391028632449519147,DZ-fHPgAAAAJ:HIFyuExEbWQC
14001,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.",Alex J Smola and Bernhard Schölkopf,10612,12887921014177981629,Statistics and computing,3,199-222,Kluwer Academic Publishers,A tutorial on support vector regression,https://link.springer.com/article/10.1023/B:STCO.0000035301.49549.88,14,2004,/scholar?cites=12887921014177981629,DZ-fHPgAAAAJ:mNrWkgRL2YcC
14002,"A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map—for instance, the space of all possible five-pixel products in 16 × 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.",Bernhard Schölkopf and Alexander Smola and Klaus-Robert Müller,9114,9700976993142487787,Neural computation,5,1299-1319,MIT Press,Nonlinear component analysis as a kernel eigenvalue problem,https://www.mitpressjournals.org/doi/abs/10.1162/089976698300017467,10,1998,/scholar?cites=9700976993142487787,DZ-fHPgAAAAJ:4vMrXwiscB8C
14003,,Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,5518,16758580020291664053,,,,MIT Press,Semi-supervised learning,http://scholar.google.com/scholar?cluster=17990066851581961929&hl=en&oi=scholarr,,2006,/scholar?cites=16758580020291664053,DZ-fHPgAAAAJ:njNX1Zl8qLMC
14004,This book addresses some theoretical aspects of semisupervised learning (SSL). The book is organized as a collection of different contributions of authors who are experts on this topic. The objectives of this book are to present a large overview of the SSL methods and to classify these methods into four classes that correspond to the first four main parts of the book (this would include generative models; low-density separation methods; graph-based methods; and algorithms). The last two parts are devoted to applications and perspectives of SSL. The book responds to its major objectives and could serve as a basis for an intermediate level graduate course on SSL. It may also serve as a useful self study and reference source for practicing engineers.,Olivier Chapelle and Bernhard Scholkopf and Alexander Zien,5496,16758580020291664053,IEEE Transactions on Neural Networks,3,542-542,IEEE,"Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]",https://ieeexplore.ieee.org/abstract/document/4787647/,20,2009,/scholar?cites=16758580020291664053,DZ-fHPgAAAAJ:Dg-NvihI0yoC
14005,This book addresses some theoretical aspects of semisupervised learning (SSL). The book is organized as a collection of different contributions of authors who are experts on this topic. The objectives of this book are to present a large overview of the SSL methods and to classify these methods into four classes that correspond to the first four main parts of the book (this would include generative models; low-density separation methods; graph-based methods; and algorithms). The last two parts are devoted to applications and perspectives of SSL. The book responds to its major objectives and could serve as a basis for an intermediate level graduate course on SSL. It may also serve as a useful self study and reference source for practicing engineers.,Olivier Chapelle and Bernhard Scholkopf and Alexander Zien,5496,16758580020291664053,IEEE Transactions on Neural Networks,3,542-542,IEEE,"Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]",https://ieeexplore.ieee.org/abstract/document/4787647/,20,2009,/scholar?cites=16758580020291664053,DZ-fHPgAAAAJ:dz48QCY6SmAC
14006,"Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a “simple” subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1.We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm.The algorithm is a natural extension of the support vector …",B Scholkopf and John C Platt and John Shawe-Taylor and Alex J Smola and Robert C Williamson,5030,2150063179952253227,,,,,Estimating the support of a high-dimensional distribution,https://www.mitpressjournals.org/doi/abs/10.1162/089976601750264965,,1999,/scholar?cites=2150063179952253227,DZ-fHPgAAAAJ:ftAh3kAbccMC
14007,"Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a “simple” subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1.We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm.The algorithm is a natural extension of the support vector …",Bernhard Schölkopf and John C Platt and John Shawe-Taylor and Alex J Smola and Robert C Williamson,5014,2150063179952253227,Neural computation,7,1443-1471,MIT Press,Estimating the support of a high-dimensional distribution,https://www.mitpressjournals.org/doi/abs/10.1162/089976601750264965,13,2001,/scholar?cites=2150063179952253227,DZ-fHPgAAAAJ:cWzG1nlazyYC
14008,"This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.",K-R Muller and Sebastian Mika and Gunnar Ratsch and Koji Tsuda and Bernhard Scholkopf,4377,9190225381421992912,IEEE Transactions on Neural Networks,2,181-201,IEEE,An introduction to kernel-based learning algorithms,https://ieeexplore.ieee.org/abstract/document/914517/,12,2001,/scholar?cites=9190225381421992912,DZ-fHPgAAAAJ:-mN3Mh-tlDkC
14009,"We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.",Dengyong Zhou and Olivier Bousquet and Thomas Lal and Jason Weston and Bernhard Schölkopf,4077,17365098896282161457,Advances in neural information processing systems,,321-328,,Learning with local and global consistency,https://proceedings.neurips.cc/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf,16,2003,/scholar?cites=17365098896282161457,DZ-fHPgAAAAJ:bKqednn6t2AC
14010,A non-linear classification technique based on Fisher's discriminant is proposed. The main ingredient is the kernel trick which allows the efficient computation of Fisher discriminant in feature space. The linear classification in feature space corresponds to a (powerful) non-linear decision function in input space. Large scale simulations demonstrate the competitiveness of our approach.,S. Mika and G. Rätsch and J. Weston and B. Schölkopf and K. Müller,3342,1313558773224216398,Neural networks for signal processing IX,,,,Fisher discriminant analysis with kernels,https://ieeexplore.ieee.org/abstract/document/788121/,,1999,/scholar?cites=1313558773224216398,DZ-fHPgAAAAJ:yxmsSjX2EkcC
14011,"My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a …",Marti A.  Hearst and Susan T Dumais and Edgar Osuna and John Platt and Bernhard Scholkopf,3126,14311272233115654791,IEEE Intelligent Systems and their applications,4,18-28,IEEE,Support vector machines,https://ieeexplore.ieee.org/abstract/document/708428/,13,1998,/scholar?cites=14311272233115654791,DZ-fHPgAAAAJ:8-aJy9WCPRoC
14012,"We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter ν lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter ε in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of ν, and report experimental results.",Bernhard Schölkopf and Alex J Smola and Robert C Williamson and Peter L Bartlett,3078,13374472696241235570,Neural computation,5,1207-1245,MIT Press,New support vector algorithms,https://www.mitpressjournals.org/doi/abs/10.1162/089976600300015565,12,2000,/scholar?cites=13374472696241235570,DZ-fHPgAAAAJ:HtS1dXgVpQUC
14013,,B Schèolkopf and Christopher JC Burges and Alexander J Smola,2512,18300208669942520672,,,,"Cambridge, Mass., MIT Press",Advances in kernel methods support vector learning,http://scholar.google.com/scholar?cluster=4609465241915590273&hl=en&oi=scholarr,,1999,/scholar?cites=18300208669942520672,DZ-fHPgAAAAJ:Hje4yfxQICoC
14014,"Regulatory regions of plant genes tend to be more compact than those of animal genes, but the complement of transcription factors encoded in plant genomes is as large or larger than that found in those of animals 1. Plants therefore provide an opportunity to study how transcriptional programs control multicellular development. We analyzed global gene expression during development of the reference plant Arabidopsis thaliana in samples covering many stages, from embryogenesis to senescence, and diverse organs. Here, we provide a first analysis of this data set, which is part of the AtGenExpress expression atlas. We observed that the expression levels of transcription factor genes and signal transduction components are similar to those of metabolic genes. Examining the expression patterns of large gene families, we found that they are often more similar than would be expected by chance, indicating that many …",Markus Schmid and Timothy S Davison and Stefan R Henz and Utz J Pape and Monika Demar and Martin Vingron and Bernhard Schölkopf and Detlef Weigel and Jan U Lohmann,2469,3148765912271437214,Nature genetics,5,501-506,Nature Publishing Group,A gene expression map of Arabidopsis thaliana development,https://www.nature.com/articles/ng1543,37,2005,/scholar?cites=3148765912271437214,DZ-fHPgAAAAJ:yFnVuubrUp4C
14015,,B. Schölkopf and A. Burges and C.J.C.: Smola,2420,18300208669942520672,,,,The MIT press,Advances in kernel methods: support vector learning,http://scholar.google.com/scholar?cluster=17859504360602324505&hl=en&oi=scholarr,,1999,/scholar?cites=18300208669942520672,DZ-fHPgAAAAJ:CRzUtm-VnGAC
14016,"A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.",Bernhard Schölkopf and Alexander Smola and Klaus-Robert Müller,2398,7391796663221950842,,,583-588,"Springer, Berlin, Heidelberg",Kernel principal component analysis,https://link.springer.com/chapter/10.1007/BFb0020217,,1997,/scholar?cites=7391796663221950842,DZ-fHPgAAAAJ:GiYFt9mpioMC
14017,"We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian …",Arthur Gretton and Karsten M Borgwardt and Malte J Rasch and Bernhard Schölkopf and Alexander Smola,1992,7596088848261177517,The Journal of Machine Learning Research,1,723-773,JMLR. org,A kernel two-sample test,https://dl.acm.org/doi/abs/10.5555/2188385.2188410,13,2012,/scholar?cites=7596088848261177517,DZ-fHPgAAAAJ:__bU50VfleQC
14018,"We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.",Thomas Hofmann and Bernhard Schölkopf and Alexander J Smola,1647,2645494119122438178,Annals of Statistics,3,1171-1220,Institute of Mathematical Statistics,Kernel methods in machine learning,https://www.jstor.org/stable/25464664,36,2008,/scholar?cites=2645494119122438178,DZ-fHPgAAAAJ:Bg7qf7VwUHIC
14019,"While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al.(2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox",Sören Sonnenburg and Gunnar Rätsch and Christin Schäfer and Bernhard Schölkopf,1612,14198664504155024956,Journal of Machine Learning Research,Jul,1531-1565,,Large scale multiple kernel learning,http://www.jmlr.org/papers/v7/sonnenburg06a.html,7,2006,/scholar?cites=14198664504155024956,DZ-fHPgAAAAJ:3htObqc8RwsC
14020,"This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel …",Bernhard Schölkopf and Sebastian Mika and Chris JC Burges and Philipp Knirsch and Klaus-Robert Müller and Gunnar Rätsch and Alexander J Smola,1521,4852333378212130772,IEEE transactions on neural networks,5,1000-1017,,Input space versus feature space in kernel-based methods,https://ieeexplore.ieee.org/abstract/document/788641/,10,1999,/scholar?cites=4852333378212130772,DZ-fHPgAAAAJ:fFSKOagxvKUC
14021,"Wahba’s classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.",Bernhard Schölkopf and Ralf Herbrich and Alex J Smola,1518,17158073457253017483,,,416-426,"Springer, Berlin, Heidelberg",A generalized representer theorem,https://link.springer.com/chapter/10.1007/3-540-44581-1_27,,2001,/scholar?cites=17158073457253017483,DZ-fHPgAAAAJ:OP4eGU-M3BUC
14022,"The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine …",Bernhard Scholkopf and Kah-Kay Sung and Christopher JC Burges and Federico Girosi and Partha Niyogi and Tomaso Poggio and Vladimir Vapnik,1515,3178624415921949792,IEEE transactions on Signal Processing,11,2758-2765,IEEE,Comparing support vector machines with Gaussian kernels to radial basis function classifiers,https://ieeexplore.ieee.org/abstract/document/650102/,45,1997,/scholar?cites=3178624415921949792,DZ-fHPgAAAAJ:Br1UauaknNIC
14023,"Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a"" simple"" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified l/between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.",Bernhard Schölkopf and Robert C Williamson and Alex Smola and John Shawe-Taylor and John Platt,1464,13152476582289052321,Advances in neural information processing systems,,582-588,,Support vector method for novelty detection,https://proceedings.neurips.cc/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf,12,1999,/scholar?cites=13152476582289052321,DZ-fHPgAAAAJ:GFxP56DSvIMC
14024,"We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to first recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.",Jiayuan Huang and Arthur Gretton and Karsten Borgwardt and Bernhard Schölkopf and Alex Smola,1340,1928999243361978237,Advances in neural information processing systems,,601-608,,Correcting sample selection bias by unlabeled data,http://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf,19,2006,/scholar?cites=1928999243361978237,DZ-fHPgAAAAJ:6bLC7aUMtPcC
14025,"We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O (m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist.",Arthur Gretton and Karsten Borgwardt and Malte Rasch and Bernhard Schölkopf and Alex Smola,1245,9776960475929275623,Advances in neural information processing systems,,513-520,,A kernel method for the two-sample-problem,https://papers.nips.cc/paper/2006/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf,19,2006,/scholar?cites=9776960475929275623,DZ-fHPgAAAAJ:1paMEeroeoQC
14026,"Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be considered as a natural generalization of linear principal component analysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA. This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data.1 peA and Feature Spaces",Sebastian Mika and Bernhard Schölkopf and Alex J Smola and Klaus-Robert Müller and Matthias Scholz and Gunnar Rätsch,1196,12203924972449408144,Advances in neural information processing systems,1,536-542,,Kernel PCA and de-noising in feature spaces,https://papers.nips.cc/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf,11,1999,/scholar?cites=12203924972449408144,DZ-fHPgAAAAJ:yMeIxYmEMEAC
14027,Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%.,K Müller and Alex Smola and Gunnar Rätsch and Bernhard Schölkopf and Jens Kohlmorgen and Vladimir Vapnik,1196,17579038858708908201,Artificial Neural Networks—ICANN'97,,999-1004,Springer Berlin/Heidelberg,Predicting time series with support vector machines,https://link.springer.com/chapter/10.1007/BFb0020283,,1997,/scholar?cites=17579038858708908201,DZ-fHPgAAAAJ:ODE9OILHJdcC
14028,An efficient optimization method called ‘Teaching–Learning-Based Optimization (TLBO)’ is proposed in this paper for large scale non-linear optimization problems for finding the global solutions. The proposed method is based on the effect of the influence of a teacher on the output of learners in a class. The basic philosophy of the method is explained in detail. The effectiveness of the method is tested on many benchmark problems with different characteristics and the results are compared with other population based methods.,R Venkata Rao and Vimal J Savsani and DP Vakharia,1144,16673987251972871293,Information sciences,1,1-15,Elsevier,Teaching–learning-based optimization: an optimization method for continuous non-linear large scale problems,https://www.sciencedirect.com/science/article/pii/S0020025511004191,183,2012,/scholar?cites=16673987251972871293,DZ-fHPgAAAAJ:Wut8az6viKkC
14029,"We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is …",Arthur Gretton and Olivier Bousquet and Alex Smola and Bernhard Schölkopf,1080,10222004050350750215,,,63-77,"Springer, Berlin, Heidelberg",Measuring statistical dependence with Hilbert-Schmidt norms,https://link.springer.com/chapter/10.1007/11564089_7,,2005,/scholar?cites=10222004050350750215,DZ-fHPgAAAAJ:artPoR2Yc-kC
14030,"We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classification on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs.",Dengyong Zhou and Jiayuan Huang and Bernhard Scholkopf,920,10874791726090672907,Advances in Neural Information Processing Systems,,1601,MIT; 1998,"Learning with hypergraphs: Clustering, classification, and embedding",https://proceedings.neurips.cc/paper/2006/file/dff8e9c2ac33381546d96deea9922999-Paper.pdf,19,2007,/scholar?cites=10874791726090672907,DZ-fHPgAAAAJ:-7ulzOJl1JYC
14031,"We explore the use of the so-called zero-norm of the parameters of linear models in learning. Minimization of such a quantity has many uses in a machine learning context: for variable or feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero-norm. The method boils down to implementing a simple modification of vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we investigate which aid our discussion include variable and feature selection on biological microarray data, and multicategory classification.",Jason Weston and André Elisseeff and Bernhard Schölkopf and Mike Tipping,901,5474343528298158917,Journal of machine learning research,Mar,1439-1461,,Use of the zero-norm with linear models and kernel methods,http://www.jmlr.org/papers/v3/weston03a.html,3,2003,/scholar?cites=5474343528298158917,DZ-fHPgAAAAJ:a3BOlSfXSfwC
14032,"Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology. Following three introductory chapters an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.",Bernhard Schölkopf and Koji Tsuda and Jean-Philippe Vert,857,15953664400274791448,,,,The MIT press,Kernel methods in computational biology,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1791801,,2004,/scholar?cites=15953664400274791448,DZ-fHPgAAAAJ:F9fV5C73w3QC
14033,"Support Vector (SV) Machines combine several techniques from statistics, machine learning and neural networks. One of the most important ingredients are kernels, ie the concept of transforming linear algorithms into nonlinear ones via a map into feature spaces. The present work focuses on the following issues:",Alex J Smola and Bernhard Schölkopf,842,10372793773687401003,,,,GMD-Forschungszentrum Informationstechnik,Learning with kernels,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.4783&rep=rep1&type=pdf,4,1998,/scholar?cites=10372793773687401003,DZ-fHPgAAAAJ:r_AWSJRzSzQC
14034,,Bernhard Schölkopf and Chris Burges and Vladimir Vapnik,828,119733947393039294,,,,,Extracting support data for a given task,,,1995,/scholar?cites=119733947393039294,DZ-fHPgAAAAJ:OR75R8vi5nAC
14035,"The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method.",Dengyong Zhou and Jason Weston and Arthur Gretton and Olivier Bousquet and Bernhard Schölkopf,826,12196774102373358753,Advances in neural information processing systems,,169-176,,Ranking on data manifolds,http://papers.nips.cc/paper/2447-ranking-on-data-manifolds.pdf,16,2003,/scholar?cites=12196774102373358753,DZ-fHPgAAAAJ:ZzlSgRqYykMC
14036,"In this paper a correspondence is derived between regularization operators used in regularization networks and support vector kernels. We prove that the Green's Functions associated with regularization operators are suitable support vector kernels with equivalent regularization properties. Moreover, the paper provides an analysis of currently used support vector kernels in the view of regularization theory and corresponding operators associated with the classes of both polynomial kernels and translation invariant kernels. The latter are also analyzed on periodical domains. As a by-product we show that a large number of radial basis functions, namely conditionally positive definite functions, may be used as support vector kernels.",Alex J Smola and Bernhard Schölkopf and Klaus-Robert Müller,797,17129903253021544713,Neural networks,4,637-649,Pergamon,The connection between regularization operators and support vector kernels,https://www.sciencedirect.com/science/article/pii/S089360809800032X,11,1998,/scholar?cites=17129903253021544713,DZ-fHPgAAAAJ:KbBQZpvPDL4C
14037,"The genomes of individuals from the same species vary in sequence as a result of different evolutionary processes. To examine the patterns of, and the forces shaping, sequence variation in Arabidopsis thaliana, we performed high-density array resequencing of 20 diverse strains (accessions). More than 1 million nonredundant single-nucleotide polymorphisms (SNPs) were identified at moderate false discovery rates (FDRs), and ∼4% of the genome was identified as being highly dissimilar or deleted relative to the reference genome sequence. Patterns of polymorphism are highly nonrandom among gene families, with genes mediating interaction with the biotic environment having exceptional polymorphism levels. At the chromosomal scale, regional variation in polymorphism was readily apparent. A scan for recent selective sweeps revealed several candidate regions, including a notable example in which almost …",Richard M Clark and Gabriele Schweikert and Christopher Toomajian and Stephan Ossowski and Georg Zeller and Paul Shinn and Norman Warthmann and Tina T Hu and Glenn Fu and David A Hinds and Huaming Chen and Kelly A Frazer and Daniel H Huson and Bernhard Schölkopf and Magnus Nordborg and Gunnar Rätsch and Joseph R Ecker and Detlef Weigel,796,3218709964637422278,science,5836,338-342,American Association for the Advancement of Science,Common sequence polymorphisms shaping genetic diversity in Arabidopsis thaliana,https://science.sciencemag.org/content/317/5836/338.abstract,317,2007,/scholar?cites=3218709964637422278,DZ-fHPgAAAAJ:nZcligLrVowC
14038,"In kernel based methods such as Regularization Networks large datasets pose signi-cant problems since the number of basis functions required for an optimal solution equals the number of samples. We present a sparse greedy approximation technique to construct a compressed representation of the design matrix. Experimental results are given and connections to Kernel-PCA, Sparse Kernel Feature Analysis, and Matching Pursuit are pointed out. 1. Introduction Many recent advances in machine learning such as Support Vector Machines [Vapnik, 1995], Regularization Networks [Girosi et al., 1995], or Gaussian Processes [Williams, 1998] are based on kernel methods. Given an m-sample f (x 1; y 1);:::;(xm; ym) g of patterns xi 2 X and target values yi 2 Y these algorithms minimize the regularized risk functional min f2H R reg [f]= 1 mm X i= 1 c (xi; yi; f (xi))+ 2 kfk 2 H:(1) Here H denotes a reproducing kernel Hilbert space (RKHS)[Aronszajn, 1950],...",Alex J Smola and Bernhard Schölkopf,783,12782513022884144793,,,,,Sparse greedy matrix approximation for machine learning,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.3153,,2000,/scholar?cites=12782513022884144793,DZ-fHPgAAAAJ:5LPpnTyo_ycC
14039,"In kernel based methods such as Regularization Networks large datasets pose signi-cant problems since the number of basis functions required for an optimal solution equals the number of samples. We present a sparse greedy approximation technique to construct a compressed representation of the design matrix. Experimental results are given and connections to Kernel-PCA, Sparse Kernel Feature Analysis, and Matching Pursuit are pointed out. 1. Introduction Many recent advances in machine learning such as Support Vector Machines [Vapnik, 1995], Regularization Networks [Girosi et al., 1995], or Gaussian Processes [Williams, 1998] are based on kernel methods. Given an m-sample f (x 1; y 1);:::;(xm; ym) g of patterns xi 2 X and target values yi 2 Y these algorithms minimize the regularized risk functional min f2H R reg [f]= 1 mm X i= 1 c (xi; yi; f (xi))+ 2 kfk 2 H:(1) Here H denotes a reproducing kernel Hilbert space (RKHS)[Aronszajn, 1950],...",Alex J Smola and Bernhard Schölkopf,783,12782513022884144793,,,,,Sparse greedy matrix approximation for machine learning,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.3153,,2000,/scholar?cites=12782513022884144793,DZ-fHPgAAAAJ:LhH-TYMQEocC
14040,"Interessant an der Arbeit von Herrn Sch olkopf sind nicht nur die fachlichen Aspekte, sondern auch die unterschiedlichen und sehr intensiven Kontakte zu internationalen Forschungseinrichtungen. Sie zeigen, da der Autor sowohl in der Lage ist, seine Ergebnisse im wissenschaftlichen Spitzenfeld zu pr asentieren und zu plazieren, als auch aus Arbeiten der\Community"" heraus seine Ergebnisse zu entwickeln. Aus dieser Sicht lat sich auch die fachliche Qualit at der Arbeit ersehen. Herr Sch olkopf untersucht zwei Grundprobleme der Klassifikation gro er Datenmengen. Zum einen ist dies die Extraktion weniger aber relevanter starker Merkmale zur Reduktion der Informations ut, und zum anderen die Beschreibung von Datenbeispielen, die charakteristisch f ur ein gegebenes Klassifikationsproblem sind. Beide Probleme werden von Herrn Sch olkopf sowohl theoretisch als auch in Experimenten ausgiebig und ersch opfend untersucht. Sowohl die in der Arbeit entwickelte, sehr elegante Methode der nichtlinearen Merkmalsextraktion (kernel PCA), als auch die vorgeschlagenen Weiterentwicklungen der Support-Vektor-Maschine benutzen schwache Merkmale und setzen sich damit konzeptuell von der oben beschriebenen Philosophie der starken Merkmale ab. Somit spiegelt sich in der Arbeit gewisserma en ein Paradigmenwechsel in der Klassifikation und Merkmalsextraktion wider. Herr Sch olkopf war w ahrend seiner Dissertation ein gern gesehener Gast der GMD FIRST Berlin, und es war eine Freude, seine Arbeit zu lesen und zu betreuen. Insbesondere freue ich mich, da Herr Sch olkopf seine Forschung in seiner neuen Position bei GMD …",Bernhard Scholkopf,752,7009497440728113520,,,,"Oldenbourg, Munich",Support vector learning,https://pure.mpg.de/rest/items/item_1794215/component/file_3214422/content,,1997,/scholar?cites=7009497440728113520,DZ-fHPgAAAAJ:3NQIlFlcGxIC
14041,"Interessant an der Arbeit von Herrn Sch olkopf sind nicht nur die fachlichen Aspekte, sondern auch die unterschiedlichen und sehr intensiven Kontakte zu internationalen Forschungseinrichtungen. Sie zeigen, da der Autor sowohl in der Lage ist, seine Ergebnisse im wissenschaftlichen Spitzenfeld zu pr asentieren und zu plazieren, als auch aus Arbeiten der\Community"" heraus seine Ergebnisse zu entwickeln. Aus dieser Sicht lat sich auch die fachliche Qualit at der Arbeit ersehen. Herr Sch olkopf untersucht zwei Grundprobleme der Klassifikation gro er Datenmengen. Zum einen ist dies die Extraktion weniger aber relevanter starker Merkmale zur Reduktion der Informations ut, und zum anderen die Beschreibung von Datenbeispielen, die charakteristisch f ur ein gegebenes Klassifikationsproblem sind. Beide Probleme werden von Herrn Sch olkopf sowohl theoretisch als auch in Experimenten ausgiebig und ersch opfend untersucht. Sowohl die in der Arbeit entwickelte, sehr elegante Methode der nichtlinearen Merkmalsextraktion (kernel PCA), als auch die vorgeschlagenen Weiterentwicklungen der Support-Vektor-Maschine benutzen schwache Merkmale und setzen sich damit konzeptuell von der oben beschriebenen Philosophie der starken Merkmale ab. Somit spiegelt sich in der Arbeit gewisserma en ein Paradigmenwechsel in der Klassifikation und Merkmalsextraktion wider. Herr Sch olkopf war w ahrend seiner Dissertation ein gern gesehener Gast der GMD FIRST Berlin, und es war eine Freude, seine Arbeit zu lesen und zu betreuen. Insbesondere freue ich mich, da Herr Sch olkopf seine Forschung in seiner neuen Position bei GMD …",Bernhard Schölkopf,750,7009497440728113520,,,,,Support vector learning,https://pure.mpg.de/rest/items/item_1794215/component/file_3214422/content,,1997,/scholar?cites=7009497440728113520,DZ-fHPgAAAAJ:Nnq8S6OXqDYC
14042,"Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.",Dennis Decoste and Bernhard Schölkopf,741,15409240688358473310,Machine learning,1-3,161-190,Kluwer Academic Publishers,Training invariant support vector machines,https://link.springer.com/article/10.1023/A:1012454411458,46,2002,/scholar?cites=15409240688358473310,DZ-fHPgAAAAJ:P7Ujq4OLJYoC
14043," Motivation: Many problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic.The Maximum Mean Discrepancy (MMD) can take advantage of the kernel trick, which allows us to apply it not only to vectors, but strings, sequences, graphs, and other common structured data types arising in molecular biology. Results: We study the practical feasibility of an MMD-based test on three central data integration tasks: Testing cross-platform comparability of microarray data, cancer diagnosis, and data …",Karsten M Borgwardt and Arthur Gretton and Malte J Rasch and Hans-Peter Kriegel and Bernhard Schölkopf and Alex J Smola,738,7785877672606699347,Bioinformatics,14,e49-e57,Oxford University Press,Integrating structured biological data by kernel maximum mean discrepancy,https://academic.oup.com/bioinformatics/article-abstract/22/14/e49/228383,22,2006,/scholar?cites=7785877672606699347,DZ-fHPgAAAAJ:buQ7SEKw-1sC
14044,"Autonomous helicopter flight is widely regarded to be a highly challenging control problem. This paper presents the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward flip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results significantly extend the state of the art in autonomous helicopter flight. We used the following approach: First we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to find a controller that is optimized for the resulting model and reward function. More specifically, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR).",Pieter Abbeel and Adam Coates and Morgan Quigley and Andrew Ng,675,16683737589594727796,Advances in neural information processing systems,,1-8,,An application of reinforcement learning to aerobatic helicopter flight,https://proceedings.neurips.cc/paper/2006/file/98c39996bf1543e974747a2549b3107c-Paper.pdf,19,2006,/scholar?cites=16683737589594727796,DZ-fHPgAAAAJ:HoJY6CbAsJcC
14045,"We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.",Alex Smola and Arthur Gretton and Le Song and Bernhard Schölkopf,654,817577703290721883,,,13-31,"Springer, Berlin, Heidelberg",A Hilbert space embedding for distributions,https://link.springer.com/chapter/10.1007/978-3-540-75225-7_5,,2007,/scholar?cites=817577703290721883,DZ-fHPgAAAAJ:PaBasH6fAo0C
14046,"Bernhard Scholkopf"" Max-Planck-Institut fur biologische Kybernetik, Spemannstr. 38 72076 Tubingen, Germany bs@ mpik-tueb. mpg. deSupport Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine two such techniques on a pattern recognition problem. The method for improving generalization performance (the"" virtual support vector"" method) does so by incorporating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the"" reduced set"" method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1% error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine.",C. Burges and B. Schölkopf,639,675612459470283947,Advances in Neural Information Processing Systems,,375-381,,Improving the accuracy and speed of support vector machines,http://papers.nips.cc/paper/1253-improving-the-accuracy-and-speed-of-support-vector-machines.pdf,9,1997,/scholar?cites=675612459470283947,DZ-fHPgAAAAJ:HeT0ZceujKMC
14047,"We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods. Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold. We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples.",Jihun Ham and Daniel D Lee and Sebastian Mika and Bernhard Schölkopf,615,15369584681682737324,,,47,,A kernel view of the dimensionality reduction of manifolds,https://dl.acm.org/doi/abs/10.1145/1015330.1015417,,2004,/scholar?cites=15369584681682737324,DZ-fHPgAAAAJ:_OXeSy2IsFwC
14048,"We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach.",Olivier Chapelle and Jason Weston and Bernhard Schölkopf,613,12685992758255245343,Advances in neural information processing systems,,601-608,,Cluster kernels for semi-supervised learning,https://papers.nips.cc/paper/2002/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf,15,2002,/scholar?cites=12685992758255245343,DZ-fHPgAAAAJ:8xutWZnSdmoC
14049,"Motivation: In order to extract protein sequences from nucleotide    sequences, it is an important step to recognize points at which    regions start that code for proteins. These points are called    translation initiation sites (TIS).Results: The task of finding TIS can be modeled as a    classification problem. We demonstrate the applicability of support    vector machines for this task, and show how to incorporate prior    biological knowledge by engineering an appropriate kernel function.    With the described techniques the recognition performance can be    improved by 26% over leading existing approaches. We provide    evidence that existing related methods (e.g. ESTScan) could    profit from advanced TIS recognition.Contact: {Alexander.Zien,Gunnar.Raetsch,Sebastian. Mika}@gmd.de;    bsc@microsoft.com ",A Zien and G Ratsch and S Mika and B Scholkopf and T Lengauer and KR Muller,611,11351321583603547668,"Bioinformatics-Oxford, GCB'99 PAPERS PRESENTED AT THE GERMAN CONFERENCE ON BIOINFORMATICS, 1999",9,799-807,"Oxford: Oxford University Press, c1998-",Engineering support vector machine kernels that recognize translation initiation sites,https://academic.oup.com/bioinformatics/article-abstract/16/9/799/307655,16,1999,/scholar?cites=11351321583603547668,DZ-fHPgAAAAJ:UmS_249rOGwC
14050,"The increasing wealth of biological data coming from a large variety of platforms and the continued development of new high-throughput methods for probing biological systems require increasingly more sophisticated computational approaches. Putting all these data in simple-to-use databases is a first step; but realizing the full potential of the data requires algorithms that automatically extract regularities from the data, which can then lead to biological insight. Many of the problems in computational biology are in the form of prediction: starting from prediction of a gene’s structure, prediction of its function, interactions, and role in disease. Support vector machines (SVMs) and related kernel methods are extremely good at solving such problems [1–3]. SVMs are widely used in computational biology due to their high accuracy, their ability to deal with high-dimensional and large datasets, and their flexibility in modeling diverse sources of data [2, 4–6]. The simplest form of a prediction problem is binary classification: trying to discriminate between objects that belong to one of two categories—positive (+ 1) or negative (21). SVMs use two key concepts to solve this problem: large margin separation and kernel functions. The idea of large margin separation can be motivated by classification of points in two dimensions (see Figure 1). A simple way to classify the points is to draw a straight line and call points lying on one side positive and on the other side negative. If the two sets are well separated, one would intuitively draw the separating line such that it is as far as possible away from the points in both sets (see Figures 2 and 3). This intuitive choice …",Asa Ben-Hur and Cheng Soon Ong and Sören Sonnenburg and Bernhard Schölkopf and Gunnar Rätsch,607,9454651990475347308,PLoS Comput Biol,10,e1000173,Public Library of Science,Support vector machines and kernels for computational biology,https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000173,4,2008,/scholar?cites=9454651990475347308,DZ-fHPgAAAAJ:AHdEip9mkN0C
14051,,Manuel Gomez-Rodriguez and David Balduzzi and Bernhard Schölkopf,563,15527232691188136467,"Twenty-eighth International Conference on Machine Learning, Bellevue, Washing",,,,Uncovering the temporal dynamics of diffusion networks,,,2011,/scholar?cites=15527232691188136467,DZ-fHPgAAAAJ:UHK10RUVsp4C
14052,"The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities.",Patrik Hoyer and Dominik Janzing and Joris Mooij and Jonas Peters and Bernhard Schölkopf,553,6461868687333835050,,,,,Nonlinear causal discovery with additive noise models,http://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models,,2009,/scholar?cites=6461868687333835050,DZ-fHPgAAAAJ:vV6vV6tmYwMC
14053,"Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically significant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O (m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.",Arthur Gretton and Kenji Fukumizu and Choon Hui Teo and Le Song and Bernhard Schölkopf and Alexander J Smola,537,18403722076542824893,,,,MIT Press,A kernel statistical test of independence,https://papers.nips.cc/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf,,2008,/scholar?cites=18403722076542824893,DZ-fHPgAAAAJ:ubry08Y2EpUC
14054,Designing a brain computer interface (BCI) system one can choose from a variety of features that may be useful for classifying brain activity during a mental task. For the special case of classifying electroencephalogram (EEG) signals we propose the usage of the state of the art feature selection algorithms Recursive Feature Elimination and Zero-Norm Optimization which are based on the training of support vector machines (SVM) . These algorithms can provide more accurate solutions than standard filter methods for feature selection . We adapt the methods for the purpose of selecting EEG channels. For a motor imagery paradigm we show that the number of used channels can be reduced significantly without increasing the classification error. The resulting best channels agree well with the expected underlying cortical activity patterns during the mental tasks. Furthermore we show how time dependent task specific …,Thomas Navin Lal and Michael Schroder and Thilo Hinterberger and Jason Weston and Martin Bogdan and Niels Birbaumer and Bernhard Scholkopf,536,17362625546734092863,IEEE transactions on biomedical engineering,6,1003-1010,IEEE,Support vector channel selection in BCI,https://ieeexplore.ieee.org/abstract/document/1300795/,51,2004,/scholar?cites=17362625546734092863,DZ-fHPgAAAAJ:CdxZDUztZiMC
14055,"We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database.",Deva Ramanan,528,15045163446431061776,Advances in neural information processing systems,,1129-1136,,Learning to parse images of articulated bodies,https://proceedings.neurips.cc/paper/2006/file/a209ca7b50dcaab2db7c2d4d1223d4d5-Paper.pdf,19,2006,/scholar?cites=15045163446431061776,DZ-fHPgAAAAJ:CZq_8wBqUikC
14056,,Matthias Hofmann and Florian Steinke and Verena Scheel and Guillaume Charpiat and Jason Farquhar and Philip Aschoff and Michael Brady and Bernhard Schölkopf and Bernd J Pichler,512,5226439069726136462,Journal of nuclear medicine,11,1875-1883,Society of Nuclear Medicine,MRI-based attenuation correction for PET/MRI: a novel approach combining pattern recognition and atlas registration,http://jnm.snmjournals.org/content/49/11/1875.short,49,2008,/scholar?cites=5226439069726136462,DZ-fHPgAAAAJ:jgBuDB5drN8C
14057,,Matthias Hofmann and Florian Steinke and Verena Scheel and Guillaume Charpiat and Jason Farquhar and Philip Aschoff and Michael Brady and Bernhard Schölkopf and Bernd J Pichler,512,5226439069726136462,Journal of nuclear medicine,11,1875-1883,Society of Nuclear Medicine,MRI-based attenuation correction for PET/MRI: a novel approach combining pattern recognition and atlas registration,http://jnm.snmjournals.org/content/49/11/1875.short,49,2008,/scholar?cites=5226439069726136462,DZ-fHPgAAAAJ:a9-T7VOCCH8C
14058,"We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.",Anat Levin,510,3987369631548888320,,,841-848,,Blind motion deblurring using image statistics,http://papers.nips.cc/paper/3085-blind-motion-deblurring-using-image-statistics.pdf,,2007,/scholar?cites=3987369631548888320,DZ-fHPgAAAAJ:YiXZnN5otT4C
14059,"A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that defines the inner product in the RKHS.We present three theoretical properties of γk. First, we consider the question of determining the conditions on the kernel k for which γk is a metric: such k are denoted characteristic kernels. Unlike pseudometrics, a metric is zero only when two distributions coincide, thus ensuring the RKHS embedding maps all distributions uniquely (ie, the embedding is injective). While previously published conditions may apply only in restricted circumstances (eg, on compact domains), and are difficult to check, our conditions are straightforward and intuitive: integrally strictly positive definite kernels are characteristic. Alternatively, if a bounded continuous kernel is translation-invariant on Rd, then it is characteristic if and only if the support of its Fourier transform is the entire Rd. Second, we show that the distance between distributions under γk results from an interplay between the properties of the kernel and the distributions, by demonstrating that distributions are close in the embedding space when their differences occur at higher frequencies. Third, to understand the",Bharath K Sriperumbudur and Arthur Gretton and Kenji Fukumizu and Bernhard Schölkopf and Gert RG Lanckriet,497,11143857768927088229,The Journal of Machine Learning Research,,1517-1561,JMLR. org,Hilbert space embeddings and metrics on probability measures,http://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf,11,2010,/scholar?cites=11143857768927088229,DZ-fHPgAAAAJ:e_rmSamDkqQC
14060,"We briefly describe the main ideas of statistical learning theory, support vector machines (SVMs), and kernel feature spaces. We place particular emphasis on a description of the so‐called ν‐SVM, including details of the algorithm and its implementation, theoretical results, and practical applications. Copyright © 2005 John Wiley & Sons, Ltd.",Pai‐Hsuen Chen and Chih‐Jen Lin and Bernhard Schölkopf,497,14746104371808218034,Applied Stochastic Models in Business and Industry,2,111-136,"John Wiley & Sons, Ltd.",A tutorial on ν‐support vector machines,https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.537,21,2005,/scholar?cites=14746104371808218034,DZ-fHPgAAAAJ:gVv57TyPmFsC
14061,"A method is described which, like the kernel trick in support vector machines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too. As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms.",Bernhard Schölkopf,488,9163127197151978783,Advances in neural information processing systems,,301-307,,The kernel trick for distances,https://papers.nips.cc/paper/2000/file/4e87337f366f72daa424dae11df0538c-Paper.pdf,13,2000,/scholar?cites=9163127197151978783,DZ-fHPgAAAAJ:LPtt_HFRSbwC
14062,"Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.",Mehdi SM Sajjadi and Bernhard Scholkopf and Michael Hirsch,472,308024524496163151,,,4491-4500,,Enhancenet: Single image super-resolution through automated texture synthesis,http://openaccess.thecvf.com/content_iccv_2017/html/Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper.html,,2017,/scholar?cites=308024524496163151,DZ-fHPgAAAAJ:bX5HOE8cPfIC
14063,"We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques. In the absence of labeled instances, this framework can be utilized as a spectral clustering method for directed graphs, which generalizes the spectral clustering approach for undirected graphs. We have applied our framework to real-world web classification problems and obtained encouraging results.",Dengyong Zhou and Jiayuan Huang and Bernhard Schölkopf,464,598062950590086511,,,1036-1043,,Learning from labeled and unlabeled data on a directed graph,https://dl.acm.org/doi/abs/10.1145/1102351.1102482,,2005,/scholar?cites=598062950590086511,DZ-fHPgAAAAJ:gKiMpY-AVTkC
14064,"Machine learning develops intelligent computer systems that are able to generalizefrom previously seen examples. A new domain of machine learning, in which the prediction mustsatisfy the additional constraints found in structured data, poses one of machine learning'sgreatest challenges: learning functional dependencies between arbitrary input and output domains. This volume presents and analyzes the state of the art in machine learning algorithms and theory inthis novel field. The contributors discuss applications as diverse as machine translation, documentmarkup, computational biology, and information extraction, among others, providing a timely overviewof an exciting field. Contributors Yasemin Altun, Gökhan Bakir [no dot over i], Olivier Bousquet, Sumit Chopra, Corinna Cortes, Hal Daumé III, Ofer Dekel, Zoubin Ghahramani, Raia Hadsell, ThomasHofmann, Fu Jie Huang, Yann LeCun, Tobias Mann, Daniel Marcu, David McAllester, Mehryar Mohri, William Stafford Noble, Fernando Pérez-Cruz, Massimiliano Pontil, Marc'Aurelio Ranzato, Juho Rousu, Craig Saunders, Bernhard Schölkopf, Matthias W. Seeger, Shai Shalev-Shwartz, John Shawe-Taylor, Yoram Singer, Alexander J. Smola, Sandor Szedmak, Ben Taskar, Ioannis Tsochantaridis, SV NVishwanathan, Jason Weston Gökhan Bakir [no dot over i] is Research Scientist at the Max PlanckInstitute for Biological Cybernetics in Tübingen, Germany. Thomas Hofmann is a Director ofEngineering at Google's Engineering Center in Zurich and Adjunct Associate Professor of ComputerScience at Brown University. Bernhard Schölkopf is Director of the Max Planck Institute forBiological …",G BakIr and T Hofmann and B Schölkopf and AJ Smola and B Taskar and SVN Vishwanathan,462,11616044371673680281,,,,The MIT Press,Predicting structured data,http://books.google.com/books?hl=en&lr=&id=b1EFKUoFF8IC&oi=fnd&pg=PR5&dq=info:mQnJxM96NKEJ:scholar.google.com&ots=RJ8jbP5I_b&sig=kKHa_4VgnRO8bi3w4DZ4IzHoNuQ,,2007,/scholar?cites=11616044371673680281,DZ-fHPgAAAAJ:z6xuaG2dYH0C
14065,"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf,455,1669877132293977025,arXiv preprint arXiv:1711.01558,,,,Wasserstein auto-encoders,https://arxiv.org/abs/1711.01558,,2017,/scholar?cites=1669877132293977025,DZ-fHPgAAAAJ:02qyplpeh24C
14066,"A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning.The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.",Jonas Peters and Dominik Janzing and Bernhard Schölkopf,453,13239810736267378235,,,288,The MIT Press,Elements of causal inference,http://library.oapen.org/handle/20.500.12657/26040,,2017,/scholar?cites=13239810736267378235,DZ-fHPgAAAAJ:TBvh2Nz22_QC
14067,"By setting apart the two functions of a support vector machine: separation of points by a nonlinear surface in the original space of patterns, and maximizing the distance between separating planes in a higher dimensional space, we are able to define indefinite, possibly discontinuous, kernels, not necessarily inner product ones, that generate highly nonlinear separating surfaces. Maximizing the distance between the separating planes in the higher dimensional space is surrogated by support vector suppression, which is achieved by minimizing any desired norm of support vector multipliers. The norm may be one induced by the separation kernel if it happens to be positive definite, or a Euclidean or a polyhedral norm. The latter norm leads to linear program whereas the former norms lead to convex quadratic programs, all with an arbitrary separation kernel. A standard support vector machine can be recovered by using the same kernel for separation and support vector suppression. On a simple test example, all models perform equally well when a positive definite kernel is used. When a negative definite kernel is used, we are unable to solve the non convex quadratic program associated with a conventional support vector machine, while all other proposed models remain convex and easily generate a surface that separates all given points.",Olvi Mangasarian,432,5796969218003862061,,,,,Generalized support vector machines,https://minds.wisconsin.edu/handle/1793/64390,,1998,/scholar?cites=5796969218003862061,DZ-fHPgAAAAJ:plOeWmaeDcAC
14068,We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transfonnations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.,Bernhard Schölkopf and Patrice Simard and Alex Smola and Vladimir Vapnik,427,3971972194865622137,Advances in neural information processing systems,,640-646,,Prior knowledge in support vector kernels,https://papers.nips.cc/paper/1997/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf,10,1997,/scholar?cites=3971972194865622137,DZ-fHPgAAAAJ:Ehil0879vHcC
14069,"We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.",Kenji Fukumizu and Arthur Gretton and Xiaohai Sun and Bernhard Schölkopf,423,11305614332388461994,Advances in neural information processing systems,,489-496,,Kernel measures of conditional dependence,https://papers.nips.cc/paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf,20,2007,/scholar?cites=11305614332388461994,DZ-fHPgAAAAJ:CaZNVDsoPx4C
14070,"Given sets of observations of training and test data, we consider the problem of re-weighting the training data such that its distribution more closely matches that of the test data. We achieve this goal by matching covariate distributions between training and test sets in a high dimensional feature space (specifically, a reproducing kernel Hilbert space). This approach does not require distribution estimation. Instead, the sample weights are obtained by a simple quadratic programming procedure. We provide a uniform convergence bound on the distance between the reweighted training feature mean and the test feature mean, a transductive bound on the expected loss of an algorithm trained on the reweighted data, and a connection to single class SVMs. While our method is designed to deal with the case of simple covariate shift (in the sense of Chapter??), we have also found benefits for sample selection bias on the labels. Our correction procedure yields its greatest and most consistent advantages when the learning algorithm returns a classifier/regressor that is “simpler” than the data might suggest.",Arthur Gretton and Alex Smola and Jiayuan Huang and Marcel Schmittfull and Karsten Borgwardt and Bernhard Schölkopf,419,17121418164885310348,Dataset shift in machine learning,,131-160,,Covariate shift by kernel mean matching,http://www.cs.cmu.edu/~arthurg/papers/covariateShiftChapter.pdf,,2009,/scholar?cites=17121418164885310348,DZ-fHPgAAAAJ:ruyezt5ZtCIC
14071," We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem.",Alex J Smola and Bernhard Scholkopf,419,13971567283940344480,GMD Technical Report: 1064,,,,"On a Kernel-based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion",https://link.springer.com/article/10.1007/PL00013831,,1997,/scholar?cites=13971567283940344480,DZ-fHPgAAAAJ:PkcyUWeTMh0C
14072,,Bernhard Schlkopf and Alexander J Smola,397,14813553777304822168,,,,MIT press,Learning with kernels,http://scholar.google.com/scholar?cluster=14813553777304822168&hl=en&oi=scholarr,,2002,/scholar?cites=14813553777304822168,DZ-fHPgAAAAJ:qm0DHzEu2WcC
14073,"Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary.",Bernhard Schölkopf and Chris Burges and Vladimir Vapnik,394,17280711061590309676,,,47-52,"Springer, Berlin, Heidelberg",Incorporating invariances in support vector learning machines,https://link.springer.com/chapter/10.1007/3-540-61510-5_12,,1996,/scholar?cites=17280711061590309676,DZ-fHPgAAAAJ:u-coK7KVo8oC
14074,"The book provides an overview of recent developments in large margin classifiers, examines connections with other methods (eg, Bayesian inference), and identifies strengths and weaknesses of the method, as well as directions for future research. The concept of large margins is a unifying principle for the analysis of many different approaches to the classification of data from examples, including boosting, mathematical programming, neural networks, and support vector machines. The fact that it is the margin, or confidence level, of a classification--that is, a scale parameter--rather than a raw training error that matters has become a key tool for dealing with classifiers. This book shows how this idea applies to both the theoretical analysis and the design of algorithms. The book provides an overview of recent developments in large margin classifiers, examines connections with other methods (eg, Bayesian inference), and identifies strengths and weaknesses of the method, as well as directions for future research. Among the contributors are Manfred Opper, Vladimir Vapnik, and Grace Wahba.",Smola and Bartlett and Schölkopf and Schuurmans,389,17904075224577240395,,,,MIT press,Advances in large margin classifiers,http://books.google.com/books?hl=en&lr=&id=gOXI3fO3VUwC&oi=fnd&pg=PP15&dq=info:S23MkI4NePgJ:scholar.google.com&ots=ik-JbdrORF&sig=kh5IzmmOw-od_pZEEsP8eb_ua0I,,2000,/scholar?cites=17904075224577240395,DZ-fHPgAAAAJ:BOlwja0KXvYC
14075,"Positron emission tomography (PET) is a fully quantitative technology for imaging metabolic pathways and dynamic processes in vivo. Attenuation correction of raw PET data is a prerequisite for quantification and is typically based on separate transmission measurements. In PET/CT attenuation correction, however, is performed routinely based on the available CT transmission data.Recently, combined PET/magnetic resonance (MR) has been proposed as a viable alternative to PET/CT. Current concepts of PET/MRI do not include CT-like transmission sources and, therefore, alternative methods of PET attenuation correction must be found. This article reviews existing approaches to MR-based attenuation correction (MR-AC). Most groups have proposed MR-AC algorithms for brain PET studies and more recently also for torso PET …",Matthias Hofmann and Bernd Pichler and Bernhard Schölkopf and Thomas Beyer,373,15233314839035930261,European journal of nuclear medicine and molecular imaging,1,93-104,Springer-Verlag,Towards quantitative PET/MRI: a review of MR-based attenuation correction techniques,https://link.springer.com/content/pdf/10.1007/s00259-008-1007-7.pdf,36,2009,/scholar?cites=15233314839035930261,DZ-fHPgAAAAJ:1taIhTC69MYC
14076,"The problem of feature selection is a difficult combinatorial task in machine learning and of high practical relevance, e.g. in bioinformatics. genetic algorithms (GAs) offer a natural way to solve this problem. In this paper, we present a special genetic algorithm, which especially takes into account the existing bounds on the generalization error for support vector machines (SVMs). This new approach is compared to the traditional method of performing cross-validation and to other existing algorithms for feature selection.",Holger Frohlich and Olivier Chapelle and Bernhard Scholkopf,373,6273944903142600380,,,142-148,IEEE,Feature selection for support vector machines by means of genetic algorithm,https://ieeexplore.ieee.org/abstract/document/1250182/,,2003,/scholar?cites=6273944903142600380,DZ-fHPgAAAAJ:FRqDyidJaO0C
14077,"During the past 3 years, the support vector machine (SVM) learning algorithm has been extensively applied within the field of computational biology. The algorithm has been used to detect patterns within and among biological sequences, to classify genes and patients based upon gene expression profiles, and has recently been applied to several new biological problems. This chapter reviews the state of the art with respect to SVM applications in computational biology.",William Stafford Noble,358,5002990866059368871,Kernel methods in computational biology,,92,,Support vector machine applications in computational biology,http://books.google.com/books?hl=en&lr=&id=SwAooknaMXgC&oi=fnd&pg=PA71&dq=info:pxn-ia8xbkUJ:scholar.google.com&ots=rMwgCvQbDh&sig=6NTSGptNzDFa1EewXDfmmtOrqDg,71,2004,/scholar?cites=5002990866059368871,DZ-fHPgAAAAJ:AdBFlfKQu2MC
14078,"Much recent attention, both experimental and theoretical, has been focussed on classification algorithms which produce voted combinations of classifiers. Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classifier having large margins on the training data. We present abstract algorithms for finding linear and convex combinations of functions that minimize arbitrary cost functionals (ie functionals that do not necessarily depend on the margin). Many existing voting methods can be shown to be special cases of these abstract algorithms. Then, following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions.Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations. Margin distribution plots verify that DOOM II is willing to'give up'on examples that are too hard in order to avoid overfitting. We also show that the overfitting behavior exhibited by AdaBoost can be quantified in terms of our proposed cost function.",Llew Mason and Jonathan Baxter and Peter L Bartlett and Marcus Frean,353,10681157738910533448,Advances in Neural Information Processing Systems,,221-246,MIT; 1998,Functional gradient techniques for combining hypotheses,https://www.researchgate.net/profile/Marcus_Frean/publication/243689632_Functional_gradient_techniques_for_combining_hypotheses/links/02e7e5322292fc93dc000000/Functional-gradient-techniques-for-combining-hypotheses.pdf,,1999,/scholar?cites=10681157738910533448,DZ-fHPgAAAAJ:NXYAu82O0W8C
14079,"Two view-based object recognition algorithms are compared: (1) a heuristic algorithm based on oriented filters, and (2) a support vector learning machine trained on low-resolution images of the objects. Classification performance is assessed using a high number of images generated by a computer graphics system under precisely controlled conditions. Training- and test-images show a set of 25 realistic three-dimensional models of chairs from viewing directions spread over the upper half of the viewing sphere. The percentage of correct identification of all 25 objects is measured.",Volker Blanz and Bernhard Schölkopf and HCBVV Bülthoff and Chris Burges and Vladimir Vapnik and Thomas Vetter,345,230282947735226958,,,251-256,"Springer, Berlin, Heidelberg",Comparison of view-based object recognition algorithms using realistic 3D models,https://link.springer.com/chapter/10.1007/3-540-61510-5_45,,1996,/scholar?cites=230282947735226958,DZ-fHPgAAAAJ:fbc8zXXH2BUC
14080,"Kernel methods in general, and support vector machines (SVMs) in particular, are increasingly used to solve various problems in computational biology. They offer versatile tools to process, analyze, and compare many types of data, and offer stateof-the-art performance in many cases. This self-contained introduction to positive definite kernels and kernel methods aims at providing the very basic knowledge and intuition that the reader might find useful in order to fully grasp the technical content of this book.",Jean-Philippe Vert and Koji Tsuda and Bernhard Schölkopf,342,5339334397881177727,Kernel methods in computational biology,,35-70,MIT press,A primer on kernel methods,http://books.google.com/books?hl=en&lr=&id=SwAooknaMXgC&oi=fnd&pg=PA35&dq=info:f1a7ilggGUoJ:scholar.google.com&ots=rMwgCvQbDn&sig=XhMogZD5kOvjDgKuKUQZufnA_3A,47,2004,/scholar?cites=5339334397881177727,DZ-fHPgAAAAJ:pLhbkOKWazAC
14081,"In recent years, kernel principal component analysis (KPCA) has been suggested for various image processing tasks requiring an image model such as, e.g., denoising or compression. The original form of KPCA, however, can be only applied to strongly restricted image classes due to the limited number of training examples that can be processed. We therefore propose a new iterative method for performing KPCA, the kernel Hebbian algorithm, which iteratively estimates the kernel principal components with only linear order memory complexity. In our experiments, we compute models for complex image classes such as faces and natural images which require a large number of training examples. The resulting image models are tested in single-frame super-resolution and denoising applications. The KPCA model is not specifically tailored to these tasks; in fact, the same model can be used in super-resolution with …",Kwang In Kim and Matthias O Franz and Bernhard Scholkopf,334,6384320846465248464,IEEE transactions on pattern analysis and machine intelligence,9,1351-1366,IEEE,Iterative kernel principal component analysis for image modeling,https://ieeexplore.ieee.org/abstract/document/1471703/,27,2005,/scholar?cites=6384320846465248464,DZ-fHPgAAAAJ:rHJHxKgnXwkC
14082,"We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.",Christian J Schuler and Michael Hirsch and Stefan Harmeling and Bernhard Schölkopf,321,15689694022022171806,IEEE transactions on pattern analysis and machine intelligence,7,1439-1451,IEEE,Learning to deblur,https://ieeexplore.ieee.org/abstract/document/7274732/,38,2015,/scholar?cites=15689694022022171806,DZ-fHPgAAAAJ:sYWwZaPVD1oC
14083,,Ulrich H-G Kreßel,318,17327770169004879892,,,255-268,"MIT Press, Cambridge, MA","Pairwise classification and support vector machines, Advances in kernel methods: support vector learning",http://scholar.google.com/scholar?cluster=17327770169004879892&hl=en&oi=scholarr,,1999,/scholar?cites=17327770169004879892,DZ-fHPgAAAAJ:MBnsgXbI6-AC
14084,"We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis.",Arthur Gretton and Ralf Herbrich and Alexander Smola and Olivier Bousquet and Bernhard Schölkopf,317,3191928505604467318,The Journal of Machine Learning Research,,2075-2129,JMLR. org,Kernel methods for measuring independence,http://www.jmlr.org/papers/v6/gretton05a.html,6,2005,/scholar?cites=3191928505604467318,DZ-fHPgAAAAJ:jL-93Qbq4QoC
14085,"Various graph-based algorithms for semi-supervised learning have been proposed in the recent literature. They rely on the idea of building a graph whose nodes are data points (labeled and unlabeled) and edges represent similarities between points. Known labels are used to propagate information through the graph in order to label all nodes. In this chapter, we show how these different algorithms can be cast into a common framework where one minimizes a quadratic cost criterion whose closedform solution is found by solving a linear system of size n (total number of data points). The cost criterion naturally leads to an extension of such algorithms to the inductive setting, where one obtains test samples one at a time: the derived induction formula can be evaluated in O (n) time, which is much more efficient than solving again exactly the linear system (which in general costs O (kn2) time for a sparse graph where each data point has k neighbors). We also use this inductive formula to show that when the similarity between points satisfies a locality property, then the algorithms are plagued by the curse of dimensionality, with respect to the dimensionality of an underlying manifold.",Yoshua Bengio and Olivier Delalleau and Nicolas Le Roux,314,1630869818472058708,,,,,11 label propagation and quadratic criterion,https://www.researchgate.net/profile/Y_Bengio/publication/238675708_Label_Propagation_and_Quadratic_Criterion/links/0f3175320aae4ada34000000/Label-Propagation-and-Quadratic-Criterion.pdf,,2006,/scholar?cites=1630869818472058708,DZ-fHPgAAAAJ:e0LTWoPxLYMC
14086,"Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.",Kun Zhang and Jonas Peters and Dominik Janzing and Bernhard Schölkopf,313,17538658264728812755,arXiv preprint arXiv:1202.3775,,,,Kernel-based conditional independence test and application in causal discovery,https://arxiv.org/abs/1202.3775,,2012,/scholar?cites=17538658264728812755,DZ-fHPgAAAAJ:4TOpqqG69KYC
14087,"Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios:(1) the marginal PY changes, while the conditional PX| Y stays the same (target shift),(2) the marginal PY is fixed, while the conditional PX| Y changes with certain constraints (conditional shift), and (3) the marginal PY changes, and the conditional PX| Y changes with constraints (generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and realworld data sets demonstrate the effectiveness of the proposed framework.",Kun Zhang and Bernhard Schölkopf and Krikamol Muandet and Zhikun Wang,310,5724484728674297162,,,819-827,,Domain adaptation under target and conditional shift,http://www.jmlr.org/proceedings/papers/v28/zhang13d.pdf,,2013,/scholar?cites=5724484728674297162,DZ-fHPgAAAAJ:RXiHnyRawswC
14088,"We present a purely vision-based scheme for learning a topological representation of an open environment. The system represents selected places by local views of the surrounding scene, and finds traversable paths between them. The set of recorded views and their connections are combined into a graph model of the environment. To navigate between views connected in the graph, we employ a homing strategy inspired by findings of insect ethology. In robot experiments, we demonstrate that complex visual exploration and navigation tasks can thus be performed without using metric information.",Matthias O Franz and Bernhard Schölkopf and Hanspeter A Mallot and Heinrich H Bülthoff,308,7092779758466767428,Autonomous robots,1,111-125,Kluwer Academic Publishers,Learning view graphs for robot navigation,https://link.springer.com/article/10.1023/A:1008821210922,5,1998,/scholar?cites=7092779758466767428,DZ-fHPgAAAAJ:-F2olFd-qnkC
14089,"This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.",Krikamol Muandet and David Balduzzi and Bernhard Schölkopf,299,4651322990057915347,,,10-18,,Domain generalization via invariant feature representation,http://www.jmlr.org/proceedings/papers/v28/muandet13.pdf,,2013,/scholar?cites=4651322990057915347,DZ-fHPgAAAAJ:6VlyvFCUEfcC
14090," In homing tasks, the goal is often not marked by visible objects but must be inferred from the spatial relation to the visual cues in the surrounding scene. The exact computation of the goal direction would require knowledge about the distances to visible landmarks, information, which is not directly available to passive vision systems. However, if prior assumptions about typical distance distributions are used, a snapshot taken at the goal suffices to compute the goal direction from the current view. We show that most existing approaches to scene-based homing implicitly assume an isotropic landmark distribution. As an alternative, we propose a homing scheme that uses parameterized displacement fields. These are obtained from an approximation that incorporates prior knowledge about perspective distortions of the visual environment. A mathematical analysis proves that both approximations do not prevent …",Matthias O Franz and Bernhard Schölkopf and Hanspeter A Mallot and Heinrich H Bülthoff,298,5327293855324902137,Biological Cybernetics,3,191-202,Springer-Verlag,Where did I take that snapshot? Scene-based homing by image matching,https://link.springer.com/article/10.1007/s004220050470,79,1998,/scholar?cites=5327293855324902137,DZ-fHPgAAAAJ:nVrZBo8bIpAC
14091,"Motivation: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data—examples with known 3D structures, organized into structural classes—whereas in practice, unlabeled data are far more plentiful.Results: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster …",Jason Weston and Christina Leslie and Eugene Ie and Dengyong Zhou and Andre Elisseeff and William Stafford Noble,297,14733944164908182794,Bioinformatics,15,3241-3247,Oxford University Press,Semi-supervised protein classification using cluster kernels,https://academic.oup.com/bioinformatics/article-abstract/21/15/3241/195405,21,2005,/scholar?cites=14733944164908182794,DZ-fHPgAAAAJ:NxmKEeNBbOMC
14092,"The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than  models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Bachem,293,15156765665179946120,,,4114-4124,PMLR,Challenging common assumptions in the unsupervised learning of disentangled representations,http://proceedings.mlr.press/v97/locatello19a.html,,2019,/scholar?cites=15156765665179946120,DZ-fHPgAAAAJ:SrJLFjpZNIkC
14093,,Thorsten Joachims and B Schölkopf,293,7856796037839381159,Making large-scale SVM learning practical,,41-56,"Cambridge, USA: MIT Press",Advances in kernel methods-support vector learning,http://scholar.google.com/scholar?cluster=7856796037839381159&hl=en&oi=scholarr,,1999,/scholar?cites=7856796037839381159,DZ-fHPgAAAAJ:EUpubTECWSAC
14094,"We show via an equivalence of mathematical programs that a support vector (SV) algorithm can be translated into an equivalent boosting-like algorithm and vice versa. We exemplify this translation procedure for a new algorithm: one-class leveraging, starting from the one-class support vector machine (1-SVM). This is a first step toward unsupervised learning in a boosting framework. Building on so-called barrier methods known from the theory of constrained optimization, it returns a function, written as a convex combination of base hypotheses, that characterizes whether a given test point is likely to have been generated from the distribution underlying the training data. Simulations on one-class classification problems demonstrate the usefulness of our approach.",Gunnar Ratsch and Sebastian Mika and Bernhard Scholkopf and K-R Muller,292,13359946369118733339,IEEE Transactions on Pattern Analysis and Machine Intelligence,9,1184-1199,IEEE,Constructing boosting algorithms from SVMs: An application to one-class classification,https://ieeexplore.ieee.org/abstract/document/1033211/,24,2002,/scholar?cites=13359946369118733339,DZ-fHPgAAAAJ:6_hjMsCP8ZoC
14095,"We present a local learning approach for clustering. The basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. An optimization problem is formulated such that its solution has the above property. Relaxation and eigen-decomposition are applied to solve this optimization problem. We also briefly investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. Experimental results are provided to validate the effectiveness of the proposed approach.",Mingrui Wu and Bernhard Schölkopf,290,6328427700084042625,,,1529-1536,,A local learning approach for clustering,http://papers.nips.cc/paper/3115-a-local-learning-approach-for-clustering.pdf,,2007,/scholar?cites=6328427700084042625,DZ-fHPgAAAAJ:QyXJ3EUuO1IC
14096,"Camera shake leads to non-uniform image blurs. State-of-the-art methods for removing camera shake model the blur as a linear combination of homographically transformed versions of the true image. While this is conceptually interesting, the resulting algorithms are computationally demanding. In this paper we develop a forward model based on the efficient filter flow framework, incorporating the particularities of camera shake, and show how an efficient algorithm for blur removal can be obtained. Comprehensive comparisons on a number of real-world blurry images show that our approach is not only substantially faster, but it also leads to better deblurring results.",Michael Hirsch and Christian J Schuler and Stefan Harmeling and Bernhard Schölkopf,288,13239680457316868333,,,463-470,IEEE,Fast removal of non-uniform camera shake,https://ieeexplore.ieee.org/abstract/document/6126276/,,2011,/scholar?cites=13239680457316868333,DZ-fHPgAAAAJ:J_g5lzvAfSwC
14097,"Motion blur due to camera shake is one of the predominant sources of degradation in handheld photography. Single image blind deconvolution (BD) or motion deblurring aims at restoring a sharp latent image from the blurred recorded picture without knowing the camera motion that took place during the exposure. BD is a long-standing problem, but has attracted much attention recently, cumulating in several algorithms able to restore photos degraded by real camera motion in high quality. In this paper, we present a benchmark dataset for motion deblurring that allows quantitative performance evaluation and comparison of recent approaches featuring non-uniform blur models. To this end, we record and analyse real camera motion, which is played back on a robot platform such that we can record a sequence of sharp images sampling the six dimensional camera motion trajectory. The goal of deblurring is …",Rolf Köhler and Michael Hirsch and Betty Mohler and Bernhard Schölkopf and Stefan Harmeling,282,6101114368042202818,,,27-40,"Springer, Berlin, Heidelberg",Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database,https://link.springer.com/chapter/10.1007/978-3-642-33786-4_3,,2012,/scholar?cites=6101114368042202818,DZ-fHPgAAAAJ:m4fbC6XIj1kC
14098,"Diffusion of information, spread of rumors and infectious diseases are all instances of stochastic processes that occur over the edges of an underlying network. Many times networks over which contagions spread are unobserved, and such networks are often dynamic and change over time. In this paper, we investigate the problem of inferring dynamic networks based on information diffusion data. We assume there is an unobserved dynamic network that changes over time, while we observe the results of a dynamic process spreading over the edges of the network. The task then is to infer the edges and the dynamics of the underlying network.",Manuel Gomez Rodriguez and Jure Leskovec and Bernhard Schölkopf,279,12994175967219338150,,,23-32,,Structure and dynamics of information pathways in online media,https://dl.acm.org/doi/abs/10.1145/2433396.2433402,,2013,/scholar?cites=12994175967219338150,DZ-fHPgAAAAJ:pYKElYtJMmwC
14099,,Matthias Hofmann and Ilja Bezrukov and Frederic Mantlik and Philip Aschoff and Florian Steinke and Thomas Beyer and Bernd J Pichler and Bernhard Schölkopf,279,14764597071769129465,Journal of Nuclear Medicine,9,1392-1399,Society of Nuclear Medicine,MRI-based attenuation correction for whole-body PET/MRI: quantitative evaluation of segmentation-and atlas-based methods,http://jnm.snmjournals.org/content/52/9/1392.short,52,2011,/scholar?cites=14764597071769129465,DZ-fHPgAAAAJ:N5tVd3kTz84C
14100,"A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original"" feature map"" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are …",Krikamol Muandet and Kenji Fukumizu and Bharath Sriperumbudur and Bernhard Schölkopf,275,15109477502432478649,arXiv preprint arXiv:1605.09522,,,,Kernel mean embedding of distributions: A review and beyond,https://arxiv.org/abs/1605.09522,,2016,/scholar?cites=15109477502432478649,DZ-fHPgAAAAJ:IzSxq8zsU60C
14101,"Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from"" What is the right fairness criterion?"" to"" What do we want to assume about our model of the causal data generating process?"" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.",Niki Kilbertus and Mateo Rojas Carulla and Giambattista Parascandolo and Moritz Hardt and Dominik Janzing and Bernhard Schölkopf,273,1456063515469711783,,,656-666,,Avoiding discrimination through causal reasoning,http://papers.nips.cc/paper/6668-avoiding-discrimination-through-causal-reasoning,,2017,/scholar?cites=1456063515469711783,DZ-fHPgAAAAJ:3U-WyKnLcZ0C
14102,"The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X,Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CAUSEEFFECTPAIRS that consists of data for 100 different causee ffect pairs …",Joris M Mooij and Jonas Peters and Dominik Janzing and Jakob Zscheischler and Bernhard Schölkopf,262,16524786335970421197,The Journal of Machine Learning Research,1,1103-1204,JMLR. org,Distinguishing cause from effect using observational data: methods and benchmarks,https://dl.acm.org/doi/abs/10.5555/2946645.2946677,17,2016,/scholar?cites=16524786335970421197,DZ-fHPgAAAAJ:pdtoNNNhT-IC
14103,"This paper addresses the bottom-up influence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear filters, eg, Gabor or Difference-of-Gaussians filters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end filters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justified. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble findings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive fields in the early human visual system.",Wolf Kienzle and Felix A Wichmann and Matthias O Franz and Bernhard Schölkopf,258,6944591056873121293,,,689-696,,A nonparametric approach to bottom-up visual saliency,http://papers.nips.cc/paper/3122-a-nonparametric-approach-to-bottom-up-visual-saliency.pdf,,2007,/scholar?cites=6944591056873121293,DZ-fHPgAAAAJ:BzfGm06jWhQC
14104,"We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.",Jonas Peters and Joris M Mooij and Dominik Janzing and Bernhard Schölkopf,249,10028053959358189251,The Journal of Machine Learning Research,1,2009-2053,JMLR. org,Causal discovery with continuous additive noise models,https://dl.acm.org/doi/abs/10.5555/2627435.2670315,15,2014,/scholar?cites=10028053959358189251,DZ-fHPgAAAAJ:oldoQiaHq2UC
14105," Motivation: Support vector machines (SVMs) have been successfully used to classify proteins into functional categories. Recently, to integrate multiple data sources, a semidefinite programming (SDP) based SVM method was introduced. In SDP/SVM, multiple kernel matrices corresponding to each of data sources are combined with weights obtained by solving an SDP. However, when trying to apply SDP/SVM to large problems, the computational cost can become prohibitive, since both converting the data to a kernel matrix for the SVM and solving the SDP are time and memory demanding. Another application-specific drawback arises when some of the data sources are protein networks. A common method of converting the network to a kernel matrix is the diffusion kernel method, which has time complexity of O(n3), and produces a dense matrix of size n × n. Results: We propose an efficient …",Koji Tsuda and Hyunjung Shin and Bernhard Schölkopf,248,5098703665678798852,Bioinformatics,suppl_2,ii59-ii65,Oxford University Press,Fast protein classification with multiple networks,https://academic.oup.com/bioinformatics/article-abstract/21/suppl_2/ii59/227047,21,2005,/scholar?cites=5098703665678798852,DZ-fHPgAAAAJ:PVgj2kMGcgYC
14106,"A method is described which, like the kernel trick in support vector machines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too. As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms.",Bernhard Scholkopf,245,17245200235930215510,Advances in neural information processing systems,,301-307,MIT; 1998,The kernel trick for distances,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.996.235&rep=rep1&type=pdf,,2001,/scholar?cites=17245200235930215510,DZ-fHPgAAAAJ:MWd7jCxOPVUC
14107,"In hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials, where the mixture coefficients correspond to the abundances of the constituting materials. We assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic MAP framework for analyzing hyperspectral data. As the material reflectance characteristics are not know a priori, we face the problem of unsupervised linear unmixing. The incorporation of different prior information (eg positivity and normalization of the abundances) naturally leads to a family of interesting algorithms, for example in the noise-free case yielding an algorithm that can be understood as constrained independent component analysis (ICA). Simulations underline the usefulness of our theory.",Sebastian Mika and Gunnar Rätsch and Jason Weston and Bernhard Schölkopf and Alex J Smola and Klaus-Robert Müller,245,5821024459167679480,,,526-532,,Invariant feature extraction and classification in kernel spaces,http://papers.nips.cc/paper/1715-invariant-feature-extraction-and-classification-in-kernel-spaces.pdf,,2000,/scholar?cites=5821024459167679480,DZ-fHPgAAAAJ:vDZJ-YLwNdEC
14108,"We incorporate prior knowledge to construct nonlinear algorithms for invariant feature extraction and discrimination. Employing a unified framework in terms of a nonlinearized variant of the Rayleigh coefficient, we propose nonlinear generalizations of Fisher's discriminant and oriented PCA using support vector kernel functions. Extensive simulations show the utility of our approach.",S Mika and G Ratsch and J Weston and B Scholkopf and A Smola and KR Muller,243,10601545773333200435,IEEE Transactions on Pattern Analysis and Machine Intelligence,5,623-627,[New York] IEEE Computer Society.,Constructing Descriptive and Discriminative Nonlinear Features: Rayleigh Coefficients in Kernel Feature Spaces,https://ieeexplore.ieee.org/abstract/document/1195996/,25,2003,/scholar?cites=10601545773333200435,DZ-fHPgAAAAJ:sA9dB-pw3HoC
14109,"Kernel methods, a new generation of learning algorithms, utilize techniques from optimization, statistics, and functional analysis to achieve maximal generality, flexibility, and performance. These algorithms are different from earlier techniques used in machine learning in many respects: For example, they are explicitly based on a theoretical model of learning rather than on loose analogies with natural learning systems or other heuristics. They come with theoretical guarantees about their performance and have a modular design that makes it possible to separately implement and analyze their components. They are not affected by the problem of local minima because their training amounts to convex optimization. In the last decade, a sizable community of theoreticians and practitioners has formed around these methods, and a number of practical applications have been realized. Although the research is not concluded, already now kernel methods are considered the state of the art in several machine learning tasks. Their ease of use, theoretical appeal, and remarkable performance have made them the system of choice for many learning problems. Successful applications range from text categorization to handwriting recognition to classification of geneexpression data.",Nello Cristianini and Bernhard Scholkopf,242,14264402101428808150,Ai Magazine,3,31-31,,Support vector machines and kernel methods: the new generation of learning machines,https://www.aaai.org/ojs/index.php/aimagazine/article/view/1655,23,2002,/scholar?cites=14264402101428808150,DZ-fHPgAAAAJ:Dem6FJhTUoYC
14110,,Klaus Robert Muller and A Smola and Gunnar Ratsch and Bernhard Scholkopf and Jens Kohlmorgen and Vladimir Vapnik,242,8419815350879449841,Advances in kernel methods: Support vector learning,,,Massachusetts: MIT Press,Using support vector machines for time series prediction,https://dl.acm.org/doi/abs/10.5555/299094.299107,253,1999,/scholar?cites=8419815350879449841,DZ-fHPgAAAAJ:AXkvAH5U_nMC
14111,"This paper describes an algorithm for finding faces within an image. The basis of the algorithm is to run an observation window at all possible positions, scales and orientation within the image. A non-linear support vector machine is used to determine whether or not a face is contained within the observation window. The non-linear support vector machine operates by comparing the input patch to a set of support vectors (which can be thought of as face and anti-face templates). Each support vector is scored by some nonlinear function against the observation window and if the resulting sum is over some threshold a face is indicated. Because of the huge search space that is considered, it is imperative to investigate ways to speed up the support vector machine. Within this paper we suggest a method of speeding up the non-linear support vector machine. A set of reduced set vectors (RVs) are calculated from the …",Sami Romdhani and Philip Torr and Bernhard Scholkopf and Andrew Blake,241,9789167916180594572,,,695-700,IEEE,Computationally efficient face detection,https://ieeexplore.ieee.org/abstract/document/937694/,2,2001,/scholar?cites=9789167916180594572,DZ-fHPgAAAAJ:WC9gN4BGCRcC
14112,"Given a directed graph in which some of the nodes are labeled, we investigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes. To that extent we propose a regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs. A powerful, yet computationally simple classification algorithm is derived within the proposed framework. The experimental evaluation on real-world Web classification problems demonstrates encouraging results that validate our approach.",Dengyong Zhou and Thomas Hofmann and Bernhard Schölkopf,239,4129333878294426633,Advances in neural information processing systems,,1633-1640,,Semi-supervised learning on directed graphs,http://papers.nips.cc/paper/2718-semi-supervised-learning-on-directed-graphs.pdf,17,2004,/scholar?cites=4129333878294426633,DZ-fHPgAAAAJ:Ak0FvsSvgGUC
14113,"Given two probability measures,  and  defined on a measurable space, , the integral probability metric (IPM) is defined as $$\gamma_ {\EuScript {F}}(\mathbb {P},\mathbb {Q})=\sup\left\{\left\vert\int_ {S} f\, d\mathbb {P}-\int_ {S} f\, d\mathbb {Q}\right\vert\,:\, f\in\EuScript {F}\right\}, $$ where $\EuScript {F} $ is a class of real-valued bounded measurable functions on . By appropriately choosing $\EuScript {F} $, various popular distances between  and , including the Kantorovich metric, Fortet-Mourier metric, dual-bounded Lipschitz distance (also called the Dudley metric), total variation distance, and kernel distance, can be obtained.",Bharath K Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Gert RG Lanckriet,235,9854276169476787826,Electronic Journal of Statistics,,1550-1599,The Institute of Mathematical Statistics and the Bernoulli Society,On the empirical estimation of integral probability metrics,https://projecteuclid.org/euclid.ejs/1347974672,6,2012,/scholar?cites=9854276169476787826,DZ-fHPgAAAAJ:JWITY9-sCbMC
14114,"If a piece of information is released from a set of media sites, can it spread, in 1 month, to a million web pages? Can we efficiently find a small set of media sites among millions that can maximize the spread of the information, in 1 month? The two problems are called influence estimation and maximization problems respectively, which are very challenging since both the time-sensitive nature of the problems and the issue of scalability need to be addressed simultaneously. In this article, we propose two algorithms for influence estimation in continuous-time diffusion networks. The first one uses continuous-time Markov chains to estimate influence exactly on networks with exponential, or, more generally, phase-type transmission functions, but does not scale to large-scale networks, and the second one is a highly efficient randomized algorithm, which estimates the influence of every node in a network with general …",Manuel Gomez-Rodriguez and Le Song and Nan Du and Hongyuan Zha and Bernhard Schölkopf,230,10015264296941634215,ACM Transactions on Information Systems (TOIS),2,1-33,ACM,Influence estimation and maximization in continuous-time diffusion networks,https://dl.acm.org/doi/abs/10.1145/2824253,34,2016,/scholar?cites=10015264296941634215,DZ-fHPgAAAAJ:ZqE1mSdD_DYC
14115,"The data in many real-world problems can be thought of as a graph, such as the web, co-author networks, and biological networks. We propose a general regularization framework on graphs, which is applicable to the classification, ranking, and link prediction problems. We also show that the method can be explained as lazy random walks. We evaluate the method on a number of experiments.",Dengyong Zhou and Bernhard Schölkopf,230,9151338305694826475,,,132-137,,A regularization framework for learning from graph data,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792131,,2004,/scholar?cites=9151338305694826475,DZ-fHPgAAAAJ:48xauSegjOkC
14116,"We briefly describe the main ideas of statistical learning theory, support vector machines, and kernel feature spaces.",Bernhard Schölkopf,229,3626497629103995283,,,3-24,"Springer, Vienna",Statistical learning and kernel methods,https://link.springer.com/content/pdf/10.1007/978-3-7091-2580-9_1.pdf,,2001,/scholar?cites=3626497629103995283,DZ-fHPgAAAAJ:5icHVeHT4IsC
14117,"Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant nonblind deconvolution. Currently, the most successful methods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step approach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.",Christian J Schuler and Harold Christopher Burger and Stefan Harmeling and Bernhard Scholkopf,226,5588202582794678724,,,1067-1074,,A machine learning approach for non-blind image deconvolution,https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Schuler_A_Machine_Learning_2013_CVPR_paper.html,,2013,/scholar?cites=5588202582794678724,DZ-fHPgAAAAJ:uUvzmPk0f8oC
14118,"We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.",Bernhard Schölkopf and Dominik Janzing and Jonas Peters and Eleni Sgouritsa and Kun Zhang and Joris Mooij,225,18084781963285290137,ICML 2012; arXiv preprint arXiv:1206.6471,,,,On causal and anticausal learning,https://arxiv.org/abs/1206.6471,,2012,/scholar?cites=18084781963285290137,DZ-fHPgAAAAJ:rbGdIwl2e6cC
14119,"During the last ten years there has been growing interest in the development of Brain Computer Interfaces (BCIs). The field has mainly been driven by the needs of completely paralyzed patients to communicate. With a few exceptions, most human BCIs are based on extracranial electroencephalography (EEG). However, reported bit rates are still low. One reason for this is the low signal-to-noise ratio of the EEG [16]. We are currently investigating if BCIs based on electrocorticography (ECoG) are a viable alternative. In this paper we present the method and examples of intracranial EEG recordings of three epilepsy patients with electrode grids placed on the motor cortex. The patients were asked to repeatedly imagine movements of two kinds, eg, tongue or finger movements. We analyze the classifiability of the data using Support Vector Machines (SVMs)[18, 21] and Recursive Channel Elimination (RCE)[11].",Thomas Lal and Thilo Hinterberger and Guido Widman and Michael Schröder and N Hill and Wolfgang Rosenstiel and Christian Elger and Niels Birbaumer and Bernhard Schölkopf,224,15199184578941559148,Advances in neural information processing systems,,737-744,,Methods towards invasive human brain computer interfaces,https://papers.nips.cc/paper/2004/file/98b418276d571e623651fc1d471c7811-Paper.pdf,17,2004,/scholar?cites=15199184578941559148,DZ-fHPgAAAAJ:sJsF-0ZLhtgC
14120,"We present a reduction framework from ordinal regression to binary classification based on extended examples. The framework consists of three steps: extracting extended examples from the original examples, learning a binary classifier on the extended examples with any binary classification algorithm, and constructing a ranking rule from the binary classifier. A weighted 0/1 loss of the binary classifier would then bound the mislabeling cost of the ranking rule. Our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classification approaches, but also to derive new generalization bounds for ordinal regression from known bounds for binary classification. In addition, our framework unifies many existing ordinal regression algorithms, such as perceptron ranking and support vector ordinal regression. When compared empirically on benchmark data sets, some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms, which demonstrates the usefulness of our framework.",Ling Li and Hsuan-Tien Lin,223,11441835801378787083,Advances in neural information processing systems,,865-872,,Ordinal regression by extended binary classification,https://proceedings.neurips.cc/paper/2006/file/019f8b946a256d9357eadc5ace2c8678-Paper.pdf,19,2006,/scholar?cites=11441835801378787083,DZ-fHPgAAAAJ:jToNXmjF0wsC
14121,"We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.",Ling Huang and XuanLong Nguyen and Minos Garofalakis and Michael Jordan and Anthony Joseph and Nina Taft,220,8452690320510908758,Advances in neural information processing systems,,617-624,,In-network PCA and anomaly detection,https://proceedings.neurips.cc/paper/2006/file/2227d753dc18505031869d44673728e2-Paper.pdf,19,2006,/scholar?cites=8452690320510908758,DZ-fHPgAAAAJ:TJlB0PdusIwC
14122,"We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like",Dimitris Achlioptas and Frank McSherry and Bernhard Scholkopf,219,11456371914262012725,Advances in Neural Information Processing Systems 14: Proceedings of the 2001 Conference,,335,MIT Press,Sampling techniques for kernel methods,http://papers.nips.cc/paper/2072-sampling-techniques-for-kernel-methods.pdf,1,2002,/scholar?cites=11456371914262012725,DZ-fHPgAAAAJ:F1b5ZUV5XREC
14123,"A new algorithm for Support Vector regression is described. For a priori chosen 1/, it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction 1/of the data points lie outside. Moreover, it is shown how to use parametric tube shapes with non-constant radius. The algorithm is analysed theoretically and experimentally.",Bernhard Schölkopf and Peter L Bartlett and Alex J Smola and Robert C Williamson,219,12300534741946504809,,,330-336,,Shrinking the tube: a new support vector regression algorithm,http://papers.nips.cc/paper/1563-shrinking-the-tube-a-new-support-vector-regression-algorithm.pdf,,1999,/scholar?cites=12300534741946504809,DZ-fHPgAAAAJ:jFemdcug13IC
14124,"Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data.",David Lopez-Paz and Léon Bottou and Bernhard Schölkopf and Vladimir Vapnik,218,13234992542235283122,arXiv preprint arXiv:1511.03643,,,,Unifying distillation and privileged information,https://arxiv.org/abs/1511.03643,,2015,/scholar?cites=13234992542235283122,DZ-fHPgAAAAJ:UI5LpD2ufboC
14125,"The performance of brain-computer interfaces (BCIs) improves with the amount of available training data; the statistical distribution of this data, however, varies across subjects as well as across sessions within individual subjects, limiting the transferability of training data or trained models between them. In this article, we review current transfer learning techniques in BCIs that exploit shared structure between training data of multiple subjects and/or sessions to increase performance. We then present a framework for transfer learning in the context of BCIs that can be applied to any arbitrary feature space, as well as a novel regression estimation method that is specifically designed for the structure of a system based on the electroencephalogram (EEG). We demonstrate the utility of our framework and method on subject-to-subject transfer in a motor-imagery paradigm as well as on session-to-session transfer in one …",Vinay Jayaram and Morteza Alamgir and Yasemin Altun and Bernhard Scholkopf and Moritz Grosse-Wentrup,215,15439496734458590377,IEEE Computational Intelligence Magazine,1,20-31,IEEE,Transfer learning in brain-computer interfaces,https://ieeexplore.ieee.org/abstract/document/7379089/,11,2016,/scholar?cites=15439496734458590377,DZ-fHPgAAAAJ:k1VWYO0b_yEC
14126,"Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention. In this paper we propose an algorithm for constructing a kernel Fisher discriminant (KFD) from training examples with noisy labels. The approach allows to associate with each example a probability of the label being flipped. We utilise an expectation maximization (EM) algorithm for updating the probabilities. The E-step uses class conditional probabilities estimated as a by-product of the KFD algorithm. The M-step updates the flip probabilities and determines the parameters of the discriminant. We demonstrate the feasibility of the approach on two real-world data-sets.",N Lawrence and Bernhard Schölkopf,214,16682440648983721561,,,306-306,Morgan Kaufmann,Estimating a kernel fisher discriminant in the presence of label noise,https://pure.mpg.de/rest/items/item_1793320/component/file_3193444/content,,2001,/scholar?cites=16682440648983721561,DZ-fHPgAAAAJ:-jrNzM816MMC
14127,,Bernhard Schölkopf and Alexander Smola,213,7114376875609316267,"Regularization, Optimization, and beyond",,,MIT press,Learning with kernels: Support vector machines,http://scholar.google.com/scholar?cluster=7114376875609316267&hl=en&oi=scholarr,,2002,/scholar?cites=7114376875609316267,DZ-fHPgAAAAJ:AgSL51lPrA4C
14128,"Ultimately being motivated by facilitating space-variant blind deconvolution, we present a class of linear transformations, that are expressive enough for space-variant filters, but at the same time especially designed for efficient matrix-vector-multiplications. Successful results on astronomical imaging through atmospheric turbulences and on noisy magnetic resonance images of constantly moving objects demonstrate the practical significance of our approach.",Michael Hirsch and Suvrit Sra and Bernhard Schölkopf and Stefan Harmeling,209,3735366258557430132,,,607-614,IEEE,Efficient filter flow for space-variant multiframe blind deconvolution,https://ieeexplore.ieee.org/abstract/document/5540158/,,2010,/scholar?cites=3735366258557430132,DZ-fHPgAAAAJ:O0nohqN1r9EC
14129,"Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",Soren Sonnenburg and Mikio L Braun and Cheng Soon Ong and Samy Bengio and Leon Bottou and Geoffrey Holmes and Yann LeCun and Klaus-Robert Mueller and Fernando Pereira and Carl E Rasmussen and Gunnar Raetsch and Bernhard Schoelkopf and Alexander Smola and Pascal Vincent and Jason Weston and Robert Williamson,209,5590990338775951504,Journal of Machine Learning Research,,2443-2466,The Australian National University,The need for open source software in machine learning,http://www.jmlr.org/papers/v8/sonnenburg07a,,2007,/scholar?cites=5590990338775951504,DZ-fHPgAAAAJ:tzM49s52ZIMC
14130,"We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images.",Jason Weston and Olivier Chapelle and Vladimir Vapnik and André Elisseeff and Bernhard Schölkopf,208,541411120474363509,Advances in neural information processing systems,,897-904,,Kernel dependency estimation,https://papers.nips.cc/paper/2297-kernel-dependency-estimation.pdf,15,2002,/scholar?cites=541411120474363509,DZ-fHPgAAAAJ:KaMxkj08jr0C
14131,"While conventional approaches to causal inference are mainly based on conditional (in) dependences, recent methods also account for the shape of (conditional) distributions. The idea is that the causal hypothesis “X causes Y” imposes that the marginal distribution P X and the conditional distribution P Y| X represent independent mechanisms of nature. Recently it has been postulated that the shortest description of the joint distribution P X, Y should therefore be given by separate descriptions of P X and P Y| X. Since description length in the sense of Kolmogorov complexity is uncomputable, practical implementations rely on other notions of independence. Here we define independence via orthogonality in information space. This way, we can explicitly describe the kind of dependence that occurs between P Y and P X| Y making the causal hypothesis “Y causes X” implausible. Remarkably, this asymmetry between …",Dominik Janzing and Joris Mooij and Kun Zhang and Jan Lemeire and Jakob Zscheischler and Povilas Daniušis and Bastian Steudel and Bernhard Schölkopf,207,13553791818361972071,Artificial Intelligence,,1-31,Elsevier,Information-geometric approach to inferring causal directions,https://www.sciencedirect.com/science/article/pii/S0004370212000045,182,2012,/scholar?cites=13553791818361972071,DZ-fHPgAAAAJ:YlPif8NxrbYC
14132,"This article presents a scheme for learning a cognitive map of a maze from a sequence of views and movement decisions. The scheme is based on an intermediate representation called the view graph, whose nodes correspond to the views whereas the labeled edges represent the movements leading from one view to another. By means of a graph theoretical reconstruction method, the view graph is shown to carry complete information on the topological and directional structure of the maze. Path planning can be carried out directly in the view graph without actually performing this reconstruction. A neural network is presented that learns the view graph during a random exploration of the maze. lt is based on an unsupervised competitive learning rule translating temporal sequence (rather than similarity) of views into connectedness in the network. The network uses its knowledge of the topological and directional …",Bernhard Schölkopf and Hanspeter A Mallot,207,1447545358919599032,Adaptive Behavior,3,311-348,Sage Publications,View-based cognitive mapping and path planning,https://journals.sagepub.com/doi/abs/10.1177/105971239500300303,3,1995,/scholar?cites=1447545358919599032,DZ-fHPgAAAAJ:tH6gc1N1XXoC
14133,"We present the results of the Gravitational LEnsing Accuracy Testing 2008 (GREAT08) Challenge, a blind analysis challenge to infer weak gravitational lensing shear distortions from images. The primary goal was to stimulate new ideas by presenting the problem to researchers outside the shear measurement community. Six GREAT08 Team methods were presented at the launch of the Challenge and five additional groups submitted results during the 6-month competition. Participants analyzed 30 million simulated galaxies with a range in signal-to-noise ratio, point spread function ellipticity, galaxy size and galaxy type. The large quantity of simulations allowed shear measurement methods to be assessed at a level of accuracy suitable for currently planned future cosmic shear observations for the first time. Different methods perform well in different parts of simulation parameter space and come close to the …",Sarah Bridle and Sreekumar T Balan and Matthias Bethge and Marc Gentile and Stefan Harmeling and Catherine Heymans and Michael Hirsch and Reshad Hosseini and Mike Jarvis and Donnacha Kirk and Thomas Kitching and Konrad Kuijken and Antony Lewis and Stephane Paulin-Henriksson and Bernhard Schölkopf and Malin Velander and Lisa Voigt and Dugan Witherick and Adam Amara and Gary Bernstein and Frédéric Courbin and Mandeep Gill and Alan Heavens and Rachel Mandelbaum and Richard Massey and Baback Moghaddam and Anais Rassat and Alexandre Réfrégier and Jason Rhodes and Tim Schrabback and John Shawe-Taylor and Marina Shmakova and Ludovic van Waerbeke and David Wittman,206,17783036595304629905,Monthly Notices of the Royal Astronomical Society,3,2044-2061,Blackwell Publishing Ltd,Results of the GREAT08 Challenge: an image analysis competition for cosmological lensing,https://academic.oup.com/mnras/article-abstract/405/3/2044/967782,405,2010,/scholar?cites=17783036595304629905,DZ-fHPgAAAAJ:XUvXOeBm_78C
14134,"We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.",Gökhan H Bakır and Jason Weston and Bernhard Schölkopf,201,9801281826406716563,Advances in Neural Information Processing Systems,,449-456,,Learning to find pre-images,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.5164&rep=rep1&type=pdf,16,2004,/scholar?cites=9801281826406716563,DZ-fHPgAAAAJ:XvxMoLDsR5gC
14135,"We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite-dimensional unit ball in feature space into a finite-dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence, we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines.",Robert C. Williamson and Alex J. Smola and Bernhard Scholkopf,198,2716288985722493017,IEEE Transactions on Information Theory,6,2516-2532,IEEE,Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators,https://ieeexplore.ieee.org/abstract/document/945262/,47,2001,/scholar?cites=2716288985722493017,DZ-fHPgAAAAJ:SjuI4pbJlxcC
14136,"In Support Vector (SV) regression, a parameter ν controls the number of Support Vectors and the number of points that come to lie outside of the so-called ε-insensitive tube. For various noise models and SV parameter settings, we experimentally determine the values of ν that lead to the lowest generalization error. We find good agreement with the values that had previously been predicted by a theoretical argument based on the asymptotic efficiency of a simplified model of SV regression. As a side effect of the experiments, valuable information about the generalization behavior of the remaining SVM parameters and their dependencies is gained. The experimental findings are valid even for complex ‘real-world’ data sets. Based on our results on the role of the ν-SVM parameters, we discuss various model selection methods.",Athanassia Chalimourda and Bernhard Schölkopf and Alex J Smola,193,7699418085077655568,Neural Networks,1,127-141,Pergamon,Experimentally optimal ν in support vector regression for different noise models and parameter settings,https://www.sciencedirect.com/science/article/pii/S0893608003002090,17,2004,/scholar?cites=7699418085077655568,DZ-fHPgAAAAJ:WMtz-WDmgKQC
14137,,Athanassia Chalimourda and Bernhard Schölkopf and Alex J Smola,192,7699418085077655568,Neural Networks,2,205,Elsevier Science Ltd.,Letter to the editor: Experimentally optimal ν in support vector regression for different noise models and parameter settings,,18,2005,/scholar?cites=7699418085077655568,DZ-fHPgAAAAJ:BrmTIyaxlBUC
14138,"We consider the classification problem on a finite set of objects. Some of them are labeled, and the task is to predict the labels of the remaining unlabeled ones. Such an estimation problem is generally referred to as transductive inference. It is well-known that many meaningful inductive or supervised methods can be derived from a regularization framework, which minimizes a loss function plus a regularization term. In the same spirit, we propose a general discrete regularization framework defined on finite object sets, which can be thought of as discrete analogue of classical regularization theory. A family of transductive inference schemes is then systemically derived from the framework, including our earlier algorithm for transductive inference, with which we obtained encouraging results on many practical classification problems. The discrete regularization framework is built on discrete analysis and …",Dengyong Zhou and Bernhard Scholkopf,183,13442526711258398119,Lecture Notes in Computer Science (Proceedings DAGM),,361-368,Berlin: Springer-Verlag,Regularization on Discrete Spaces,https://link.springer.com/chapter/10.1007/11550518_45,,2005,/scholar?cites=13442526711258398119,DZ-fHPgAAAAJ:JTqpx9DYBaYC
14139,"The combination of brain–computer interfaces (BCIs) with robot-assisted physical therapy constitutes a promising approach to neurorehabilitation of patients with severe hemiparetic syndromes caused by cerebrovascular brain damage (eg stroke) and other neurological conditions. In such a scenario, a key aspect is how to reestablish the disrupted sensorimotor feedback loop. However, to date it is an open question how artificially closing the sensorimotor feedback loop influences the decoding performance of a BCI. In this paper, we answer this issue by studying six healthy subjects and two stroke patients. We present empirical evidence that haptic feedback, provided by a seven degrees of freedom robotic arm, facilitates online decoding of arm movement intention. The results support the feasibility of future rehabilitative treatments based on the combination of robot-assisted physical therapy with BCIs.",Manuel Gomez-Rodriguez and Jan Peters and Jeremy Hill and Bernhard Schölkopf and Alireza Gharabaghi and Moritz Grosse-Wentrup,182,2570058034573436473,Journal of neural engineering,3,036005,IOP Publishing,Closing the sensorimotor loop: haptic feedback facilitates decoding of motor imagery,https://iopscience.iop.org/article/10.1088/1741-2560/8/3/036005/meta,8,2011,/scholar?cites=2570058034573436473,DZ-fHPgAAAAJ:iPtaxXGL5a8C
14140,,M Gomez-Rodriguez and Jan Peters and Jeremy Hill and B Schölkopf and A Gharabaghi and M Grosse-Wentrup,182,2570058034573436473,,,121-126,IEEE,Closing the sensorimotor loop: Haptic feedback facilitates decoding of arm movement imagery,,,2010,/scholar?cites=2570058034573436473,DZ-fHPgAAAAJ:kzcSZmkxUKAC
14141,"We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.",Peter Auer and Ronald Ortner,178,11014111024389052482,,,49-56,,Logarithmic online regret bounds for undiscounted reinforcement learning,http://papers.nips.cc/paper/3052-logarithmic-online-regret-bounds-for-undiscounted-reinforcement-learning.pdf,,2007,/scholar?cites=11014111024389052482,DZ-fHPgAAAAJ:9l_67PIJ4zYC
14142,"We aim to color greyscale images automatically, without any manual intervention. The color proposition could then be interactively corrected by user-provided color landmarks if necessary. Automatic colorization is nontrivial since there is usually no one-to-one correspondence between color and local texture. The contribution of our framework is that we deal directly with multimodality and estimate, for each pixel of the image to be colored, the probability distribution of all possible colors, instead of choosing the most probable color at the local level. We also predict the expected variation of color at each pixel, thus defining a non-uniform spatial coherency criterion. We then use graph cuts to maximize the probability of the whole colored image at the global level. We work in the L-a-b color space in order to approximate the human perception of distances between colors, and we use machine learning tools to …",Guillaume Charpiat and Matthias Hofmann and Bernhard Schölkopf,177,10229273743261098362,,,126-139,"Springer, Berlin, Heidelberg",Automatic image colorization via multimodal predictions,https://link.springer.com/chapter/10.1007/978-3-540-88690-7_10,,2008,/scholar?cites=10229273743261098362,DZ-fHPgAAAAJ:nb7KW1ujOQ8C
14143,"The human visual system is foveated, that is, outside the central visual field resolution and acuity drop rapidly. Nonetheless much of a visual scene is perceived after only a few saccadic eye movements, suggesting an effective strategy for selecting saccade targets. It has been known for some time that local image structure at saccade targets influences the selection process. However, the question of what the most relevant visual features are is still under debate. Here we show that center-surround patterns emerge as the optimal solution for predicting saccade targets from their local image structure. The resulting model, a one-layer feed-forward network, is surprisingly simple compared to previously suggested models which assume much more complex computations such as multi-scale processing and multiple feature channels. Nevertheless, our model is equally predictive. Furthermore, our findings are consistent with neurophysiological hardware in the superior colliculus. Bottom-up visual saliency may thus not be computed cortically as has been thought previously.",Wolf Kienzle and Matthias O Franz and Bernhard Schölkopf and Felix A Wichmann,176,18405171263410989356,Journal of vision,5,7-7,The Association for Research in Vision and Ophthalmology,Center-surround patterns emerge as optimal predictors for human saccade targets,https://jov.arvojournals.org/article.aspx?articleid=2122672,9,2009,/scholar?cites=18405171263410989356,DZ-fHPgAAAAJ:vDijr-p_gm4C
14144,"A new algorithm for Support Vector regression is proposed. For a priori chosen ν, it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction ν of the data points lie outside. The algorithm is analysed theoretically and experimentally.",B Schölkopf and P Bartlett and A Smola and R Williamson,170,12575378281287423563,,,111-116,"Springer, London",Support vector regression with automatic accuracy control,https://link.springer.com/chapter/10.1007/978-1-4471-1599-1_12,,1998,/scholar?cites=12575378281287423563,DZ-fHPgAAAAJ:LgRImbQfgY4C
14145,Under the assumption of asymptotically unbiased estimators we show that there exists a nontrivial choice of the insensitivity parameter in Vapnik’s ε-insensitive loss function which scales linearly with the input noise of the training data. This finding is backed by experimental results.,AJ Smola and N Murata and B Schölkopf and K-R Müller,168,386562422944204081,,,105-110,"Springer, London",Asymptotically optimal choice of ε-loss for support vector machines,https://link.springer.com/chapter/10.1007/978-1-4471-1599-1_11,,1998,/scholar?cites=386562422944204081,DZ-fHPgAAAAJ:QD3KBmkZPeQC
14146,"Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation mainly considers the covariate shift situation where the distribution of the features P (X) changes across domains while the conditional distribution P (Y| X) stays the same. To reduce domain discrepancy, recent methods try to find invariant components T (X) that have similar P (T (X)) on different domains by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P (Y| T (X)) in different domains is also similar when P (Y| X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identifiable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P (X| Y) and P (Y) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P (T (X)| Y) is invariant after proper location-scale (LS) transformations, and identify how P (Y) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method.",Mingming Gong and Kun Zhang and Tongliang Liu and Dacheng Tao and Clark Glymour and Bernhard Schölkopf,167,8690882839274105451,,,2839-2848,,Domain adaptation with conditional transferable components,http://www.jmlr.org/proceedings/papers/v48/gong16.pdf,,2016,/scholar?cites=8690882839274105451,DZ-fHPgAAAAJ:g6z5zl4k3DgC
14147,"Inferring the causal structure that links n observables is usually based upon detecting statistical dependences and choosing simple graphs that make the joint measure Markovian. Here we argue why causal inference is also possible when the sample size is one. We develop a theory how to generate causal graphs explaining similarities between single objects. To this end, we replace the notion of conditional stochastic independence in the causal Markov condition with the vanishing of conditional algorithmic mutual information and describe the corresponding causal inference rules. We explain why a consistent reformulation of causal inference in terms of algorithmic complexity implies a new inference principle that takes into account also the complexity of conditional probability densities, making it possible to select among Markov equivalent causal graphs. This insight provides a theoretical foundation of a heuristic …",Dominik Janzing and Bernhard Schölkopf,165,15831736761680150765,IEEE Transactions on Information Theory,10,5168-5194,IEEE,Causal inference using the algorithmic Markov condition,https://ieeexplore.ieee.org/abstract/document/5571886/,56,2010,/scholar?cites=15831736761680150765,DZ-fHPgAAAAJ:HhcuHIWmDEUC
14148,"In a pre-processing step prior to training a learning machine, pre-processing includes reducing the quantity of features to be processed using feature selection methods selected from the group consisting of recursive feature elimination (RFE), minimizing the number of non-zero parameters of the system (l o-norm minimization), evaluation of cost function to identify a subset of features that are compatible with constraints imposed by the learning set, unbalanced correlation score and transductive feature selection. The features remaining after feature selection are then used to train a learning machine for purposes of pattern classification, regression, clustering and/or novelty detection.(FIG. 3, 300, 301, 302, 304, 306, 308, 309, 310, 311, 312, 314)",,165,5973577103290294497,,,,,Methods for feature selection in a learning machine,https://patents.google.com/patent/US7318051B2/en,,2008,/scholar?cites=5973577103290294497,DZ-fHPgAAAAJ:0yVh_IzAmHcC
14149,"Semi-supervised SVMs (S3VM) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples. The associated optimization problem is non-convex. To examine the full potential of S3VMs modulo local minima problems in current implementations, we apply branch and bound techniques for obtaining exact, globally optimal solutions. Empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely. While our current implementation is only applicable to small datasets, we discuss variants that can potentially lead to practically useful algorithms.",Olivier Chapelle and Vikas Sindhwani and S Sathiya Keerthi,164,1666077447838373412,,,217-224,,Branch and bound for semi-supervised support vector machines,http://papers.nips.cc/paper/3135-branch-and-bound-for-semi-supervised-support-vector-machines.pdf,,2007,/scholar?cites=1666077447838373412,DZ-fHPgAAAAJ:GTs858yTtw8C
14150," Motivation: Eukaryotic pre-mRNAs are spliced to form mature mRNA. Pre-mRNA alternative splicing greatly increases the complexity of gene expression. Estimates show that more than half of the human genes and at least one-third of the genes of less complex organisms, such as nematodes or flies, are alternatively spliced. In this work, we consider one major form of alternative splicing, namely the exclusion of exons from the transcript. It has been shown that alternatively spliced exons have certain properties that distinguish them from constitutively spliced exons. Although most recent computational studies on alternative splicing apply only to exons which are conserved among two species, our method only uses information that is available to the splicing machinery, i.e. the DNA sequence itself. We employ advanced machine learning techniques in order to answer the following two questions: (1) Is a …",Gunnar Rätsch and Sören Sonnenburg and Bernhard Schölkopf,164,15880841501795538964,Bioinformatics,suppl_1,i369-i377,Oxford University Press,RASE: recognition of alternatively spliced exons in C.elegans,https://academic.oup.com/bioinformatics/article-abstract/21/suppl_1/i369/203589,21,2005,/scholar?cites=15880841501795538964,DZ-fHPgAAAAJ:4xDN1ZYqzskC
14151,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.",Krikamol Muandet and Kenji Fukumizu and Francesco Dinuzzo and Bernhard Schölkopf,163,8595944455330607478,,,10-18,,Learning from distributions via support measure machines,http://papers.nips.cc/paper/4825-learning-from-distributions-via-support-measure-machines,,2012,/scholar?cites=8595944455330607478,DZ-fHPgAAAAJ:Wq2b2clWBLsC
14152,"Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs for exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that acquires data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels both at each step and in the long run. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to existing one-shot mechanisms.",Wolf Kienzle and Matthias Franz and Bernhard Schölkopf and Gökhan Bakir,163,8662070901448774694,Advances in Neural Information Processing Systems,,673-680,,Face detection---efficient and rank deficient,https://proceedings.neurips.cc/paper/2004/file/f2e43fa3400d826df4195a9ac70dca62-Paper.pdf,17,2004,/scholar?cites=8662070901448774694,DZ-fHPgAAAAJ:otzGkya1bYkC
14153,"To clarify the physiological and behavioral boundaries between locked-in (LIS) and the completely locked-in state (CLIS) (no voluntary eye movements, no communication possible) through electrophysiological data and to secure brain–computer-interface (BCI) communication.Electromyography from facial muscles, external anal sphincter (EAS), electrooculography and electrocorticographic data during different psychophysiological tests were acquired to define electrophysiological differences in an amyotrophic lateral sclerosis (ALS) patient with an intracranially implanted grid of 112 electrodes for nine months while the patient passed from the LIS to the CLIS.At the very end of the LIS there was no facial muscle activity, nor external anal sphincter but eye control. Eye movements were slow and lasted for short periods only. During CLIS event related brain potentials (ERP) to passive limb …",A Ramos Murguialday and J Hill and M Bensch and S Martens and S Halder and Femke Nijboer and Bernhard Schoelkopf and N Birbaumer and A Gharabaghi,160,3151940320315318514,Clinical Neurophysiology,5,925-933,Elsevier,Transition from the locked in to the completely locked-in state: a physiological analysis,https://www.sciencedirect.com/science/article/pii/S1388245710006619,122,2011,/scholar?cites=3151940320315318514,DZ-fHPgAAAAJ:_FM0Bhl9EiAC
14154,"Networks provide a ‘skeleton’for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.",Manuel Gomez-Rodriguez and Jure Leskovec and Bernhard Schölkopf,157,5595909019212232861,,,666-674,,Modeling information propagation with survival theory,http://www.jmlr.org/proceedings/papers/v28/gomez-rodriguez13.pdf,,2013,/scholar?cites=5595909019212232861,DZ-fHPgAAAAJ:sbeIDTyQOFgC
14155,"Hitting and batting tasks, such as tennis forehands, ping-pong strokes, or baseball batting, depend on predictions where the ball can be intercepted and how it can properly be returned to the opponent. These predictions get more accurate over time, hence the behaviors need to be continuously modified. As a result, movement templates with a learned global shape need to be adapted during the execution so that the racket reaches a target position and velocity that will return the ball over to the other side of the net or court. It requires altering learned movements to hit a varying target with the necessary velocity at a specific instant in time. Such a task cannot be incorporated straightforwardly in most movement representations suitable for learning. For example, the standard formulation of the dynamical system based motor primitives (introduced by Ijspeert et al. [1]) does not satisfy this property despite their flexibility …",J. Kober and K. Mulling and O. Krömer and C. H. Lampert and B. Schölkopf and J. Peters,157,3172900771997913811,,,853 - 858,IEEE,Movement templates for learning of hitting and batting,https://ieeexplore.ieee.org/abstract/document/5509672/,,2010,/scholar?cites=3172900771997913811,DZ-fHPgAAAAJ:FiDNX6EVdGUC
14156,"Face images are subject to changes in view and illumination. Such changes cause data distribution to be highly nonlinear and complex in the image space. It is desirable to learn a nonlinear mapping from the image space to a low dimensional space such that the distribution becomes simpler tighter and therefore more predictable for better modeling effaces. In this paper we present a kernel machine based approach for learning such nonlinear mappings. The aim is to provide an effective view-based representation for multi-view face detection and pose estimation. Assuming that the view is partitioned into a number of distinct ranges, one nonlinear view-subspace is learned for each (range of) view from a set of example face images of that view (range), by using kernel principal component analysis (KPCA). Projections of the data onto the view-subspaces are then computed as view-based nonlinear features. Multi …",Stan Z Li and Qingdong Fu and Lie Gu and Bernhard Scholkopf and Yimin Cheng and Hongjiag Zhang,157,3202779535970056868,,,674-679,IEEE,Kernel machine based learning for multi-view face detection and pose estimation,https://ieeexplore.ieee.org/abstract/document/937691/,2,2001,/scholar?cites=3202779535970056868,DZ-fHPgAAAAJ:1Ye0OR6EYb4C
14157,"We study the problem of domain transfer for a supervised classification task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We find that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classification performance.",Gabriele Schweikert and Gunnar Rätsch and Christian Widmer and Bernhard Schölkopf,156,10859304063388549194,Advances in neural information processing systems,,1433-1440,,An empirical analysis of domain adaptation algorithms for genomic sequence analysis,https://papers.nips.cc/paper/2008/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html,21,2008,/scholar?cites=10859304063388549194,DZ-fHPgAAAAJ:kuK5TVdYjLIC
14158,"Information overload has become an ubiquitous problem in modern society. Social media users and microbloggers receive an endless flow of information, often at a rate far higher than their cognitive abilities to process the information. In this paper, we conduct a large scale quantitative study of information overload and evaluate its impact on information dissemination in the Twitter social media site. We model social media users as information processing systems that queue incoming information according to some policies, process information from the queue at some unknown rates and decide to forward some of the incoming information to other users. We show how timestamped data about tweets received and forwarded by users can be used to uncover key properties of their queueing policies and estimate their information processing rates and limits. Such an understanding of users' information processing behaviors allows us to infer whether and to what extent users suffer from information overload.Our analysis provides empirical evidence of information processing limits for social media users and the prevalence of information overloading. The most active and popular social media users are often the ones that are overloaded. Moreover, we find that the rate at which users receive information impacts their processing behavior, including how they prioritize information from different sources, how much information they process, and how quickly they process information. Finally, the susceptibility of a social media user to social contagions depends crucially on the rate at which she receives information. An exposure to a piece of information, be it …",Manuel Gomez Rodriguez and Krishna Gummadi and Bernhard Schoelkopf,155,8214004505925067252,arXiv preprint arXiv:1403.6838,,,,Quantifying information overload in social media and its impact on social contagions,https://arxiv.org/abs/1403.6838,,2014,/scholar?cites=8214004505925067252,DZ-fHPgAAAAJ:h1pkognVyKwC
14159,"Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.",Ilya O Tolstikhin and Sylvain Gelly and Olivier Bousquet and Carl-Johann Simon-Gabriel and Bernhard Schölkopf,154,15958765505735872542,,,5424-5433,,Adagan: Boosting generative models,https://papers.nips.cc/paper/7126-adagan-boosting-generative-models,,2017,/scholar?cites=15958765505735872542,DZ-fHPgAAAAJ:0dL35dZyZCwC
14160,"Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The …",Kenji Fukumizu and Arthur Gretton and Gert Lanckriet and Bernhard Schölkopf and Bharath K Sriperumbudur,154,7143156798799014294,Advances in neural information processing systems,,1750-1758,,Kernel choice and classifiability for RKHS embeddings of probability distributions,https://papers.nips.cc/paper/3750-kernel-choice-and-classifiability-for-rkhs-embeddings-of-probability-distributions.pdf,22,2009,/scholar?cites=7143156798799014294,DZ-fHPgAAAAJ:Ug5p-4gJ2f0C
14161,"The idea of local learning, classifying a particular point based on its neighbors, has been successfully applied to supervised learning problems. In this paper, we adapt it for Transductive Classification (TC) problems. Specifically, we formulate a Local Learning Regularizer (LL-Reg) which leads to a solution with the property that the label of each data point can be well predicted based on its neighbors and their labels. For model selection, an efficient way to compute the leave-one-out classification error is provided for the proposed and related algorithms. Experimental results using several benchmark datasets illustrate the effectiveness of the proposed approach.",Mingrui Wu and Bernhard Schölkopf,153,10739922816297000953,Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,,628-635,,Transductive classification via local learning regularization,http://www.jmlr.org/proceedings/papers/v2/wu07a/wu07a.pdf,,2007,/scholar?cites=10739922816297000953,DZ-fHPgAAAAJ:MpfHP-DdYjUC
14162,"Recent progress has allowed hybrid positron emission tomography/magnetic resonance (PET/MR) systems to make the transition from research prototypes to systems with full potential for clinical imaging. Options for directly measuring the attenuation maps, as is possible with PET/computed tomography or PET transmission scans, are not included in PET/MR scanners. New methods to compute attenuation maps from MR data have therefore been developed.",Ilja Bezrukov and Frédéric Mantlik and Holger Schmidt and Bernhard Schölkopf and Bernd J Pichler,152,9042353522385095128,,1,45-59,WB Saunders,MR-based PET attenuation correction for PET/MR imaging,https://www.sciencedirect.com/science/article/pii/S0001299812000785,43,2013,/scholar?cites=9042353522385095128,DZ-fHPgAAAAJ:nPTYJWkExTIC
14163,"A system and process for creating an interactive digital image, which allows a viewer to interact with a displayed image so as to change it with regard to a desired effect, such as exposure, focus or color, among others. An interactive image includes representative images which depict a scene with some image parameter varying between them. The interactive image also includes an index image, whose pixels each identify the representative image that exhibits the desired effect related to the varied image parameter at a corresponding pixel location. For example, a pixel of the index image might identify the representative image having a correspondingly-located pixel that depicts a portion of the scene at the sharpest focus. One primary form of interaction involves selecting a pixel of a displayed image whereupon the representative image identified in the index image at a corresponding pixel location is displayed in …",,151,11800508752906729801,,,,,Interactive images,https://patents.google.com/patent/US7120293B2/en,,2006,/scholar?cites=11800508752906729801,DZ-fHPgAAAAJ:sNmaIFBj_lkC
14164,"Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale.In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements.",David Lopez-Paz and Suvrit Sra and Alex Smola and Zoubin Ghahramani and Bernhard Schölkopf,150,13835474440204073152,,,1359-1367,,Randomized nonlinear component analysis,http://www.jmlr.org/proceedings/papers/v32/lopez-paz14.pdf,,2014,/scholar?cites=13835474440204073152,DZ-fHPgAAAAJ:XeErXHja3Z8C
14165,"We summarize results from a series of related studies that aim to develop a motor-imagery-based brain-computer interface using a single recording session of electroencephalogram (EEG) or electrocorticogram (ECoG) signals for each subject. We apply the same experimental and analytical methods to 11 nonparalysed subjects (eight EEG, three ECoG), and to five paralyzed subjects (four EEG, one ECoG) who had been unable to communicate for some time. While it was relatively easy to obtain classifiable signals quickly from most of the nonparalyzed subjects, it proved impossible to classify the signals obtained from the paralyzed patients by the same methods. This highlights the fact that though certain BCI paradigms may work well with healthy subjects, this does not necessarily indicate success with the target user group. We outline possible reasons for this failure to transfer.",N Jeremy Hill and Thomas Navin Lal and M Schroder and Thilo Hinterberger and Barbara Wilhelm and Femke Nijboer and Ursula Mochty and Guido Widman and Christian Elger and Bernhard Scholkopf and A Kubler and Niels Birbaumer,149,10453996912376389662,IEEE transactions on neural systems and rehabilitation engineering,2,183-186,IEEE,Classifying EEG and ECoG signals without subject training for fast BCI implementation: comparison of nonparalyzed and completely paralyzed subjects,https://ieeexplore.ieee.org/abstract/document/1642764/,14,2006,/scholar?cites=10453996912376389662,DZ-fHPgAAAAJ:RoXSNcbkSzsC
14166,,Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,148,9408751332424138239,"Cambridge: MIT Press. Cortes, C., & Mohri, M.(2014). Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science",,103126,,"Semi-supervised learning, vol. 2",http://scholar.google.com/scholar?cluster=9408751332424138239&hl=en&oi=scholarr,519,2006,/scholar?cites=9408751332424138239,DZ-fHPgAAAAJ:sxkgGq_6IbsC
14167,"We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.",David Lopez-Paz and Philipp Hennig and Bernhard Schölkopf,147,1533885614443677251,,,1-9,,The randomized dependence coefficient,http://papers.nips.cc/paper/5138-the-randomized-dependence-coefficient,,2013,/scholar?cites=1533885614443677251,DZ-fHPgAAAAJ:idthP5jqfYAC
14168,"We consider the general problem of learning from labeled and unlabeled data. Given a set of points, some of them are labeled, and the remaining points are unlabeled. The goal is to predict the labels of the unlabeled points. Any supervised learning algorithm can be applied to this problem, for instance, Support Vector Machines (SVMs). The problem of our interest is if we can implement a classifier which uses the unlabeled data information in some way and has higher accuracy than the classifiers which use the labeled data only. Recently we proposed a simple algorithm, which can substantially benefit from large amounts of unlabeled data and demonstrates clear superiority to supervised learning methods. Here we further investigate the algorithm using random walks and spectral graph theory, which shed light on the key steps in this algorithm.",Dengyong Zhou and Bernhard Schölkopf,147,11616495546222650972,,,237-244,"Springer, Berlin, Heidelberg",Learning from labeled and unlabeled data using random walks,https://link.springer.com/chapter/10.1007/978-3-540-28649-3_29,,2004,/scholar?cites=11616495546222650972,DZ-fHPgAAAAJ:cK4Rrx0J3m0C
14169,"Algorithms based on Mercer kernels construct their solutions in terms of expansions in a high-dimensional feature space F. Previous work has shown that all algorithms which can be formulated in terms of dot products in F can be performed using a kernel without explicitly working in F. The list of such algorithms includes support vector machines and nonlinear kernel principal component extraction. So far, however, it did not include the reconstruction of patterns from their largest nonlinear principal components, a technique which is common practice in linear principal component analysis.The present work proposes an idea for approximately performing this task. As an illustrative example, an application to the de-noising of data clusters is presented.",Bernhard Schölkopf and Sebastian Mika and Alex Smola and Gunnar Rätsch and Klaus-Robert Müller,147,16700594454090949458,,,147-152,"Springer, London",Kernel PCA Pattern Reconstruction via Approximate Pre-Images,https://link.springer.com/chapter/10.1007/978-1-4471-1599-1_18,,1998,/scholar?cites=16700594454090949458,DZ-fHPgAAAAJ:6yz0xqPARnAC
14170,"Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease with substantial heterogeneity in its clinical presentation. This makes diagnosis and effective treatment difficult, so better tools for estimating disease progression are needed. Here, we report results from the DREAM-Phil Bowen ALS Prediction Prize4Life challenge. In this crowdsourcing competition, competitors developed algorithms for the prediction of disease progression of 1,822 ALS patients from standardized, anonymized phase 2/3 clinical trials. The two best algorithms outperformed a method designed by the challenge organizers as well as predictions by ALS clinicians. We estimate that using both winning algorithms in future trial designs could reduce the required number of patients by at least 20%. The DREAM-Phil Bowen ALS Prediction Prize4Life challenge also identified several potential nonstandard predictors of disease …",Robert Küffner and Neta Zach and Raquel Norel and Johann Hawe and David Schoenfeld and Liuxia Wang and Guang Li and Lilly Fang and Lester Mackey and Orla Hardiman and Merit Cudkowicz and Alexander Sherman and Gokhan Ertaylan and Moritz Grosse-Wentrup and Torsten Hothorn and Jules Van Ligtenberg and Jakob H Macke and Timm Meyer and Bernhard Schölkopf and Linh Tran and Rubio Vaughan and Gustavo Stolovitzky and Melanie L Leitner,144,14282846863988917456,Nature biotechnology,1,51-57,Nature Publishing Group,Crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression,https://www.nature.com/nbt/journal/v33/n1/abs/nbt.3051.html,33,2015,/scholar?cites=14282846863988917456,DZ-fHPgAAAAJ:mtdGyXoswmMC
14171,"A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). The embedding function has been proven to be injective when the reproducing kernel is universal. In this case, the embedding induces a metric on the space of probability distributions defined on compact metric spaces. In the present work, we consider more broadly the problem of specifying characteristic kernels, defined as kernels for which the RKHS embedding of probability measures is injective. In particular, characteristic kernels can include non-universal kernels. We restrict ourselves to translation-invariant kernels on Euclidean space, and define the associated metric on probability measures in terms of the Fourier spectrum of the kernel and characteristic functions of these measures. The support of the kernel spectrum is important in finding whether a kernel is characteristic: in particular, the embedding is injective if and only if the kernel spectrum has the entire domain as its support. Characteristic kernels may nonetheless have difficulty in distinguishing certain distributions on the basis of finite samples, again due to the interaction of the kernel spectrum and the characteristic functions of the measures.",Bharath K Sriperumbudur and Arthur Gretton and Kenji Fukumizu and Gert Lanckriet and Bernhard Schölkopf,144,12633268718077183016,,,111-122,Omnipress,Injective Hilbert space embeddings of probability measures,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1789782,,2008,/scholar?cites=12633268718077183016,DZ-fHPgAAAAJ:FiytvqdAVhgC
14172,"Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between  variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other  variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question.",Dominik Janzing and David Balduzzi and Moritz Grosse-Wentrup and Bernhard Schölkopf,142,5550368645723853781,The Annals of Statistics,5,2324-2358,Institute of Mathematical Statistics,Quantifying causal influences,https://projecteuclid.org/euclid.aos/1383661266,41,2013,/scholar?cites=5550368645723853781,DZ-fHPgAAAAJ:c1e4I3QdEKYC
14173,"Volterra and Wiener series are perhaps the best-understood nonlinear system representations in signal processing. Although both approaches have enjoyed a certain popularity in the past, their application has been limited to rather low-dimensional and weakly nonlinear systems due to the exponential growth of the number of terms that have to be estimated. We show that Volterra and Wiener series can be represented implicitly as elements of a reproducing kernel Hilbert space by using polynomial kernels. The estimation complexity of the implicit representation is linear in the input dimensionality and independent of the degree of nonlinearity. Experiments show performance advantages in terms of convergence, interpretability, and system sizes that can be handled.",Matthias O Franz and Bernhard Schölkopf,142,7921760019167462588,Neural computation,12,3097-3118,MIT Press,A unifying view of Wiener and Volterra theory and polynomial kernel regression,https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.12.3097,18,2006,/scholar?cites=7921760019167462588,DZ-fHPgAAAAJ:EkHepimYqZsC
14174,"We consider two variables that are related to each other by an invertible function. While it has previously been shown that the dependence structure of the noise can provide hints to determine which of the two variables is the cause, we presently show that even in the deterministic (noise-free) case, there are asymmetries that can be exploited for causal inference. Our method is based on the idea that if the function and the probability density of the cause are chosen independently, then the distribution of the effect will, in a certain sense, depend on the function. We provide a theoretical analysis of this method, showing that it also works in the low noise regime, and link it to information geometry. We report strong empirical results on various real-world data sets from different domains.",Povilas Daniusis and Dominik Janzing and Joris Mooij and Jakob Zscheischler and Bastian Steudel and Kun Zhang and Bernhard Schölkopf,141,14059375901798437238,arXiv preprint arXiv:1203.3475,,,,Inferring deterministic causal relations,https://arxiv.org/abs/1203.3475,,2012,/scholar?cites=14059375901798437238,DZ-fHPgAAAAJ:OTTXONDVkokC
14175,"We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",Arthur Gretton and Karsten Borgwardt and Malte J Rasch and Bernhard Scholkopf and Alexander J Smola,139,7460424011084419888,arXiv preprint arXiv:0805.2368,,,,A kernel method for the two-sample problem,https://arxiv.org/abs/0805.2368,,2008,/scholar?cites=7460424011084419888,DZ-fHPgAAAAJ:Ade32sEp0pkC
14176,"We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reflectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we present competitive results by integrating an additional edge model. We believe that our approach is a solid starting point for future development in this domain.",Carsten Rother and Martin Kiefel and Lumin Zhang and Bernhard Schölkopf and Peter Gehler,138,5438593079896905661,Advances in neural information processing systems,,765-773,,Recovering intrinsic images with a global sparsity prior on reflectance,http://papers.nips.cc/paper/4256-recovering-intrinsic-images-with-a-global-sparsity-prior-on-reflectance,24,2011,/scholar?cites=5438593079896905661,DZ-fHPgAAAAJ:mB3voiENLucC
14177,"Intention inference can be an essential step toward efficient human–robot interaction. For this purpose, we propose the Intention-Driven Dynamics Model (IDDM) to probabilistically model the generative process of movements that are directed by the intention. The IDDM allows the intention to be inferred from observed movements using Bayes’ theorem. The IDDM simultaneously finds a latent state representation of noisy and high-dimensional observations, and models the intention-driven dynamics in the latent states. As most robotics applications are subject to real-time constraints, we develop an efficient online algorithm that allows for real-time intention inference. Two human–robot interaction scenarios, i.e. target prediction for robot table tennis and action recognition for interactive humanoid robots, are used to evaluate the performance of our inference algorithm. In both intention inference tasks, the proposed …",Zhikun Wang and Katharina Mülling and Marc Peter Deisenroth and Heni Ben Amor and David Vogt and Bernhard Schölkopf and Jan Peters,136,7317899485302635199,The International Journal of Robotics Research,7,841-858,SAGE Publications,Probabilistic movement modeling for intention inference in human–robot interaction,https://journals.sagepub.com/doi/abs/10.1177/0278364913478447,32,2013,/scholar?cites=7317899485302635199,DZ-fHPgAAAAJ:FsLZdJ3BAzkC
14178,"We present a fast training algorithm for the kernel Fisher discriminant classifier. It uses a greedy approximation technique and has an empirical scaling behavior which improves upon the state of the art by more than an order of magnitude, thus rendering the kernel Fisher algorithm a viable option also for large datasets.",Sebastian Mika and Alexander J Smola and Bernhard Schölkopf,135,14231881277107453673,,,98-104,,An improved training algorithm for kernel Fisher discriminants.,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2000-77.pdf,,2001,/scholar?cites=14231881277107453673,DZ-fHPgAAAAJ:2tRrZ1ZAMYUC
14179,"Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. The case of two random variables is particularly challenging since no (conditional) independences can be exploited. Recent methods that are based on additive noise models suggest the following principle: Whenever the joint distribution P (X,Y)  admits such a model in one direction, e.g., Y = f(X)+N, N ⊥X, but does not admit the reversed model X=g(Y)+Ñ, Ñ ⊥ Y, one infers the former direction to be causal (i.e., X → Y). Up to now, these approaches only dealt with continuous variables. In many situations, however, the variables of interest are discrete or even have only finitely many states. In this work, we extend the notion of additive noise models to these cases. We prove that it almost never occurs that additive noise models can be fit in both directions. We further propose an …",Jonas Peters and Dominik Janzing and Bernhard Scholkopf,134,7025167062701156301,IEEE Transactions on Pattern Analysis and Machine Intelligence,12,2436-2450,IEEE,Causal inference on discrete data using additive noise models,https://ieeexplore.ieee.org/abstract/document/5740928/,33,2011,/scholar?cites=7025167062701156301,DZ-fHPgAAAAJ:rTD5ala9j4wC
14180,"A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis.",Paul Hayton and Bernhard Schölkopf and Lionel Tarassenko and Paul Anuzis,133,12209841516159068483,Advances in neural information processing systems,,946-952,,Support vector novelty detection applied to jet engine vibration spectra,https://proceedings.neurips.cc/paper/2000/file/7302e3f5e7c072aea8801faf8a492be0-Paper.pdf,13,2000,/scholar?cites=12209841516159068483,DZ-fHPgAAAAJ:HGTzPopzzJcC
14181,"The optimization of k‐space sampling for nonlinear sparse MRI reconstruction is phrased as a Bayesian experimental design problem. Bayesian inference is approximated by a novel relaxation to standard signal processing primitives, resulting in an efficient optimization algorithm for Cartesian and spiral trajectories. On clinical resolution brain image data from a Siemens 3T scanner, automatically optimized trajectories lead to significantly improved images, compared to standard low‐pass, equispaced, or variable density randomized designs. Insights into the nonlinear design optimization problem for MRI are given. Magn Reson Med, 2010. © 2009 Wiley‐Liss, Inc.",Matthias Seeger and Hannes Nickisch and Rolf Pohmann and Bernhard Schölkopf,131,16745956704086420361,Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine,1,116-126,"Wiley Subscription Services, Inc., A Wiley Company",Optimization of k‐space trajectories for compressed sensing by Bayesian experimental design,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.22180,63,2010,/scholar?cites=16745956704086420361,DZ-fHPgAAAAJ:k8Z6L05lTy4C
14182,The concept of Support Vector Regression is extended to a more general class of convex cost functions. Moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method. Both computational feasibility and improvement of estimation is demonstrated in the experiments.,Alex J Smola and Bernhard Schölkopf and Klaus-Robert Müller,131,6598047152291215709,,,,,General cost functions for support vector regression,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.2760,,1998,/scholar?cites=6598047152291215709,DZ-fHPgAAAAJ:4X0JR2_MtJMC
14183,"Motivation: In drug discovery a key task is to identify characteristics that separate active (binding) compounds from inactive (non-binding) ones. An automated prediction system can help reduce resources necessary to carry out this task.Results: Two methods for prediction of molecular bioactivity for drug design are introduced and shown to perform well in a data set previously studied as part of the KDD (Knowledge Discovery and Data Mining) Cup 2001. The data is characterized by very few positive examples, a very large number of features (describing three-dimensional properties of the molecules)  and rather different distributions between training and test data. Two techniques are introduced specifically to tackle these problems: a feature selection method for unbalanced data and a classifier which adapts to the distribution of the the unlabeled test data (a so-called transductive method). We show both …",Jason Weston and Fernando Pérez-Cruz and Olivier Bousquet and Olivier Chapelle and André Elisseeff and Bernhard Schölkopf,130,7058356445093759156,Bioinformatics,6,764,Oxford University Press,Feature selection and transduction for prediction of molecular bioactivity for drug design,https://academic.oup.com/bioinformatics/article-abstract/19/6/764/234339,19,2003,/scholar?cites=7058356445093759156,DZ-fHPgAAAAJ:U4n9YNQMCAIC
14184,"Extreme multi-label classification refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. Datasets in extreme classification exhibit fit to power-law distribution, ie a large fraction of labels have very few positive instances in the data distribution. Most state-of-the-art approaches for extreme multi-label classification attempt to capture correlation among labels by embedding the label matrix to a low-dimensional linear sub-space. However, in the presence of power-law distributed extremely large and diverse label spaces, structural assumptions such as low rank can be easily violated.",Rohit Babbar and Bernhard Schölkopf,129,4731047684174464949,,,721-729,,Dismec: Distributed sparse machines for extreme multi-label classification,https://dl.acm.org/doi/abs/10.1145/3018661.3018741,,2017,/scholar?cites=4731047684174464949,DZ-fHPgAAAAJ:aT2_oyLf33AC
14185,"Statistical learning theory is regarded as one of the most beautifully developed branches of artificial intelligence. It provides the theoretical basis for many of today's machine learning algorithms. The theory helps to explore what permits to draw valid conclusions from empirical data. This chapter provides an overview of the key ideas and insights of statistical learning theory. The statistical learning theory begins with a class of hypotheses and uses empirical data to select one hypothesis from the class. If the data generating mechanism is benign, then it is observed that the difference between the training error and test error of a hypothesis from the class is small. The statistical learning theory generally avoids metaphysical statements about aspects of the true underlying dependency, and thus is precise by referring to the difference between training and test error. The chapter also describes some …",Ulrike Von Luxburg and Bernhard Schölkopf,129,435762242180296422,Arxiv preprint arXiv:0810.4752,,,,"Statistical learning theory: models, concepts, and results",https://www.sciencedirect.com/science/article/pii/B9780444529367500161,,2008,/scholar?cites=435762242180296422,DZ-fHPgAAAAJ:Tiz5es2fbqcC
14186,"Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In order to overcome such limitations we contribute threefold:(i) we introduce a taxonomy of camera shakes,(ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from Hirsch et al.~ and a fast algorithm for single image blind deconvolution for space-invariant filters from Cho and Lee to introduce a method for blind deconvolution for space-variant blur. And (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.",Stefan Harmeling and Hirsch Michael and Bernhard Schölkopf,128,14122073423788348878,Advances in Neural Information Processing Systems,,829-837,,Space-variant single-image blind deconvolution for removing camera shake,https://papers.nips.cc/paper/2010/hash/7f5d04d189dfb634e6a85bb9d9adf21e-Abstract.html,23,2010,/scholar?cites=14122073423788348878,DZ-fHPgAAAAJ:eflP2zaiRacC
14187,"Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classifier, whose key component is a weight vector in a feature space implicitly introduced by a positive definite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modified optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classifiers. These classifiers essentially find a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the different classes of data are linearly well separated. Experimental results over several classification benchmarks demonstrate the effectiveness of our approach.",Mingrui Wu and Bernhard Schölkopf and Gökhan Bakır,126,6341564050906139927,Journal of Machine Learning Research,Apr,603-624,,A direct method for building sparse kernel learning algorithms,http://www.jmlr.org/papers/v7/wu06a.html,7,2006,/scholar?cites=6341564050906139927,DZ-fHPgAAAAJ:lgwcVrK6X84C
14188,"Most EEG-based brain-computer interface (BCI) paradigms come along with specific electrode positions, for example, for a visual-based BCI, electrode positions close to the primary visual cortex are used. For new BCI paradigms it is usually not known where task relevant activity can be measured from the scalp. For individual subjects, Lal et al. in 2004 showed that recording positions can be found without the use of prior knowledge about the paradigm used. However it remains unclear to what extent their method of recursive channel elimination (RCE) can be generalized across subjects. In this paper we transfer channel rankings from a group of subjects to a new subject. For motor imagery tasks the results are promising, although cross-subject channel selection does not quite achieve the performance of channel selection on data of single subjects. Although the RCE method was not provided with prior …",Michael Schröder and Thomas Navin Lal and Thilo Hinterberger and Martin Bogdan and N Jeremy Hill and Niels Birbaumer and Wolfgang Rosenstiel and Bernhard Schölkopf,122,15856719151856243844,EURASIP Journal on Advances in Signal Processing,19,174746,Springer International Publishing,Robust EEG channel selection across subjects for brain-computer interfaces,https://link.springer.com/article/10.1155/ASP.2005.3103,2005,2005,/scholar?cites=15856719151856243844,DZ-fHPgAAAAJ:WC23djZS0W4C
14189,"Many settings of unsupervised learning can be viewed as quantization problems-the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding. We explore this connection in two ways:(1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach.",Alexander J Smola and Sebastian Mika and Bernhard Schölkopf and Robert C Williamson,122,13804495789299748071,Journal of Machine Learning Research,Jun,179-209,,Regularized principal manifolds,http://www.jmlr.org/papers/v1/smola01a.html,1,2001,/scholar?cites=13804495789299748071,DZ-fHPgAAAAJ:abG-DnoFyZgC
14190,"We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection {(Si, li)} n i= 1, where each Si is a sample drawn from the probability distribution of Xi× Yi, and li is a binary label indicating whether “Xi→ Yi” or “Xi← Yi”. Given these data, we build a causal inference rule in two steps. First, we featurize each Si using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables.",David Lopez-Paz and Krikamol Muandet and Bernhard Schölkopf and Iliya Tolstikhin,121,11898881539328089290,,,1452-1461,,Towards a learning theory of cause-effect inference,http://www.jmlr.org/proceedings/papers/v37/lopez-paz15.pdf,,2015,/scholar?cites=11898881539328089290,DZ-fHPgAAAAJ:GSAVY4pwiygC
14191,"We reveal the presence of refractory and overlap effects in the event-related potentials in visual P300 speller datasets, and we show their negative impact on the performance of the system. This finding has important implications for how to encode the letters that can be selected for communication. However, we show that such effects are dependent on stimulus parameters: an alternative stimulus type based on apparent motion suffers less from the refractory effects and leads to an improved letter prediction performance.",SMM Martens and NJ Hill and J Farquhar and B Schölkopf,121,4869353161713752665,Journal of neural engineering,2,026003,IOP Publishing,Overlap and refractory effects in a brain–computer interface speller based on the visual P300 event-related potential,https://iopscience.iop.org/article/10.1088/1741-2560/6/2/026003/meta,6,2009,/scholar?cites=4869353161713752665,DZ-fHPgAAAAJ:L1USKYWJimsC
14192,"Photometry of stars from the K2 extension of NASA's Kepler mission is afflicted by systematic effects caused by small (few-pixel) drifts in the telescope pointing and other spacecraft issues. We present a method for searching K2 light curves for evidence of exoplanets by simultaneously fitting for these systematics and the transit signals of interest. This method is more computationally expensive than standard search algorithms but we demonstrate that it can be efficiently implemented and used to discover transit signals. We apply this method to the full Campaign 1 data set and report a list of 36 planet candidates transiting 31 stars, along with an analysis of the pipeline performance and detection efficiency based on artificial signal injections and recoveries. For all planet candidates, we present posterior distributions on the properties of each system based strictly on the transit observables.",Daniel Foreman-Mackey and Benjamin T Montet and David W Hogg and Timothy D Morton and Dun Wang and Bernhard Schölkopf,117,8013378667497127212,The Astrophysical Journal,2,215,IOP Publishing,A systematic search for transiting planets in the K2 data,https://iopscience.iop.org/article/10.1088/0004-637X/806/2/215/meta,806,2015,/scholar?cites=8013378667497127212,DZ-fHPgAAAAJ:_-QPzy9dQywC
14193,"Understanding the impacts of climate extremes on the carbon cycle is important for quantifying the carbon-cycle climate feedback and highly relevant to climate change assessments. Climate extremes and fires can have severe regional effects, but a spatially explicit global impact assessment is still lacking. Here, we directly quantify spatiotemporal contiguous extreme anomalies in four global data sets of gross primary production (GPP) over the last 30 years. We find that positive and negative GPP extremes occurring on 7% of the spatiotemporal domain explain 78% of the global interannual variation in GPP and a significant fraction of variation in the net carbon flux. The largest thousand negative GPP extremes during 1982–2011 (4.3% of the data) account for a decrease in photosynthetic carbon uptake of about 3.5 Pg C yr− 1, with most events being attributable to water scarcity. The results imply that it is essential …",Jakob Zscheischler and Miguel D Mahecha and Jannis Von Buttlar and Stefan Harmeling and Martin Jung and Anja Rammig and James T Randerson and Bernhard Schölkopf and Sonia I Seneviratne and Enrico Tomelleri and Sönke Zaehle and Markus Reichstein,117,17565794335967588729,Environmental Research Letters,3,035001,IOP Publishing,A few extreme events dominate global interannual variability in gross primary production,https://iopscience.iop.org/article/10.1088/1748-9326/9/3/035001/meta,9,2014,/scholar?cites=17565794335967588729,DZ-fHPgAAAAJ:oH8HCDhqVGsC
14194,"Motivated by the particular problems involved in communicating with “locked-in” paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using Support Vector Machine classification and Recursive Channel Elimination on the independent components of averaged eventrelated potentials, we show that an untrained user’s EEG data can be classified with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI.",N Hill and Thomas Lal and Karin Bierig and Niels Birbaumer and Bernhard Schölkopf,115,8301212968959143049,Advances in neural information processing systems,,569-576,,An auditory paradigm for brain-computer interfaces,https://proceedings.neurips.cc/paper/2004/file/921c2dc40d0b979c2910298d2f880152-Paper.pdf,17,2004,/scholar?cites=8301212968959143049,DZ-fHPgAAAAJ:CB2v5VPnA5kC
14195,"Online social networking sites are experimenting with the following crowd-powered procedure to reduce the spread of fake news and misinformation: whenever a user is exposed to a story through her feed, she can flag the story as misinformation and, if the story receives enough flags, it is sent to a trusted third party for fact checking. If this party identifies the story as misinformation, it is marked as disputed. However, given the uncertain number of exposures, the high cost of fact checking, and the trade-off between flags and exposures, the above mentioned procedure requires careful reasoning and smart algorithms which, to the best of our knowledge, do not exist to date. In this paper, we first introduce a flexible representation of the above procedure using the framework of marked temporal point processes. Then, we develop a scalable online algorithm, CURB, to select which stories to send for fact checking and …",Jooyeon Kim and Behzad Tabibian and Alice Oh and Bernhard Schölkopf and Manuel Gomez-Rodriguez,114,5357353839312739955,,,324-332,,Leveraging the crowd to detect and reduce the spread of fake news and misinformation,https://dl.acm.org/doi/abs/10.1145/3159652.3159734,,2018,/scholar?cites=5357353839312739955,DZ-fHPgAAAAJ:LNwED7MYg98C
14196,"Time plays an essential role in the diffusion of information, influence, and disease over networks. In many cases we can only observe when a node is activated by a contagion—when a node learns about a piece of information, makes a decision, adopts a new behavior, or becomes infected with a disease. However, the underlying network connectivity and transmission rates between nodes are unknown. Inferring the underlying diffusion dynamics is important because it leads to new insights and enables forecasting, as well as influencing or containing information propagation. In this paper we model diffusion as a continuous temporal process occurring at different rates over a latent, unobserved network that may change over time. Given information diffusion data, we infer the edges and dynamics of the underlying network. Our model naturally imposes sparse solutions and requires no parameter tuning. We develop an efficient inference algorithm that uses stochastic convex optimization to compute online estimates of the edges and transmission rates. We evaluate our method by tracking information diffusion among 3.3 million mainstream media sites and blogs, and experiment with more than 179 million different instances of information spreading over the network in a one-year period. We apply our network inference algorithm to the top 5,000 media sites and blogs and report several interesting observations. First, information pathways for general recurrent topics are more stable across time than for on-going news events. Second, clusters of news media sites and blogs often emerge and vanish in a matter of days for on-going news events. Finally …",Manuel Gomez Rodriguez and Jure Leskovec and David Balduzzi and Bernhard Schölkopf,114,13512348144175399511,Network Science,1,26-65,Cambridge University Press,Uncovering the structure and temporal dynamics of information propagation,https://doc.rero.ch/record/300832,2,2014,/scholar?cites=13512348144175399511,DZ-fHPgAAAAJ:kvJssbFybhEC
14197,"We demonstrate the use of support vector regression (SVR) techniques for black-box system identification. These methods derive from statistical learning theory, and are of great theoretical and practical interest. We describe the theory underpinning SVR, and compare support vector methods with other approaches using radial basis networks. Finally, we apply SVR to modeling the behaviour of a hydraulic robot arm, and show that SVR improves on previously published results.",Arthur Gretton and Arnaud Doucet and Ralf Herbrich and Peter JW Rayner and Bernhard Scholkopf,113,16657018855278657853,,,341-344,IEEE,Support vector regression for black-box system identification,https://ieeexplore.ieee.org/abstract/document/955292/,,2001,/scholar?cites=16657018855278657853,DZ-fHPgAAAAJ:NDuN12AVoxsC
14198,Semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model. We extend two learning algorithms-Support Vector machines and Linear Programming machines to this case and give experimental results for SV machines.,Alex Smola and Thilo-Thomas Frieß and Bernhard Schölkopf,112,2406641217162398127,Advances in neural information processing systems,,585-591,,Semiparametric support vector and linear programming machines,https://proceedings.neurips.cc/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Paper.pdf,11,1998,/scholar?cites=2406641217162398127,DZ-fHPgAAAAJ:0CzhzZyukY4C
14199,"We have recently proposed a new approach to control the number of basis functions and the accuracy in support vector machines. The latter is transferred to a linear programming setting, which inherently enforces sparseness of the solution. The algorithm computes a nonlinear estimate in terms of kernel functions and an ɛ>0 with the property that at most a fraction ν of the training set has an error exceeding ɛ. The algorithm is robust to local perturbations of these points' target values. We give an explicit formulation of the optimization equations needed to solve the linear program and point out which modifications of the standard optimization setting are necessary to take advantage of the particular structure of the equations in the regression case.",Alex Smola and Bernhard Scholkopf and Gunnar Ratsch,111,787824485862888708,,,575-580,IET Digital Library,Linear programs for automatic accuracy control in regression,https://digital-library.theiet.org/content/conferences/10.1049/cp_19991171,,1999,/scholar?cites=787824485862888708,DZ-fHPgAAAAJ:OBSaB-F7qqsC
14200,"We provide a new linear program to deal with classification of data in the case of data given in terms of pairwise proximities. This allows to avoid the problems inherent in using feature spaces with indefinite metric in support vector machines, since the notion of a margin is purely needed in input space where the classification actually occurs. Moreover in our approach we can enforce sparsity in the proximity representation by sacrificing training error. This turns out to be favorable for proximity data. Similar to ν-SV methods, the only parameter needed in the algorithm is the (asymptotical) number of data points being classified with a margin. Finally, the algorithm is successfully compared with ν-SV learning in proximity space and K-nearest-neighbors on real world data from neuroscience and molecular biology.",Thore Graepel and Ralf Herbrich and Bernhard Scholkopf and Alex Smola and Peter Bartlett and K-R Muller and Klaus Obermayer and Robert Williamson,111,15237264566992282947,,,304-309,IET Digital Library,Classification on proximity data with LP-machines,https://digital-library.theiet.org/content/conferences/10.1049/cp_19991126,,1999,/scholar?cites=15237264566992282947,DZ-fHPgAAAAJ:jE2MZjpN3IcC
14201,"Gamma oscillations of the electromagnetic field of the brain are known to be involved in a variety of cognitive processes, and are believed to be fundamental for information processing within the brain. While gamma oscillations have been shown to be correlated with brain rhythms at different frequencies, to date no empirical evidence has been presented that supports a causal influence of gamma oscillations on other brain rhythms. In this work, we study the relation of gamma oscillations and the sensorimotor rhythm (SMR) in healthy human subjects using electroencephalography. We first demonstrate that modulation of the SMR, induced by motor imagery of either the left or right hand, is positively correlated with the power of frontal and occipital gamma oscillations, and negatively correlated with the power of centro-parietal gamma oscillations. We then demonstrate that the most simple causal structure, capable of …",Moritz Grosse-Wentrup and Bernhard Schölkopf and Jeremy Hill,110,7054149021563939926,NeuroImage,2,837-842,Academic Press,Causal influence of gamma oscillations on the sensorimotor rhythm,https://www.sciencedirect.com/science/article/pii/S1053811910006932,56,2011,/scholar?cites=7054149021563939926,DZ-fHPgAAAAJ:5qfkUJPXOUwC
14202,"Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on-and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.",Shixiang Shane Gu and Timothy Lillicrap and Richard E Turner and Zoubin Ghahramani and Bernhard Schölkopf and Sergey Levine,109,18031173648112197230,Advances in neural information processing systems,,3846-3855,,Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning,https://proceedings.neurips.cc/paper/2017/hash/a1d7311f2a312426d710e1c617fcbc8c-Abstract.html,30,2017,/scholar?cites=18031173648112197230,DZ-fHPgAAAAJ:z8CHdd96DfkC
14203,"Gene expression maps for model organisms, including Arabidopsis thaliana, have typically been created using gene-centric expression arrays. Here, we describe a comprehensive expression atlas, Arabidopsis thaliana Tiling Array Express (At-TAX), which is based on whole-genome tiling arrays. We demonstrate that tiling arrays are accurate tools for gene expression analysis and identified more than 1,000 unannotated transcribed regions. Visualizations of gene expression estimates, transcribed regions, and tiling probe measurements are accessible online at the At-TAX homepage.",Sascha Laubinger and Georg Zeller and Stefan R Henz and Timo Sachsenberg and Christian K Widmer and Naïra Naouar and Marnik Vuylsteke and Bernhard Schölkopf and Gunnar Rätsch and Detlef Weigel,109,17099921309602422274,Genome biology,7,R112,BioMed Central,At-TAX: a whole genome tiling array resource for developmental expression analysis and transcript identification in Arabidopsis thaliana,https://link.springer.com/article/10.1186/gb-2008-9-7-r112,9,2008,/scholar?cites=17099921309602422274,DZ-fHPgAAAAJ:sszUF3NjhM4C
14204,"My first exposure to Support Vector Machines came this spring when I heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue s collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Schölkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a …",Bernhard Scholkopf,109,18050060217622736285,IEEE Intelligent systems,,,,Support vector machines: a practical consequence of learning theory,http://jmvidal.cse.sc.edu/lib/scholkopf98a.html,13,1998,/scholar?cites=18050060217622736285,DZ-fHPgAAAAJ:kVjdVfd2voEC
14205,"Kernel-based learning methods provide their solutions as expansions in terms of a kernel. We consider the problem of reducing the computational complexity of evaluating these expansions by approximating them using fewer terms. As a by-product, we point out a connection between clustering and approximation in reproducing kernel Hilbert spaces generated by a particular class of kernels.",Bernhard Schölkopf and Phil Knirsch and Alex Smola and Chris Burges,109,12491525007639034200,,,125-132,"Springer, Berlin, Heidelberg","Fast approximation of support vector kernel expansions, and an interpretation of clustering as approximation in feature spaces",https://link.springer.com/chapter/10.1007/978-3-642-72282-0_12,,1998,/scholar?cites=12491525007639034200,DZ-fHPgAAAAJ:uDGL6kOW6j0C
14206,Suppose you are given some dataset drawn from an underlying probability distribution È and you want to estimate a subset Ë of input space such that the probability that a test point drawn from È lies outside of Ë is bounded by some a priori specified ¼ ½. We propose an algorithm which approaches this problem by trying to estimate a function which is positive on Ë and negative on the complement. The functional form of is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.,Bernhard SchölkopfÜ and Robert C Williamson and Alex SmolaÜ and John Shawe-TaylorÝ,108,6737170857937320602,,,,,SV estimation of a distribution’s support,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.7416&rep=rep1&type=pdf,,1999,/scholar?cites=6737170857937320602,DZ-fHPgAAAAJ:IaI1MmNe2tcC
14207,"We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.",Oliver Stegle and Dominik Janzing and Kun Zhang and Joris M Mooij and Bernhard Schölkopf,102,6634150696169644891,,,1687-1695,,Probabilistic latent variable models for distinguishing between cause and effect,http://papers.nips.cc/paper/4173-probabilistic-latent-variable-models-for-distinguishing-between-cause-and-effect,,2010,/scholar?cites=6634150696169644891,DZ-fHPgAAAAJ:738O_yMBCRsC
14208,"We report on the development and online testing of an electroencephalogram-based brain–computer interface (BCI) that aims to be usable by completely paralysed users—for whom visual or motor-system-based BCIs may not be suitable, and among whom reports of successful BCI use have so far been very rare. The current approach exploits covert shifts of attention to auditory stimuli in a dichotic-listening stimulus design. To compare the efficacy of event-related potentials (ERPs) and steady-state auditory evoked potentials (SSAEPs), the stimuli were designed such that they elicited both ERPs and SSAEPs simultaneously. Trial-by-trial feedback was provided online, based on subjects' modulation of N1 and P3 ERP components measured during single 5 s stimulation intervals. All 13 healthy subjects were able to use the BCI, with performance in a binary left/right choice task ranging from 75% to 96% correct …",NJ Hill and B Schölkopf,99,11383388726167643954,Journal of Neural Engineering,,026011,IOP Publishing,An online brain–computer interface based on shifting attention to concurrent streams of auditory stimuli,https://iopscience.iop.org/article/10.1088/1741-2560/9/2/026011/meta,9,2012,/scholar?cites=11383388726167643954,DZ-fHPgAAAAJ:kJDgFkosVoMC
14209,"Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution of the noise, ie, it is non-parametric with respect to the noise distribution. We argue that the proposed regression method is well suited to the task of causal inference in additive noise models. A practical disadvantage is that the resulting optimization problem is generally non-convex and can be difficult to solve. Nevertheless, we report good results on one of the tasks of the NIPS 2008 Causality Challenge, where the goal is to distinguish causes from effects in pairs of statistically dependent variables. In addition, we propose an algorithm for efficiently inferring causal models from observational data for more than two variables. The required number of …",Joris Mooij and Dominik Janzing and Jonas Peters and Bernhard Schölkopf,99,9325616471072034932,,,745-752,,Regression by dependence minimization and its application to causal inference in additive noise models,https://dl.acm.org/doi/abs/10.1145/1553374.1553470,,2009,/scholar?cites=9325616471072034932,DZ-fHPgAAAAJ:lmc2jWPfTJgC
14210,"We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifica1ly, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels.",Alex Smola and Bernhard Schölkopf,99,5389973563945082022,Advances in Neural information processing systems,,343-349,,From regularization operators to support vector kernels,http://papers.nips.cc/paper/1372-from-regularization-operators-to-support-vector-kernels.pdf,10,1997,/scholar?cites=5389973563945082022,DZ-fHPgAAAAJ:wMgC3FpKEyYC
14211,"Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?Despite the increasing availability of cascadedata and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O (d3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient softthresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.",Hadi Daneshmand and Manuel Gomez-Rodriguez and Le Song and Bernhard Schölkopf,97,15324553191112131119,,,793-801,,"Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm",http://www.jmlr.org/proceedings/papers/v32/daneshmand14.pdf,,2014,/scholar?cites=15324553191112131119,DZ-fHPgAAAAJ:SPgmg5JLkoEC
14212,"This work addresses the following question: Under what assumptions on the data generating process can one infer the causal graph from the joint distribution? The approach taken by conditional independence-based causal discovery methods is based on two assumptions: the Markov condition and faithfulness. It has been shown that under these assumptions the causal graph can be identified up to Markov equivalence (some arrows remain undirected) using methods like the PC algorithm. In this work we propose an alternative by defining Identifiable Functional Model Classes (IFMOCs). As our main theorem we prove that if the data generating process belongs to an IFMOC, one can identify the complete causal graph. To the best of our knowledge this is the first identifiability result of this kind that is not limited to linear functional relationships. We discuss how the IFMOC assumption and the Markov and faithfulness assumptions relate to each other and explain why we believe that the IFMOC assumption can be tested more easily on given data. We further provide a practical algorithm that recovers the causal graph from finitely many data; experiments on simulated data support the theoretical findings.",Jonas Peters and Joris Mooij and Dominik Janzing and Bernhard Schölkopf,97,12213260373341944926,arXiv preprint arXiv:1202.3757,,,,Identifiability of causal graphs using functional models,https://arxiv.org/abs/1202.3757,,2012,/scholar?cites=12213260373341944926,DZ-fHPgAAAAJ:V3AGJWp-ZtQC
14213,"This letter introduces a nonlinear measure of independence between random variables for remote sensing supervised feature selection. The so-called Hilbert-Schmidt independence criterion (HSIC) is a kernel method for evaluating statistical dependence and it is based on computing the Hilbert-Schmidt norm of the cross-covariance operator of mapped samples in the corresponding Hilbert spaces. The HSIC empirical estimator is easy to compute and has good theoretical and practical properties. Rather than using this estimate for maximizing the dependence between the selected features and the class labels, we propose the more sensitive criterion of minimizing the associated HSIC  p -value. Results in multispectral, hyperspectral, and SAR data feature selection for classification show the good performance of the proposed approach.",Gustavo Camps-Valls and Joris Mooij and Bernhard Scholkopf,97,14471695196535999931,IEEE Geoscience and Remote Sensing Letters,3,587-591,IEEE,Remote sensing feature selection by kernel dependence measures,https://ieeexplore.ieee.org/abstract/document/5440922/,7,2010,/scholar?cites=14471695196535999931,DZ-fHPgAAAAJ:EPG8bYD4jVwC
14214,"For modern biology, precise genome annotations are of prime importance, as they allow the accurate definition of genic regions. We employ state-of-the-art machine learning methods to assay and improve the accuracy of the genome annotation of the nematode Caenorhabditis elegans. The proposed machine learning system is trained to recognize exons and introns on the unspliced mRNA, utilizing recent advances in support vector machines and label sequence learning. In 87% (coding and untranslated regions) and 95% (coding regions only) of all genes tested in several out-of-sample evaluations, our method correctly identified all exons and introns. Notably, only 37% and 50%, respectively, of the presently unconfirmed genes in the C. elegans genome annotation agree with our predictions, thus we hypothesize that a sizable fraction of those genes are not correctly annotated. A retrospective evaluation of the Wormbase WS120 annotation [1] of C. elegans reveals that splice form predictions on unconfirmed genes in WS120 are inaccurate in about 18% of the considered cases, while our predictions deviate from the truth only in 10%–13%. We experimentally analyzed 20 controversial genes on which our system and the annotation disagree, confirming the superiority of our predictions. While our method correctly predicted 75% of those cases, the standard annotation was never completely correct. The accuracy of our system is further corroborated by a comparison with two other recently proposed systems that can be used for splice form prediction: SNAP and ExonHunter. We conclude that the genome annotation of C. elegans and other …",Gunnar Rätsch and Sören Sonnenburg and Jagan Srinivasan and Hanh Witte and Klaus-R Müller and Ralf-J Sommer and Bernhard Schölkopf,97,15518050299303448512,PLoS Comput Biol,2,e20,Public Library of Science,Improving the Caenorhabditis elegans genome annotation using machine learning,https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030020,3,2007,/scholar?cites=15518050299303448512,DZ-fHPgAAAAJ:7BrZ7Jt4UNcC
14215,"Kernel Principal Component Analysis (KPCA) has proven to be a versatile tool for unsupervised learning, however at a high computational cost due to the dense expansions in terms of kernel functions. We overcome this problem by proposing a new class of feature extractors employing ℓ1 norms in coefficient space instead of the Reproducing Kernel Hilbert Space in which KPCA was originally formulated in. Moreover, the modified setting allows us to efficiently extract features which maximize criteria other than the variance in a way similar to projection pursuit.",Alex J Smola and Olvi L Mangasarian and Bernhard Schölkopf,97,5872798353494643736,,,167-178,"Springer, Berlin, Heidelberg",Sparse kernel feature analysis,https://link.springer.com/chapter/10.1007/978-3-642-55991-4_18,,2002,/scholar?cites=5872798353494643736,DZ-fHPgAAAAJ:zCSUwVk65WsC
14216,"We propose statistical learning methods for approximating implicit surfaces and computing dense 3D deformation fields. Our approach is based on Support Vector (SV) Machines, which are state of the art in machine learning. It is straightforward to implement and computationally competitive; its parameters can be automatically set using standard machine learning methods.The surface approximation is based on a modified Support Vector regression. We present applications to 3D head reconstruction, including automatic removal of outliers and hole filling. In a second step, we build on our SV representation to compute dense 3D deformation fields between two objects. The fields are computed using a generalized SV Machine enforcing correspondence between the previously learned implicit SV object representations, as well as correspondences between feature points if such points are available. We apply the method to the morphing of 3D heads and other objects.",Florian Steinke and Bernhard Schölkopf and Volker Blanz,95,12773007436133045341,Computer Graphics Forum,3,285-294,"Amsterdam: North Holland, 1982-",Support vector machines for 3D shape processing,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5472&rep=rep1&type=pdf,24,2005,/scholar?cites=12773007436133045341,DZ-fHPgAAAAJ:LdasjJ6CEcoC
14217,"This paper is concerned with the problem of domain adaptation with multiple sources from a causal point of view. In particular, we use causal models to represent the relationship between the features X and class label Y, and consider possible situations where different modules of the causal model change with the domain. In each situation, we investigate what knowledge is appropriate to transfer and find the optimal target-domain hypothesis. This gives an intuitive interpretation of the assumptions underlying certain previous methods and motivates new ones. We finally focus on the case where Y is the cause for X with changing PY and PX| Y, that is, PY and PX| Y change independently across domains. Under appropriate assumptions, the availability of multiple source domains allows a natural way to reconstruct the conditional distribution on the target domain; we propose to model PX| Y (the process to generate effect X from cause Y) on the target domain as a linear mixture of those on source domains, and estimate all involved parameters by matching the target-domain feature distribution. Experimental results on both synthetic and real-world data verify our theoretical results.Traditional machine learning relies on the assumption that both training and test data are from the same distribution. In practice, however, training and test data are probably sampled under different conditions, thus violating this assumption, and the problem of domain adaptation (DA) arises. Consider remote sensing image classification as an example. Suppose we already have several data sets on which the class labels are known; they are called source domains here …",Kun Zhang and Mingming Gong and Bernhard Schölkopf,94,3327240748886939249,AAAI,,3150-3157,,Multi-Source Domain Adaptation: A Causal View.,http://www.is.mpg.de/uploads_file/attachment/attachment/1/ZhangGS_AAAI15.pdf,1,2015,/scholar?cites=3327240748886939249,DZ-fHPgAAAAJ:Llf8zd38bwoC
14218,"In kernel methods, all the information about the training data is contained in the Gram matrix. If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well. We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine.",Bernhard Schölkopf and Jason Weston and Eleazar Eskin and Christina Leslie and William Stafford Noble,90,1110269494137599911,,,511-528,"Springer, Berlin, Heidelberg",A kernel approach for learning from almost orthogonal patterns,https://link.springer.com/chapter/10.1007/3-540-36755-1_44,,2002,/scholar?cites=1110269494137599911,DZ-fHPgAAAAJ:nRpfm8aw39MC
14219,"Detection of epistatic interaction between loci has been postulated to provide a more in-depth understanding of the complex biological and biochemical pathways underlying human diseases. Studying the interaction between two loci is the natural progression following traditional and well-established single locus analysis. However, the added costs and time duration required for the computation involved have thus far deterred researchers from pursuing a genome-wide analysis of epistasis. In this paper, we propose a method allowing such analysis to be conducted very rapidly. The method, dubbed EPIBLASTER, is applicable to case–control studies and consists of a two-step process in which the difference in Pearson's correlation coefficients is computed between controls and cases across all possible SNP pairs as an indication of significant interaction warranting further analysis. For the subset of interactions …",Tony Kam-Thong and Darina Czamara and Koji Tsuda and Karsten Borgwardt and Cathryn M Lewis and Angelika Erhardt-Lehmann and Bernhard Hemmer and Peter Rieckmann and Markus Daake and Frank Weber and Christiane Wolf and Andreas Ziegler and Benno Pütz and Florian Holsboer and Bernhard Schölkopf and Bertram Müller-Myhsok,89,18415713760398301651,European Journal of Human Genetics,4,465-471,Nature Publishing Group,EPIBLASTER-fast exhaustive two-locus epistasis detection strategy using graphical processing units,https://www.nature.com/articles/ejhg2010196,19,2010,/scholar?cites=18415713760398301651,DZ-fHPgAAAAJ:kh2fBNsKQNwC
14220,"The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance, it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice.",Olivier Chapelle and Bernhard Schölkopf,88,10239619584564482208,Advances in neural information processing systems,,609-616,,Incorporating invariances in non-linear support vector machines,https://papers.nips.cc/paper/2001/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf,14,2001,/scholar?cites=10239619584564482208,DZ-fHPgAAAAJ:mKu_rENv82IC
14221,"Digital Collections.
",Gunnar Rätsch and Bernhard Schölkopf and Alexander Smola and Sebastian Mika and Takashi Onoda and Klaus-Robert Muller,88,10885204159137214606,,,,MIT Press,Robust ensemble learning,https://openresearch-repository.anu.edu.au/handle/1885/90838,,2000,/scholar?cites=10885204159137214606,DZ-fHPgAAAAJ:U_HPUtbDl20C
14222,"We study a pattern classification algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results.",Olivier Chapelle and Alekh Agarwal and Fabian H Sinz and Bernhard Schölkopf,85,8673777550219917480,,,1369-1376,,An analysis of inference with the universum,https://papers.nips.cc/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf,,2008,/scholar?cites=8673777550219917480,DZ-fHPgAAAAJ:l2Vku4UUHLUC
14223,"While it is well-known that model can enhance the control performance in terms of precision or energy efficiency, the practical application has often been limited by the complexities of manually obtaining sufficiently accurate models. In the past, learning has proven a viable alternative to using a combination of rigid-body dynamics and handcrafted approximations of nonlinearities. However, a major open question is what nonparametric learning method is suited best for learning dynamics? Traditionally, locally weighted projection regression (LWPR), has been the standard method as it is capable of online, real-time learning for very complex robots. However, while LWPR has had significant impact on learning in robotics, alternative nonparametric regression methods such as support vector regression (SVR) and Gaussian processes regression (GPR) offer interesting alternatives with fewer open parameters and potentially higher accuracy. In this paper, we evaluate these three alternatives for model learning. Our comparison consists out of the evaluation of learning quality for each regression method using original data from SARCOS robot arm, as well as the robot tracking performance employing learned models. The results show that GPR and SVR achieve a superior learning precision and can be applied for real-time control obtaining higher accuracy. However, for the online learning LWPR presents the better method due to its lower computational requirements.",Duy Nguyen-Tuong and Jan Peters and Matthias Seeger and Bernhard Schölkopf,84,15122281393480526422,,CONF,,,Learning inverse dynamics: a comparison,https://infoscience.epfl.ch/record/175477/files/esann08_nguyenetal.pdf,,2008,/scholar?cites=15122281393480526422,DZ-fHPgAAAAJ:EYYDruWGBe4C
14224,,Bernhard Schölkopf and Alex Smola,84,8421080131442786417,Encyclopedia of Biostatistics,,,"John Wiley & Sons, Ltd",Support vector machines,,,2005,/scholar?cites=8421080131442786417,DZ-fHPgAAAAJ:ILKRHgRFtOwC
14225,"We briefly describe the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. This includes a derivation of the support vector optimization problem for classification and regression, the v-trick, various kernels and an overview over applications of kernel methods.",B Schölkopf and AJ Smola,83,6999491358809659103,Proceedings of the Machine Learning Summer School,,,Springer,A Short Introduction to Learning with Kernels,https://link.springer.com/chapter/10.1007/3-540-36434-X_2,,2003,/scholar?cites=6999491358809659103,DZ-fHPgAAAAJ:yD5IFk8b50cC
14226,"The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme. net to close the gap between method users and developers.",Jakob Runge and Sebastian Bathiany and Erik Bollt and Gustau Camps-Valls and Dim Coumou and Ethan Deyle and Clark Glymour and Marlene Kretschmer and Miguel D Mahecha and Jordi Muñoz-Marí and Egbert H van Nes and Jonas Peters and Rick Quax and Markus Reichstein and Marten Scheffer and Bernhard Schölkopf and Peter Spirtes and George Sugihara and Jie Sun and Kun Zhang and Jakob Zscheischler,82,7550335433275839951,,1,1-13,Nature Publishing Group,Inferring causation from time series in Earth system sciences,https://www.nature.com/articles/s41467-019-10105-3,10,2019,/scholar?cites=7550335433275839951,DZ-fHPgAAAAJ:9jrkFlUinGsC
14227,"We describe a fast system for the detection and localization of human faces in images using a nonlinear ‘support–vector machine’. We approximate the decision surface in terms of a reduced set of expansion vectors and propose a cascaded evaluation which has the property that the full support–vector expansion is only evaluated on the face–like parts of the image, while the largest part of typical images is classified using a single expansion vector (a simpler and more efficient classifier). As a result, only three reduced–set vectors are used, on average, to classify an image patch. Hence, the cascaded evaluation, presented in this paper, offers a thirtyfold speed–up over an evaluation using the full set of reduced–set vectors, which is itself already thirty times faster than classification using all the support vectors.",Sami Romdhani and Philip Torr and Bernhard Schölkopf and Andrew Blake,82,15765883980729141777,"Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences",2051,3283-3297,The Royal Society,Efficient face detection by a cascaded support–vector machine expansion,https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2004.1333,460,2004,/scholar?cites=15765883980729141777,DZ-fHPgAAAAJ:j7_hQOaDUrUC
14228,"What is disclosed is acquiring information regarding a web page, without having to commit to downloading that page. In one embodiment, after a current web page is downloaded from one source, and information regarding web pages linked to by links in the current web page are downloaded from a second source, when a user hovers a cursor over a link on a current web page, an informational region is displayed by the link that includes the information from the second source. The informational region may include, for example, a text box that apparently floats by the link. The information in the region can include, for example, keywords in the meta tags of the web page; paragraph headings of the web page; links on the web page to other pages; etc.",,81,6587603200475093651,,,,,Acquiring web page information without commitment to downloading the web page,https://patents.google.com/patent/US7565409B2/en,,2009,/scholar?cites=6587603200475093651,DZ-fHPgAAAAJ:xtRiw3GOFMkC
14229,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other …",Daewon Lee and Matthias Hofmann and Florian Steinke and Yasemin Altun and Nathan D Cahill and Bernhard Scholkopf,80,6371567823475209381,,,186-193,IEEE,Learning similarity measure for multi-modal 3D image registration,https://ieeexplore.ieee.org/abstract/document/5206840/,,2009,/scholar?cites=6371567823475209381,DZ-fHPgAAAAJ:KNjnJ3z-R6IC
14230,"The aim of this paper is to show that machine learning techniques can be used to derive a classifying function for human brain signal data measured by magnetoencephalography (MEG), for the use in a brain computer interface (BCI). This is especially helpful for evaluating quickly whether a BCI approach based on electroencephalography, on which training may be slower due to lower signal-to-noise ratio, is likely to succeed. We apply RCE and regularized SVMs to the experimental data of ten healthy subjects performing a motor imagery task. Four subjects were able to use a trained classifier to write a short name. Further analysis gives evidence that the proposed imagination task is suboptimal for the possible extension to a multiclass interface. To the best of our knowledge this paper is the first working online MEG-based BCI and is therefore a"" proof of concept"".",Thomas Navin Lal and Michael Schröder and N Jeremy Hill and Hubert Preissl and Thilo Hinterberger and Jürgen Mellinger and Martin Bogdan and Wolfgang Rosenstiel and Thomas Hofmann and Niels Birbaumer and Bernhard Schölkopf,80,4462458843264650989,,,465-472,,A brain computer interface with online feedback based on magnetoencephalography,https://dl.acm.org/doi/abs/10.1145/1102351.1102410,,2005,/scholar?cites=4462458843264650989,DZ-fHPgAAAAJ:jU7OWUQzBzMC
14231,"We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution  and the latent variable model distribution . We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from  and . We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE)(Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE)(Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.",Olivier Bousquet and Sylvain Gelly and Ilya Tolstikhin and Carl-Johann Simon-Gabriel and Bernhard Schoelkopf,79,9795665996915269486,arXiv preprint arXiv:1705.07642,,,,From optimal transport to generative modeling: the VEGAN cookbook,https://arxiv.org/abs/1705.07642,,2017,/scholar?cites=9795665996915269486,DZ-fHPgAAAAJ:SzdLQmZCmkwC
14232,,Burges Scholkopf and Chris Burges and Alex Smola,79,17730618236765585065,Advances in Kernel Methods-Support Vector Learning,,356,,Introduction to support vector learning,https://dl.acm.org/doi/abs/10.5555/299094.299095,,1999,/scholar?cites=17730618236765585065,DZ-fHPgAAAAJ:EJ6cC2BCOHIC
14233,"We investigate the problem of testing whether  random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the -dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the -variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the  variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (ie, it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small , it performs well in practice. Finally, we apply the test to a problem in causal discovery.",Niklas Pfister and Peter Bühlmann and Bernhard Schölkopf and Jonas Peters,78,5335267860192794518,arXiv preprint arXiv:1603.00285,,,,Kernel-based tests for joint independence,https://arxiv.org/abs/1603.00285,,2016,/scholar?cites=5335267860192794518,DZ-fHPgAAAAJ:SbDlCG0g-rIC
14234,"Embeddings of random variables in reproducing kernel Hilbert spaces (RKHSs) may be used to conduct statistical inference based on higher order moments. For sufficiently rich (characteristic) RKHSs, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration. Necessary and sufficient conditions for an RKHS to be characteristic exist for $\R^ n $. In the present work, conditions are established for an RKHS to be characteristic on groups and semigroups. Illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and $\R^ n_+ $.",Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Bharath K Sriperumbudur,78,2411716725079455598,Advances in neural information processing systems,,473-480,,Characteristic kernels on groups and semigroups,https://proceedings.neurips.cc/paper/2008/hash/d07e70efcfab08731a97e7b91be644de-Abstract.html,21,2008,/scholar?cites=2411716725079455598,DZ-fHPgAAAAJ:uWiczbcajpAC
14235,"A new technique, “serial block face scanning electron microscopy” (SBFSEM), allows for automatic sectioning and imaging of biological tissue with a scanning electron microscope. Image stacks generated with this technology have a resolution sufficient to distinguish different cellular compartments, including synaptic structures, which should make it possible to obtain detailed anatomical knowledge of complete neuronal circuits. Such an image stack contains several thousands of images and is recorded with a minimal voxel size of 10–20 nm in the x- and y-direction and 30 nm in z-direction. Consequently, a tissue block of 1 mm3(the approximate volume of the Calliphora vicina brain) will produce several hundred terabytes of data. Therefore, highly automated 3D reconstruction algorithms are needed. As a first step in this direction we have developed semi-automated segmentation algorithms for a precise …",Jakob H Macke and Nina Maack and Rocky Gupta and Winfried Denk and Bernhard Schölkopf and Alexander Borst,77,1559094998668329976,Journal of neuroscience methods,2,349-357,Elsevier,Contour-propagation algorithms for semi-automated reconstruction of neural processes,https://www.sciencedirect.com/science/article/pii/S0165027007003780,167,2008,/scholar?cites=1559094998668329976,DZ-fHPgAAAAJ:7wO8s98CvbsC
14236,"As handheld video cameras are now commonplace and available in every smartphone images and videos can be recorded almost everywhere at any time. However, taking a quick shot frequently ends up in a blurry result due to unwanted camera shake during recording or moving objects in the scene. Removing these artifacts from the blurry recordings is a highly ill-posed problem as neither the sharp image nor the motion blur is known. Propagating information between multiple consecutive blurry observations can help to restore the desired sharp image or video. Solutions for blind deconvolution based on neural networks rely on a massive amount of ground-truth data which was difficult to acquire. In this work, we propose an efficient approach to produce a significant amount of realistic training data and introduce a novel recurrent network architecture to deblur frames, which can efficiently handle arbitrary spatial and temporal input sizes.",Patrick Wieschollek and Michael Hirsch and Bernhard Scholkopf and Hendrik Lensch,76,11735096712549895899,,,231-240,,Learning blind motion deblurring,http://openaccess.thecvf.com/content_iccv_2017/html/Wieschollek_Learning_Blind_Motion_ICCV_2017_paper.html,,2017,/scholar?cites=11735096712549895899,DZ-fHPgAAAAJ:eLuibZyXBAwC
14237,"Most inpainting approaches require a good image model to infer the unknown pixels. In this work, we directly learn a mapping from image patches, corrupted by missing pixels, onto complete image patches. This mapping is represented as a deep neural network that is automatically trained on a large image data set. In particular, we are interested in the question whether it is helpful to exploit the shape information of the missing regions, i.e. the masks, which is something commonly ignored by other approaches. In comprehensive experiments on various images, we demonstrate that our learning-based approach is able to use this extra information and can achieve state-of-the-art inpainting results. Furthermore, we show that training with such extra information is useful for blind inpainting, where the exact shape of the missing region might be uncertain, for instance due to aliasing effects.",Rolf Köhler and Christian Schuler and Bernhard Schölkopf and Stefan Harmeling,76,3930793642750654393,,,523-534,"Springer, Cham",Mask-specific inpainting with deep neural networks,https://link.springer.com/chapter/10.1007/978-3-319-11752-2_43,,2014,/scholar?cites=3930793642750654393,DZ-fHPgAAAAJ:zfsRRabFVBUC
14238,"We employed three different brain signal recording methods to perform Brain-Computer Interface studies on untrained subjects. In all cases, we aim to develop a system that could be used for fast, reliable preliminary screening in clinical BCI application, and we are interested in knowing how long screening sessions need to be. Good performance could be achieved, on average, after the first 200 trials in EEG, 75–100 trials in MEG, or 25–50 trials in ECoG. We compare the performance of Independent Component Analysis and the Common Spatial Pattern algorithm in each of the three sensor types, finding that spatial filtering does not help in MEG, helps a little in ECoG, and improves performance a great deal in EEG. In all cases the unsupervised ICA algorithm performed at least as well as the supervised CSP algorithm, which can suffer from poor generalization performance due to overfitting, particularly in …",N Jeremy Hill and Thomas Navin Lal and Michael Schröder and Thilo Hinterberger and Guido Widman and Christian E Elger and Bernhard Schölkopf and Niels Birbaumer,76,16506163104489202152,,,404-413,"Springer, Berlin, Heidelberg","Classifying event-related desynchronization in EEG, ECoG and MEG signals",https://link.springer.com/chapter/10.1007/11861898_41,,2006,/scholar?cites=16506163104489202152,DZ-fHPgAAAAJ:yB1At4FlUx8C
14239,"This Chapter presents the PASCAL Evaluating Predictive Uncertainty Challenge, introduces the contributed Chapters by the participants who obtained outstanding results, and provides a discussion with some lessons to be learnt. The Challenge was set up to evaluate the ability of Machine Learning algorithms to provide good “probabilistic predictions”, rather than just the usual “point predictions” with no measure of uncertainty, in regression and classification problems. Parti-cipants had to compete on a number of regression and classification tasks, and were evaluated by both traditional losses that only take into account point predictions and losses we proposed that evaluate the quality of the probabilistic predictions.",Joaquin Quinonero-Candela and Carl Edward Rasmussen and Fabian Sinz and Olivier Bousquet and Bernhard Schölkopf,76,4056481117611150699,,,1-27,"Springer, Berlin, Heidelberg",Evaluating predictive uncertainty challenge,https://link.springer.com/chapter/10.1007/11736790_1,,2005,/scholar?cites=4056481117611150699,DZ-fHPgAAAAJ:SpbeaW3--B0C
14240,"Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from …",Mateo Rojas-Carulla and Bernhard Schölkopf and Richard Turner and Jonas Peters,75,3744585272896446716,The Journal of Machine Learning Research,1,1309-1342,JMLR. org,Invariant models for causal transfer learning,https://dl.acm.org/doi/abs/10.5555/3291125.3291161,19,2018,/scholar?cites=3744585272896446716,DZ-fHPgAAAAJ:4_AkaH4LQzwC
14241,"Taking a sharp photo at several megapixel resolution traditionally relies on high grade lenses. In this paper, we present an approach to alleviate image degradations caused by imperfect optics. We rely on a calibration step to encode the optical aberrations in a space-variant point spread function and obtain a corrected image by non-stationary deconvolution. By including the Bayer array in our image formation model, we can perform demosaicing as part of the deconvolution.",Christian J Schuler and Michael Hirsch and Stefan Harmeling and Bernhard Schölkopf,75,15609704083102782698,,,659-666,IEEE,Non-stationary correction of optical aberrations,https://ieeexplore.ieee.org/abstract/document/6126301/,,2011,/scholar?cites=15609704083102782698,DZ-fHPgAAAAJ:D03iK_w7-QYC
14242,"Diffusion and propagation of information, influence and diseases take place over increasingly larger networks. We observe when a node copies information, makes a decision or becomes infected but networks are often hidden or unobserved. Since networks are highly dynamic, changing and growing rapidly, we only observe a relatively small set of cascades before a network changes significantly. Scalable network inference based on a small cascade set is then necessary for understanding the rapidly evolving dynamics that govern diffusion. In this article, we develop a scalable approximation algorithm with provable near-optimal performance based on submodular maximization which achieves a high accuracy in such scenario, solving an open problem first introduced by Gomez-Rodriguez et al (2010). Experiments on synthetic and real diffusion data show that our algorithm in practice achieves an optimal trade-off between accuracy and running time.",Manuel Gomez Rodriguez and Bernhard Schölkopf,73,8554523116771156960,arXiv preprint arXiv:1205.1671,,,,Submodular inference of diffusion networks from multiple trees,https://arxiv.org/abs/1205.1671,,2012,/scholar?cites=8554523116771156960,DZ-fHPgAAAAJ:H_jBuBxbQIAC
14243,"We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications.",Krikamol Muandet and Bernhard Schölkopf,72,9660088192911903778,arXiv preprint arXiv:1303.0309,,,,One-class support measure machines for group anomaly detection,https://arxiv.org/abs/1303.0309,,2013,/scholar?cites=9660088192911903778,DZ-fHPgAAAAJ:rOcdG6UcVlcC
14244,"Subjects operating a brain–computer interface (BCI) based on sensorimotor rhythms exhibit large variations in performance over the course of an experimental session. Here, we show that high-frequency γ-oscillations, originating in fronto-parietal networks, predict such variations on a trial-to-trial basis. We interpret this finding as empirical support for an influence of attentional networks on BCI performance via modulation of the sensorimotor rhythm.",M Grosse-Wentrup and B Schölkopf,72,7489770089171555242,Journal of Neural Engineering},4,1991-2000,,High gamma-power predicts performance in sensorimotor-rhythm brain-computer interfaces,https://iopscience.iop.org/article/10.1088/1741-2560/9/4/046001/meta,9,2012,/scholar?cites=7489770089171555242,DZ-fHPgAAAAJ:s9ia6_kGH2AC
14245,"We study a particular class of cyclic causal models, where each variable is a (possibly nonlinear) function of its parents and additive noise. We prove that the causal graph of such models is generically identifiable in the bivariate, Gaussian-noise case. We also propose a method to learn such models from observational data. In the acyclic case, the method reduces to ordinary regression, but in the more challenging cyclic case, an additional term arises in the loss function, which makes it a special case of nonlinear independent component analysis. We illustrate the proposed method on synthetic data.",Joris M Mooij and Dominik Janzing and Tom Heskes and Bernhard Schölkopf,71,1967944948509723861,,,639-647,,On causal discovery with cyclic additive noise models,http://papers.nips.cc/paper/4424-on-causal-discovery-with-cyclic-additive-noise-models,,2011,/scholar?cites=1967944948509723861,DZ-fHPgAAAAJ:VLnqNzywnoUC
14246,"Interest point detection in still images is a well-studied topic in computer vision. In the spatiotemporal domain, however, it is still unclear which features indicate useful interest points. In this paper we approach the problem by learning a detector from examples: we record eye movements of human subjects watching video sequences and train a neural network to predict which locations are likely to become eye movement targets. We show that our detector outperforms current spatiotemporal interest point architectures on a standard classification dataset.",Wolf Kienzle and Bernhard Schölkopf and Felix A Wichmann and Matthias O Franz,71,15299844705132699509,,,405-414,"Springer, Berlin, Heidelberg",How to find interesting locations in video: a spatiotemporal interest point detector learned from human eye movements,https://link.springer.com/chapter/10.1007/978-3-540-74936-3_41,,2007,/scholar?cites=15299844705132699509,DZ-fHPgAAAAJ:VaXvl8Fpj5cC
14247,"In this paper we investigate connections between statistical learning theory and data compression on the basis of support vector machine (SVM) model selection. Inspired by several generalization bounds we construct"" compression coefficients"" for SVMs which measure the amount by which the training labels can be compressed by a code built from the separating hyperplane. The main idea is to relate the coding precision to geometrical concepts such as the width of the margin or the shape of the data in the feature space. The so derived compression coefficients combine well known quantities such as the radius-margin term R 2/ρ 2, the eigenvalues of the kernel matrix, and the number of support vectors. To test whether they are useful in practice we ran model selection experiments on benchmark data sets. As a result we found that compression coefficients can fairly accurately predict the parameters for which the test error is minimized.",Ulrike Von Luxburg and Olivier Bousquet and Bernhard Schölkopf,71,17317487441113450683,Journal of Machine Learning Research,Apr,293-323,,A compression approach to support vector model selection,http://www.jmlr.org/papers/v5/luxburg04a.html,5,2004,/scholar?cites=17317487441113450683,DZ-fHPgAAAAJ:WHdLCjDvYFkC
14248,,Bernhard Schölkopf and Joachim Giesen and Simon Spalinger,71,4075224102637456362,,,1193-1200,,Kernel methods for implicit surface modeling,,,2004,/scholar?cites=4075224102637456362,DZ-fHPgAAAAJ:MhiOAD_qIWkC
14249,"Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions:(1) Theoretical: By restricting the model class (eg to additive noise) we provide more general identifiability results than existing ones. The results cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series.(2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. When the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers. The Structural Equation Model point of view allows us to extend both the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays (as may happen for fMRI data, for example). TiMINo outperforms existing methods on artificial and real data. Code is provided.",Jonas Peters and Dominik Janzing and Bernhard Schölkopf,70,13954555197016246647,,,154-162,,Causal inference on time series using restricted structural equation models,http://papers.nips.cc/paper/5063-causal-inference-on-time-series-using-restricted-structural-equation,,2013,/scholar?cites=13954555197016246647,DZ-fHPgAAAAJ:LGlY6t8CeOMC
14250,"State-of-the-art video deblurring methods are capable of removing non-uniform blur caused by unwanted camera shake and/or object motion in dynamic scenes. However, most existing methods are based on batch processing and thus need access to all recorded frames, rendering them computationally demanding and time-consuming and thus limiting their practical use. In contrast, we propose an online (sequential) video deblurring method based on a spatio-temporal recurrent network that allows for real-time performance. In particular, we introduce a novel architecture which extends the receptive field while keeping the overall size of the network small to enable fast execution. In doing so, our network is able to remove even large blur caused by strong camera shake and/or fast moving objects. Furthermore, we propose a novel network layer that enforces temporal consistency between consecutive frames by dynamic temporal blending which compares and adaptively (at test time) shares features obtained at different time steps. We show the superiority of the proposed method in an extensive experimental evaluation.",Tae Hyun Kim and Kyoung Mu Lee and Bernhard Scholkopf and Michael Hirsch,69,4874657422788409331,,,4038-4047,,Online video deblurring via dynamic temporal blending network,http://openaccess.thecvf.com/content_iccv_2017/html/Kim_Online_Video_Deblurring_ICCV_2017_paper.html,,2017,/scholar?cites=4874657422788409331,DZ-fHPgAAAAJ:pawnNKgKThcC
14251,"Causal terminology is often introduced in the interpretation of encoding and decoding models trained on neuroimaging data. In this article, we investigate which causal statements are warranted and which ones are not supported by empirical evidence. We argue that the distinction between encoding and decoding models is not sufficient for this purpose: relevant features in encoding and decoding models carry a different meaning in stimulus- and in response-based experimental paradigms.We show that only encoding models in the stimulus-based setting support unambiguous causal interpretations. By combining encoding and decoding models trained on the same data, however, we obtain insights into causal relations beyond those that are implied by each individual model type. We illustrate the empirical relevance of our theoretical findings on EEG data recorded during a visuo-motor learning task.",Sebastian Weichwald and Timm Meyer and Ozan Özdenizci and Bernhard Schölkopf and Tonio Ball and Moritz Grosse-Wentrup,69,13704908520855111164,NeuroImage,,48-59,Academic Press,Causal interpretation rules for encoding and decoding models in neuroimaging,https://www.sciencedirect.com/science/article/pii/S105381191500052X,110,2015,/scholar?cites=13704908520855111164,DZ-fHPgAAAAJ:VHM5RxzNINsC
14252,"Kernel methods are among the most successful tools in machine learning and are used in challenging data analysis problems in many disciplines. Here we provide examples where kernel methods have proven to be powerful tools for analyzing behavioral data, especially for identifying features in categorization experiments. We also demonstrate that kernel methods relate to perceptrons and exemplar models of categorization. Hence, we argue that kernel methods have neural and psychological plausibility, and theoretical results concerning their behavior are therefore potentially relevant for human category learning. In particular, we believe kernel methods have the potential to provide explanations ranging from the implementational via the algorithmic to the computational level.",Frank Jäkel and Bernhard Schölkopf and Felix A Wichmann,69,4106203044708760638,Trends in cognitive sciences,9,381-388,Elsevier Current Trends,Does cognitive science need kernels?,https://www.sciencedirect.com/science/article/pii/S1364661309001430,13,2009,/scholar?cites=4106203044708760638,DZ-fHPgAAAAJ:3bvyWxjaHKcC
14253,"Most existing sparse Gaussian process (gp) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the gp with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on m Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs O (m 2 n) time where n is the number of data points, and compare the method to various other sparse gp methods. Although we focus on gp regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector …",Christian Walder and Kwang In Kim and Bernhard Schölkopf,69,5864804434985856368,,,1112-1119,,Sparse multiscale Gaussian process regression,https://dl.acm.org/doi/abs/10.1145/1390156.1390296,,2008,/scholar?cites=5864804434985856368,DZ-fHPgAAAAJ:URolC5Kub84C
14254,"We consider the problem of constructing a globally smooth analytic function that represents a surface implicitly by way of its zero set, given sample points with surface normal vectors. The contributions of the paper include a novel means of regularising multi-scale compactly supported basis functions that leads to the desirable interpolation properties previously only associated with fully supported bases. We also provide a regularisation framework for simpler and more direct treatment of surface normals, along with a corresponding generalisation of the representer theorem lying at the core of kernel-based machine learning methods.",Christian Walder and Bernhard Schölkopf and Olivier Chapelle,69,16252923376888037436,Computer Graphics Forum,3,635-644,"Amsterdam: North Holland, 1982-",Implicit surface modelling with a globally regularised basis of compact support,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.5196&rep=rep1&type=pdf,25,2006,/scholar?cites=16252923376888037436,DZ-fHPgAAAAJ:owLR8QvbtFgC
14255,"This chapter contains sections titled: Introduction, Gaussian Process Classification, Modeling the Noise, From Gaussian Processes to SVM, Leave-One-Out Estimator, Naive Mean Field Algorithm, Simulation Results, Conclusion",Alexander J Smola and Peter Bartlett and Bernhard Schölkopf and Dale Schuurmans,69,5747017007361092860,,,311-326,MIT press,Gaussian processes and SVM: Mean field and leave-one-out,https://ieeexplore.ieee.org/abstract/document/6274990/,,2000,/scholar?cites=5747017007361092860,DZ-fHPgAAAAJ:topJdmUrRPYC
14256,"A neurorehabilitation approach that combines robot-assisted active physical therapy and Brain-Computer Interfaces (BCIs) may provide an additional mileage with respect to traditional rehabilitation methods for patients with severe motor impairment due to cerebrovascular brain damage (e.g., stroke) and other neurological conditions. In this paper, we describe the design and modes of operation of a robot-based rehabilitation framework that enables artificial support of the sensorimotor feedback loop. The aim is to increase cortical plasticity by means of Hebbian-type learning rules. A BCI-based shared-control strategy is used to drive a Barret WAM 7-degree-of-freedom arm that guides a subject's arm. Experimental validation of our setup is carried out both with healthy subjects and stroke patients. We review the empirical results which we have obtained to date, and argue that they support the feasibility of future …",Manuel Gomez-Rodriguez and M Grosse-Wentrup and J Hill and A Gharabaghi and B Schölkopf and J Peters,68,8971410633006650323,,,1-6,IEEE,Towards brain-robot interfaces in stroke rehabilitation,https://ieeexplore.ieee.org/abstract/document/5975385/,,2011,/scholar?cites=8971410633006650323,DZ-fHPgAAAAJ:jSAVyFp_754C
14257,"The Common Spatial Pattern (CSP) algorithm is a highly successful method for efficiently calculating spatial filters for brain signal classification. Spatial filtering can improve classification performance considerably, but demands that a large number of electrodes be mounted, which is inconvenient in day-to-day BCI usage. The CSP algorithm is also known for its tendency to overfit, ie to learn the noise in the training set rather than the signal. Both problems motivate an approach in which spatial filters are sparsified. We briefly sketch a reformulation of the problem which allows us to do this, using 1-norm regularisation. Focusing on the electrode selection issue, we present preliminary results on EEG data sets that suggest that effective spatial filters may be computed with as few as 10–20 electrodes, hence offering the potential to simplify the practical realisation of BCI systems significantly.",Jason Farquhar and Jeremy Hill and Thomas Navin Lal and Bernhard Schölkopf,68,1621940158457495073,,,,,Regularised CSP for sensor selection in BCI,http://www.neurotechcenter.org/sites/default/files/misc/Regularised%20CSP%20for%20sensor%20selection%20in%20BCI.pdf,,2006,/scholar?cites=1621940158457495073,DZ-fHPgAAAAJ:4MWp96NkSFoC
14258,"The ν-Support Vector Machine for classification (ν-SVC) has been presented as a different formulation for solving SVMs in which the C parameter is transformed by a more meaningful parameter ν, that roughly represents the fraction of support vectors. The value of ν cannot always take all possible values between 0 and 1, which limits the range of possible solutions. Either, because the training set is non-separable in the feature space, or because the classes are unbalanced. In this chapter, we will deal with both restrictions, presenting a new Extended ν-SVC, in which the value of ν can move from 0 to 1 in any circumstance. The modification to extend the range up to 1 is trivial, we only need to modify the cost associated to the margin errors to balance the classes. The modification to extend the range down to zero is far more complex. We will first need to revisit how maximum margin classifiers can be obtained for a …",F Perez-Cruz and J Weston and DJL Herrmann and B Schölkopf,68,14200106545574672800,,,,IOS Press,Extension of the nu-SVM range for classification,http://scholar.google.com/scholar?cluster=14200106545574672800&hl=en&oi=scholarr,190,2003,/scholar?cites=14200106545574672800,DZ-fHPgAAAAJ:WA5NYHcadZ8C
14259,"The ever-growing availability of high-quality genotypes for a multitude of species has enabled researchers to explore the underlying genetic architecture of complex phenotypes at an unprecedented level of detail using genome-wide association studies (GWAS). The systematic comparison of results obtained from GWAS of different traits opens up new possibilities, including the analysis of pleiotropic effects. Other advantages that result from the integration of multiple GWAS are the ability to replicate GWAS signals and to increase statistical power to detect such signals through meta-analyses. In order to facilitate the simple comparison of GWAS results, we present easyGWAS, a powerful, species-independent online resource for computing, storing, sharing, annotating, and comparing GWAS. The easyGWAS tool supports multiple species, the uploading of private genotype data and summary statistics of existing …",Dominik G Grimm and Damian Roqueiro and Patrice A Salomé and Stefan Kleeberger and Bastian Greshake and Wangsheng Zhu and Chang Liu and Christoph Lippert and Oliver Stegle and Bernhard Schölkopf and Detlef Weigel and Karsten M Borgwardt,67,2676325211862386608,The Plant Cell,1,5-19,American Society of Plant Biologists,easyGWAS: A cloud-based platform for comparing the results of genome-wide association studies,http://www.plantcell.org/content/29/1/5?utm_source=TrendMD&utm_medium=cpc&utm_campaign=Plant_Cell_TrendMD_1,29,2017,/scholar?cites=2676325211862386608,DZ-fHPgAAAAJ:vgb0g4rEShEC
14260,"Characterizing how different cortical rhythms interact and how their interaction changes with sensory stimulation is important to gather insights into how these rhythms are generated and what sensory function they may play. Concepts from information theory, such as Transfer Entropy (TE), offer principled ways to quantify the amount of causation between different frequency bands of the signal recorded from extracellular electrodes; yet these techniques are hard to apply to real data. To address the above issues, in this study we develop a method to compute fast and reliably the amount of TE from experimental time series of extracellular potentials. The method consisted in adapting efficiently the calculation of TE to analog signals and in providing appropriate sampling bias corrections. We then used this method to quantify the strength and significance of causal interaction between frequency bands of field …",Michel Besserve and Bernhard Schölkopf and Nikos K Logothetis and Stefano Panzeri,67,18276976581223837432,Journal of computational neuroscience,3,547-566,Springer US,Causal relationships between frequency bands of extracellular signals in visual cortex revealed by an information theoretic analysis,https://link.springer.com/content/pdf/10.1007/s10827-010-0236-5.pdf,29,2010,/scholar?cites=18276976581223837432,DZ-fHPgAAAAJ:Ri6SYOTghG4C
14261,"Exemplar theories of categorization depend on similarity for explaining subjects’ ability to generalize to new stimuli. A major criticism of exemplar theories concerns their lack of abstraction mechanisms and thus, seemingly, of generalization ability. Here, we use insights from machine learning to demonstrate that exemplar models can actually generalize very well. Kernel methods in machine learning are akin to exemplar models and are very successful in real-world applications. Their generalization performance depends crucially on the chosen similarity measure. Although similarity plays an important role in describing generalization behavior, it is not the only factor that controls generalization performance. In machine learning, kernel methods are often combined with regularization techniques in order to ensure good generalization. These same techniques are easily incorporated in exemplar models. We …",Frank Jäkel and Bernhard Schölkopf and Felix A Wichmann,67,14698722206580735079,Psychonomic Bulletin & Review,2,256-271,Springer-Verlag,Generalization and similarity in exemplar models of categorization: Insights from machine learning,https://link.springer.com/article/10.3758/PBR.15.2.256,15,2008,/scholar?cites=14698722206580735079,DZ-fHPgAAAAJ:q3CdL3IzO_QC
14262,"This paper establishes the existence of observable footprints that reveal the"" causal dispositions"" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.",David Lopez-Paz and Robert Nishihara and Soumith Chintala and Bernhard Scholkopf and Léon Bottou,66,4684930995355004969,,,6979-6987,,Discovering causal signals in images,http://openaccess.thecvf.com/content_cvpr_2017/html/Lopez-Paz_Discovering_Causal_Signals_CVPR_2017_paper.html,,2017,/scholar?cites=4684930995355004969,DZ-fHPgAAAAJ:UPWeaTv7XkgC
14263,"In genomic sequence analysis tasks like splice site recognition or promoter identification, large amounts of training sequences are available, and indeed needed to achieve sufficiently high classification performances. In this work we study two recently proposed and successfully used kernels, namely the Spectrum kernel and the Weighted Degree kernel (WD). In particular, we suggest several extensions using Suffix Trees and modifications of an SMO-like SVM training algorithm in order to accelerate the training of the SVMs and their evaluation on test sequences. Our simulations show that for the spectrum kernel and WD kernel, large scale SVM training can be accelerated by factors of 20 and 4 times, respectively, while using much less memory (eg no kernel caching). The evaluation on new sequences is often several thousand times faster using the new techniques (depending on the number of Support Vectors …",Sören Sonnenburg and Gunnar Rätsch and Bernhard Schölkopf,66,17739465576132233706,,,848-855,,Large scale genomic sequence SVM classifiers,https://dl.acm.org/doi/abs/10.1145/1102351.1102458,,2005,/scholar?cites=17739465576132233706,DZ-fHPgAAAAJ:prdVHNxh-e8C
14264,"The Kepler mission has discovered thousands of exoplanets and revolutionized our understanding of their population. This large, homogeneous catalog of discoveries has enabled rigorous studies of the occurrence rate of exoplanets and planetary systems as a function of their physical properties. However, transit surveys such as Kepler are most sensitive to planets with orbital periods much shorter than the orbital periods of Jupiter and Saturn, the most massive planets in our solar system. To address this deficiency, we perform a fully automated search for long-period exoplanets with only one or two transits in the archival Kepler light curves. When applied to the~ 40,000 brightest Sun-like target stars, this search produces 16 long-period exoplanet candidates. Of these candidates, six are novel discoveries and five are in systems with inner short-period transiting planets. Since our method involves no human …",Daniel Foreman-Mackey and Timothy D Morton and David W Hogg and Eric Agol and Bernhard Schölkopf,65,2495337849209835865,The Astronomical Journal,6,206,IOP Publishing,The population of long-period transiting exoplanets,https://iopscience.iop.org/article/10.3847/0004-6256/152/6/206/meta,152,2016,/scholar?cites=2495337849209835865,DZ-fHPgAAAAJ:Sc6NBd4wEwEC
14265,"The abilities to learn and to categorize are fundamental for cognitive systems, be it animals or machines, and therefore have attracted attention from engineers and psychologists alike. Modern machine learning methods and psychological models of categorization are remarkably similar, partly because these two fields share a common history in artificial neural networks and reinforcement learning. However, machine learning is now an independent and mature field that has moved beyond psychologically or neurally inspired algorithms towards providing foundations for a theory of learning that is rooted in statistics and functional analysis. Much of this research is potentially interesting for psychological theories of learning and categorization but also hardly accessible for psychologists. Here, we provide a tutorial introduction to a popular class of machine learning tools, called kernel methods. These methods are …",Frank Jäkel and Bernhard Schölkopf and Felix A Wichmann,65,5536762195076707242,,6,343-358,Academic Press,A tutorial on kernel methods for categorization,https://www.sciencedirect.com/science/article/pii/S0022249607000375,51,2007,/scholar?cites=5536762195076707242,DZ-fHPgAAAAJ:KUbvn5osdkgC
14266,"Determining conditional independence (CI) relationships between random variables is a challenging but important task for problems such as Bayesian network learning and causal discovery. We propose a new kernel CI test that uses a single, learned permutation to convert the CI test problem into an easier two-sample test problem. The learned permutation leaves the joint distribution unchanged if and only if the null hypothesis of CI holds. Then, a kernel two-sample test, which has been studied extensively in prior work, can be applied to a permuted and an unpermuted sample to test for CI. We demonstrate that the test (1) easily allows the incorporation of prior knowledge during the permutation step,(2) has power competitive with state-of-the-art kernel CI tests, and (3) accurately estimates the null distribution of the test statistic, even as the dimensionality of the conditioning variable grows.",Gary Doran and Krikamol Muandet and Kun Zhang and Bernhard Schölkopf,63,2039442272852628723,,,132-141,,A Permutation-Based Kernel Conditional Independence Test.,https://dslpitt.org/papers/14/p132-doran.pdf,,2014,/scholar?cites=2039442272852628723,DZ-fHPgAAAAJ:_AkkBXT-jcoC
14267,"We show via an equivalence of mathematical programs that a Support Vector (SV) algorithm can be translated into an equivalent boosting-like algorithm and vice versa. We exemplify this translation procedure for a new algorithm—one-class Leveraging—starting from the one-class Support Vector Machines (1-SVM). This is a first step towards unsupervised learning in a Boosting framework. Building on so-called barrier methods known from the theory of constrained optimization, it returns a function, written as a convex combination of basis hypotheses, that characterizes whether a given test point is likely to have been generated from the distribution underlying the training data. Simulations on one-class classification problems demonstrate the usefulness of our approach.",Gunnar Rätsch and Bernhard Schölkopf and Sebastian Mika and Klaus-Robert Müller,63,14365535296682733856,Submitted to NIPS’00,,,,SVM and boosting: One class,https://www.researchgate.net/profile/Gunnar_Raetsch/publication/228892955_SVM_and_boosting_One_class/links/0fcfd50bdecf53f1f6000000.pdf,,2000,/scholar?cites=14365535296682733856,DZ-fHPgAAAAJ:t7zJ5fGR-2UC
14268,"We explore whether we can observe Time's Arrow in a temporal sequence--is it possible to tell whether a video is running forwards or backwards? We investigate this somewhat philosophical question using computer vision and machine learning techniques. We explore three methods by which we might detect Time's Arrow in video sequences, based on distinct ways in which motion in video sequences might be asymmetric in time. We demonstrate good video forwards/backwards classification results on a selection of YouTube video clips, and on natively-captured sequences (with no temporally-dependent video compression), and examine what motions the models have learned that help discriminate forwards from backwards time.",Lyndsey C Pickup and Zheng Pan and Donglai Wei and YiChang Shih and Changshui Zhang and Andrew Zisserman and Bernhard Scholkopf and William T Freeman,61,15813295680107448518,,,2035-2042,,Seeing the arrow of time,http://openaccess.thecvf.com/content_cvpr_2014/html/Pickup_Seeing_the_Arrow_2014_CVPR_paper.html,,2014,/scholar?cites=15813295680107448518,DZ-fHPgAAAAJ:DUFsPKDdMi0C
14269,"From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could benefit from the use of error-correcting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a significantly reduced average target-to-target interval (TTI), leading to difficulties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, finding an interaction between the two factors. Our data demonstrate that the traditional, row-column code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used.",Jeremy Hill and Jason Farquhar and Suzanna Martens and Felix Bießmann and Bernhard Schölkopf,61,16427750721004576783,Advances in neural information processing systems,,665-672,,Effects of stimulus type and of error-correcting code design on BCI speller performance,https://proceedings.neurips.cc/paper/2008/hash/c058f544c737782deacefa532d9add4c-Abstract.html,21,2008,/scholar?cites=16427750721004576783,DZ-fHPgAAAAJ:BUYA1_V_uYcC
14270,"Model selection in Support Vector machines is usually carried out by minimizing the quotient of the radius of the smallest enclosing sphere of the data and the observed margin on the training set. We provide a new criterion taking the distribution within that sphere into account by considering the Gram matrix of the data. In particular, this makes use of the eigenvalue distribution of the matrix. Experimental results on real world data show that this new criterion provides a good prediction of the shape of the curve relating generalization error to kernel width.",Bernhard Schölkopf and John Shawe-Taylor and Alexander J Smola and Robert C Williamson,61,17779411112432761551,,,,,Generalization bounds via eigenvalues of the gram matrix,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.349,,1999,/scholar?cites=17779411112432761551,DZ-fHPgAAAAJ:kw52XkFRtyQC
14271,"Every mathematical discipline goes through three periods of development: the naive, the formal, and the critical.",M Jordan and J Kleinberg and B Schölkopf,60,953654009071217560,,,,"Cambridge, UK: Springer",Information science and statistics,https://link.springer.com/content/pdf/10.1007/978-0-387-77242-4.pdf,,2006,/scholar?cites=953654009071217560,DZ-fHPgAAAAJ:QIV2ME_5wuYC
14272,"Subject motion can severely degrade MR images. A retrospective motion correction algorithm, Gradient‐based motion correction, which significantly reduces ghosting and blurring artifacts due to subject motion was proposed. The technique uses the raw data of standard imaging sequences; no sequence modifications or additional equipment such as tracking devices are required. Rigid motion is assumed.The approach iteratively searches for the motion trajectory yielding the sharpest image as measured by the entropy of spatial gradients. The vast space of motion parameters is efficiently explored by gradient‐based optimization with a convergence guarantee.The method has been evaluated on both synthetic and real data in two and three dimentions using standard imaging techniques. MR images are consistently improved over different kinds of motion trajectories. Using a graphics …",Alexander Loktyushin and Hannes Nickisch and Rolf Pohmann and Bernhard Schölkopf,59,1430982419017075414,Magnetic resonance in medicine,6,1608-1618,,Blind retrospective motion correction of MR images,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.24615,70,2013,/scholar?cites=1430982419017075414,DZ-fHPgAAAAJ:M0leSnx2MbUC
14273,"We briefly describe the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. In addition, we present an overview of applications of kernel methods in bioinformatics. 1",Bernhard Schölkopf and Isabelle Guyon and Jason Weston,59,13486153320623285308,,,1-21,IOS press,Statistical learning and kernel methods in bioinformatics,https://pure.mpg.de/rest/items/item_1792624/component/file_3185879/content,,2003,/scholar?cites=13486153320623285308,DZ-fHPgAAAAJ:ce2CqMG-AY4C
14274,"GRavitational lEnsing Accuracy Testing 2010 (GREAT10) is a public image analysis challenge aimed at the development of algorithms to analyze astronomical images. Specifically, the challenge is to measure varying image distortions in the presence of a variable convolution kernel, pixelization and noise. This is the second in a series of challenges set to the astronomy, computer science and statistics communities, providing a structured environment in which methods can be improved and tested in preparation for planned astronomical surveys. GREAT10 extends upon previous work by introducing variable fields into the challenge. The ""Galaxy Challenge"" involves the precise measurement of galaxy shape distortions, quantified locally by two parameters called shear, in the presence of a known convolution kernel. Crucially, the convolution kernel and the simulated gravitational lensing shape distortion both now …",Thomas Kitching and Adam Amara and Mandeep Gill and Stefan Harmeling and Catherine Heymans and Richard Massey and Barnaby Rowe and Tim Schrabback and Lisa Voigt and Sreekumar Balan and Gary Bernstein and Matthias Bethge and Sarah Bridle and Frederic Courbin and Marc Gentile and Alan Heavens and Michael Hirsch and Reshad Hosseini and Alina Kiessling and Donnacha Kirk and Konrad Kuijken and Rachel Mandelbaum and Baback Moghaddam and Guldariya Nurbaeva and Stephane Paulin-Henriksson and Anais Rassat and Jason Rhodes and Bernhard Schölkopf and John Shawe-Taylor and Marina Shmakova and Andy Taylor and Malin Velander and Ludovic van Waerbeke and Dugan Witherick and David Wittman,58,37704143306195438,The Annals of applied statistics,,2231-2263,Institute of Mathematical Statistics,Gravitational lensing accuracy testing 2010 (great10) challenge handbook,https://www.jstor.org/stable/23069371,,2011,/scholar?cites=37704143306195438,DZ-fHPgAAAAJ:OU6Ihb5iCvQC
14275,"In order to apply the maximum margin method in arbitrary metric spaces, we suggest to embed the metric space into a Banach or Hilbert space and to perform linear classification in this space. We propose several embeddings and recall that an isometric embedding in a Banach space is always possible while an isometric embedding in a Hilbert space is only possible for certain metric spaces. As a result, we obtain a general maximum margin classification algorithm for arbitrary metric spaces (whose solution is approximated by an algorithm of Graepel et al. (International Conference on Artificial Neural Networks 1999, pp. 304–309)). Interestingly enough, the embedding approach, when applied to a metric which can be embedded into a Hilbert space, yields the support vector machine (SVM) algorithm, which emphasizes the fact that its solution depends on the metric and not on the kernel. Furthermore, we give …",Matthias Hein and Olivier Bousquet and Bernhard Schölkopf,58,5201481591404786610,Journal of Computer and System Sciences,3,333-359,Academic Press,Maximal margin classification for metric spaces,https://www.sciencedirect.com/science/article/pii/S0022000004001412,71,2005,/scholar?cites=5201481591404786610,DZ-fHPgAAAAJ:kWvqk_afx_IC
14276,"We describe a modular framework for video frame prediction. We refer to it as a Flexible Spatio-Temporal Network (FSTN) as it allows the extrapolation of a video sequence as well as the estimation of synthetic frames lying in between observed frames and thus the generation of slow-motion videos. By devising a customized objective function comprising decoding, encoding, and adversarial losses, we are able to mitigate the common problem of blurry predictions, managing to retain high frequency information even for relatively distant future predictions. We propose and analyse different training strategies to optimize our model. Extensive experiments on several challenging public datasets demonstrate both the versatility and validity of our model.",Chaochao Lu and Michael Hirsch and Bernhard Scholkopf,57,8260444226677990814,,,6523-6531,,Flexible spatio-temporal networks for video prediction,http://openaccess.thecvf.com/content_cvpr_2017/html/Lu_Flexible_Spatio-Temporal_Networks_CVPR_2017_paper.html,,2017,/scholar?cites=8260444226677990814,DZ-fHPgAAAAJ:gHzeFJ_WuCsC
14277,"Hybrid PET/MR systems have recently entered clinical practice. Thus, the accuracy of MR-based attenuation correction in simultaneously acquired data can now be investigated. We assessed the accuracy of 4 methods of MR-based attenuation correction in lesions within soft tissue, bone, and MR susceptibility artifacts: 2 segmentation-based methods (SEG1, provided by the manufacturer, and SEG2, a method with atlas-based susceptibility artifact correction); an atlas- and pattern recognition–based method (AT&PR), which also used artifact correction; and a new method combining AT&PR and SEG2 (SEG2wBONE). Methods: Attenuation maps were calculated for the PET/MR datasets of 10 patients acquired on a whole-body PET/MR system, allowing for simultaneous acquisition of PET and MR data. Eighty percent iso-contour volumes of interest were placed on lesions in soft tissue (n = 21), in bone (n = 20), near …",Ilja Bezrukov and Holger Schmidt and Frédéric Mantlik and Nina Schwenzer and Cornelia Brendle and Bernhard Schölkopf and Bernd J Pichler,56,17786019974341640153,Journal of Nuclear Medicine,10,1768-1774,Society of Nuclear Medicine,MR-based attenuation correction methods for improved PET quantification in lesions within bone and susceptibility artifact regions,http://jnm.snmjournals.org/content/54/10/1768.short,54,2013,/scholar?cites=17786019974341640153,DZ-fHPgAAAAJ:WIXB4To3Tx4C
14278,"Inference of human intention may be an essential step towards understanding human actions and is hence important for realizing efficient human-robot interaction. In this paper, we propose the Intention-Driven Dynamics Model (IDDM), a latent variable model for inferring unknown human intentions. We train the model based on observed human movements/actions. We introduce an efficient approximate inference algorithm to infer the human’s intention from an ongoing movement. We verify the feasibility of the IDDM in two scenarios, ie, target inference in robot table tennis and action recognition for interactive humanoid robots. In both tasks, the IDDM achieves substantial improvements over state-of-the-art regression and classification.",Zhikun Wang and Marc Peter Deisenroth and Heni Ben Amor and David Vogt and Bernhard Schölkopf and Jan Peters,56,5072897786651063137,"Proceedings of robotics: Science and systems, VIII",,,,Probabilistic modeling of human movements for intention inference,http://books.google.com/books?hl=en&lr=&id=NOrxCwAAQBAJ&oi=fnd&pg=PA433&dq=info:YbMz26iNZkYJ:scholar.google.com&ots=dNHkglfsPW&sig=IJxOCJ2sM5LjNYLgqeIflO9fd50,,2012,/scholar?cites=5072897786651063137,DZ-fHPgAAAAJ:RuPIJ_LgqDgC
14279,"The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufficiently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data.",Francesco Dinuzzo and Bernhard Schölkopf,56,1026473944260752675,Advances in neural information processing systems,,189-196,,The representer theorem for Hilbert spaces: a necessary and sufficient condition,http://papers.nips.cc/paper/4841-the-representer-theorem-for-hilbert-spaces-a-necessary-and-sufficient-condition,25,2012,/scholar?cites=1026473944260752675,DZ-fHPgAAAAJ:4aZ_i-5WJEQC
14280,"Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. Recently, methods using additive noise models have been suggested to approach the case of continuous variables. In many situations, however, the variables of interest are discrete or even have only finitely many states. In this work we extend the notion of additive noise models to these cases. Whenever the joint distribution P (X, Y) admits such a model in one direction, eg Y= f (X)+ N, N⊥⊥ X, it does not admit the reversed model X= g (Y)+ N, N⊥⊥ Y as long as the model is chosen in a generic way. Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables. We show that this algorithm works both on synthetic and real data sets.",Jonas Peters and Dominik Janzing and Bernhard Schölkopf,56,15460195782528535698,,,597-604,,Identifying cause and effect on discrete data using additive noise models,http://www.jmlr.org/proceedings/papers/v9/peters10a/peters10a.pdf,,2010,/scholar?cites=15460195782528535698,DZ-fHPgAAAAJ:mlAyqtXpCwEC
14281,,Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,56,14752685886276723087,"MIT Press, Cambridge, MA, USA. Cited in page (s)",1,2,,Semi-supervised Learning. Adaptive computation and machine learning,http://scholar.google.com/scholar?cluster=14752685886276723087&hl=en&oi=scholarr,21,2010,/scholar?cites=14752685886276723087,DZ-fHPgAAAAJ:4opXj0ecdLoC
14282,"This paper presents a fully automated algorithm for reconstructing a textured 3D model of a face from a single photograph or a raw video stream. The algorithm is based on a combination of Support Vector Machines (SVMs) and a Morphable Model of 3D faces. After SVM face detection, individual facial features are detected using a novel regression- and classification-based approach, and probabilistically plausible configurations of features are selected to produce a list of candidates for several facial feature positions. In the next step, the configurations of feature points are evaluated using a novel criterion that is based on a Morphable Model and a combination of linear projections. To make the algorithm robust with respect to head orientation, this process is iterated while the estimate of pose is refined. Finally, the feature points initialize a model-fitting procedure of the Morphable Model. The result is a high resolution …",Pia Breuer and Kwang-In Kim and Wolf Kienzle and Bernhard Scholkopf and Volker Blanz,56,2632241395992753676,,,1-8,IEEE,Automatic 3D face reconstruction from single images or video,https://ieeexplore.ieee.org/abstract/document/4813339/,,2008,/scholar?cites=2632241395992753676,DZ-fHPgAAAAJ:oi2SiIJ9l4AC
14283,"Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment matching in that space. This allows us to tailor density estimators to a function class of interest (ie, for which we would like to compute expectations). We show our density estimation approach is useful in applications such as message compression in graphical models, and image classification and retrieval.",Le Song and Xinhua Zhang and Alex Smola and Arthur Gretton and Bernhard Schölkopf,56,10027224020702063115,,,992-999,,Tailoring density estimation via reproducing kernel moment matching,https://dl.acm.org/doi/abs/10.1145/1390156.1390281,,2008,/scholar?cites=10027224020702063115,DZ-fHPgAAAAJ:0N-VGjzr574C
14284,"We show how the SVM can be viewed as a maximum likelihood estimate of a class of probabilistic models. This model class can be viewed as a reparametrization of the SVM in a similar vein to the ν-SVM reparametrizing the classical (C-) SVM. It is not discriminative, but has a non-uniform marginal. We illustrate the benefits of this new view by rederiving and re-investigating two established SVM-related algorithms.",Vojtech Franc and Alexander Zien and Bernhard Schölkopf,55,8006092704255779601,,,665-672,,Support vector machines as probabilistic models,https://www.researchgate.net/profile/Vojtch_Franc3/publication/221345053_Support_Vector_Machines_as_Probabilistic_Models/links/55b20b8208aec0e5f4313e3f/Support-Vector-Machines-as-Probabilistic-Models.pdf,,2011,/scholar?cites=8006092704255779601,DZ-fHPgAAAAJ:isU91gLudPYC
14285,,Chapelle Olivier and Schölkopf Bernhard and Zien Alexander,55,12079349377078025811,,3,542-542,,Semi-supervised learning,http://scholar.google.com/scholar?cluster=12079349377078025811&hl=en&oi=scholarr,20,2006,/scholar?cites=12079349377078025811,DZ-fHPgAAAAJ:ZPyn03lkmQEC
14286,"A widely applied approach to causal inference from a time series X, often referred to as “(linear) Granger causal analysis”, is to simply regress present on past and interpret the regression matrixˆB causally. However, if there is an unmeasured time series Z that influences X, then this approach can lead to wrong causal conclusions, ie, distinct from those one would draw if one had additional information such as Z. In this paper we take a different approach: We assume that X together with some hidden Z forms a first order vector autoregressive (VAR) process with transition matrix A, and argue why it is more valid to interpret A causally instead ofˆB. Then we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from X to Z. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using X.",Philipp Geiger and Kun Zhang and Bernhard Schoelkopf and Mingming Gong and Dominik Janzing,54,10356800094709183572,,,1917-1925,,Causal inference by identification of vector autoregressive processes with hidden components,http://www.jmlr.org/proceedings/papers/v37/geiger15.pdf,,2015,/scholar?cites=10356800094709183572,DZ-fHPgAAAAJ:5lpRS_NW7LAC
14287,"Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning.",Bernhard Schölkopf,53,9878937533285205636,arXiv preprint arXiv:1911.10500,,,,Causality for machine learning,https://arxiv.org/abs/1911.10500,,2019,/scholar?cites=9878937533285205636,DZ-fHPgAAAAJ:RPh6sY6S0Z0C
14288,"We propose a method for inferring the existence of a latent common cause ('confounder') of two observed random variables. The method assumes that the two effects of the confounder are (possibly nonlinear) functions of the confounder plus independent, additive noise. We discuss under which conditions the model is identifiable (up to an arbitrary reparameterization of the confounder) from the joint distribution of the effects. We state and prove a theoretical result that provides evidence for the conjecture that the model is generically identifiable under suitable technical conditions. In addition, we propose a practical method to estimate the confounder from a finite iid sample of the effects and illustrate that the method works well on both simulated and real-world data.",Dominik Janzing and Jonas Peters and Joris Mooij and Bernhard Schölkopf,53,8832860749953204555,,,249-257,AUAI Press,Identifying confounders using additive noise models,https://arxiv.org/abs/1205.2640,,2009,/scholar?cites=8832860749953204555,DZ-fHPgAAAAJ:9c2xU6iGI7YC
14289,"We describe a causal learning method, which employs measuring the strength of statistical dependences in terms of the Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based causal learning, our approach assumes that a variable Z is likely to be a common effect of X and Y, if conditioning on Z increases the dependence between X and Y. Based on this assumption, we collect"" votes"" for hypothetical causal directions and orient the edges by the majority principle. In most experiments with known causal structures, our method provided plausible results and outperformed the conventional constraint-based PC algorithm.",Xiaohai Sun and Dominik Janzing and Bernhard Schölkopf and Kenji Fukumizu,53,17509949490211205145,,,855-862,,A kernel-based causal learning algorithm,https://dl.acm.org/doi/abs/10.1145/1273496.1273604,,2007,/scholar?cites=17509949490211205145,DZ-fHPgAAAAJ:xtoqd-5pKcoC
14290,"The problem of feature selection is a difficult combinatorial task in Machine Learning and of high practical relevance, e.g. in bioinformatics. Genetic Algorithms (GAs) offer a natural way to solve this problem. In this paper we present a special Genetic Algorithm, which especially takes into account the existing bounds on the generalization error for Support Vector Machines (SVMs). This new approach is compared to the traditional method of performing cross-validation and to other existing algorithms for feature selection.",Holger Fröhlich and Olivier Chapelle and Bernhard Schölkopf,53,5303507867987864873,International journal on artificial intelligence tools,04,791-800,World Scientific Publishing Company,Feature selection for support vector machines using genetic algorithms,https://www.worldscientific.com/doi/abs/10.1142/S0218213004001818,13,2004,/scholar?cites=5303507867987864873,DZ-fHPgAAAAJ:Fja2VXgnB68C
14291,"Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules.",Partha Ghosh and Mehdi SM Sajjadi and Antonio Vergari and Michael Black and Bernhard Schölkopf,52,10583740506297544895,arXiv preprint arXiv:1903.12436,,,,From variational to deterministic autoencoders,https://arxiv.org/abs/1903.12436,,2019,/scholar?cites=10583740506297544895,DZ-fHPgAAAAJ:_eaeta6ualoC
14292,"Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the highfrequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches.",Mingming Gong and Kun Zhang and Bernhard Schoelkopf and Dacheng Tao and Philipp Geiger,52,5229466014222980256,,,1898-1906,,Discovering temporal causal relations from subsampled data,http://www.jmlr.org/proceedings/papers/v37/gongb15.pdf,,2015,/scholar?cites=5229466014222980256,DZ-fHPgAAAAJ:STPpQ-Nbj3QC
14293,"We address the problem of causal discovery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X→ Y (X causes Y), the marginal distribution of the cause, P (X), contains no information about the conditional distribution P (Y| X). Consequently, estimating P (Y| X) from P (X) should not be possible. However, estimating P (X| Y) based on P (Y) may be possible.This paper employs this asymmetry to propose CURE, a causal discovery method which decides upon the causal direction by comparing the accuracy of the estimations of P (Y| X) and P (X| Y). To this end, we propose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP regression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference methods.",Eleni Sgouritsa and Dominik Janzing and Philipp Hennig and Bernhard Schölkopf,52,1398969347538819417,,,847-855,,Inference of cause and effect with unsupervised inverse regression,http://www.jmlr.org/proceedings/papers/v38/sgouritsa15.pdf,,2015,/scholar?cites=1398969347538819417,DZ-fHPgAAAAJ:Uq2dEd23X6oC
14294,"In many applications, relationships among objects of interest are more complex than pairwise. Simply approximating complex relationships as pairwise ones can lead to loss of information. An alternative for these applications is to analyze complex relationships among data directly, without the need to first represent the complex relationships into pairwise ones. A natural way to describe complex relationships is to use hypergraphs. A hypergraph is a graph in which edges can connect more than two vertices. Thus we consider learning from a hypergraph, and develop a general framework which is applicable to classification and clustering for complex relational data. We have applied our framework to real-world web classification problems and obtained encouraging results.",Dengyong Zhou and Jiayuan Huang and Bernhard Schölkopf,52,16808043637380071222,,,,Max Planck Institute for Biological Cybernetics,Beyond pairwise classification and clustering using hypergraphs,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1791366,,2005,/scholar?cites=16808043637380071222,DZ-fHPgAAAAJ:pAkWuXOU-OoC
14295,"For IEEE to continue sending you helpful information on our products and services, please consent 
to our updated Privacy Policy … I have read and accepted the IEEE Privacy Policy … A 
not-for-profit organization, IEEE is the world's largest technical professional organization dedicated 
to advancing technology for the benefit of humanity. © Copyright 2019 IEEE - All rights 
reserved. Use of this web site signifies your agreement to the terms and conditions.  ",Alexander Smola and Peter Barlett and Bernhard Schoelkopf and Dale Schuurmans,52,11905513703047361717,,,,MIT Press,Introduction to large margin classifiers,https://ieeexplore.ieee.org/abstract/document/6274975/,,2000,/scholar?cites=11905513703047361717,DZ-fHPgAAAAJ:x21FZCSn4ZoC
14296,"A computer-implemented method is provided for ranking features within a large dataset containing a large number of features according to each feature's ability to separate data into classes. For each feature, a support vector machine separates the dataset into two classes and determines the margins between extremal points in the two classes. The margins for all of the features are compared and the features are ranked based upon the size of the margin, with the highest ranked features corresponding to the largest margins. A subset of features for classifying the dataset is selected from a group of the highest ranked features. In one embodiment, the method is used to identify the best genes for disease prediction and diagnosis using gene expression data from micro-arrays.",,51,15312186941870478453,,,,,Pre-processed feature ranking for a support vector machine,https://patents.google.com/patent/US7475048B2/en,,2009,/scholar?cites=15312186941870478453,DZ-fHPgAAAAJ:blknAaTinKkC
14297,,Y Weiss and B Schölkopf and J Platt,51,17258458485706161996,"MA, USA",,147-154,,MIT Press: Cambridge,http://scholar.google.com/scholar?cluster=17258458485706161996&hl=en&oi=scholarr,,2006,/scholar?cites=17258458485706161996,DZ-fHPgAAAAJ:5fCWA2nXjDoC
14298,"We discuss reproducing kernel Hilbert space (RKHS)-based measures of statistical dependence, with emphasis on constrained covariance (COCO), a novel criterion to test dependence of random variables. We show that COCO is a test for independence if and only if the associated RKHSs are universal. That said, no independence test exists that can distinguish dependent and independent random variables in all circumstances. Dependent random variables can result in a COCO which is arbitrarily close to zero when the source densities are highly non-smooth. All current kernel-based independence tests share this behaviour. We demonstrate exponential convergence between the population and empirical COCO. Finally, we use COCO as a measure of joint neural activity between voxels in MRI recordings of the macaque monkey, and compare the results to the mutual information and the correlation. We also show the effect of removing breathing artefacts from the MRI recording.",Arthur Gretton and Alex Smola and Olivier Bousquet and Ralf Herbrich and Andreas Belitski and Mark Augath and Yusuke Murayama and Jon Pauls and Bernhard Schölkopf and Nikos Logothetis,51,11087363827669785499,,,,,Kernel constrained covariance for dependence measurement,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.8829&rep=rep1&type=pdf#page=121,,2005,/scholar?cites=11087363827669785499,DZ-fHPgAAAAJ:p__nRnzSRKYC
14299,"Many settings of unsupervised learning can be viewed as quantization problems — the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised settings. Moreover, this setting is very closely related to both principal curves and the generative topographic map.We explore this connection in two ways: 1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways. Experimental results demonstrate the feasibility of the approach. 2) We derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give good bounds on the covering numbers which allows us to obtain a nearly optimal learning rate of order   for certain types of regularization operators, where m is the sample …",Alex J Smola and Robert C Williamson and Sebastian Mika and Bernhard Schölkopf,51,8186429282886053495,,,214-229,"Springer, Berlin, Heidelberg",Regularized principal manifolds,https://link.springer.com/chapter/10.1007/3-540-49097-3_17,,1999,/scholar?cites=8186429282886053495,DZ-fHPgAAAAJ:PYBJJbyH-FwC
14300,"The ability to operate a brain-computer interface (BCI) varies not only across subjects but also across time within each individual subject. In this article, we review recent progress in understanding the origins of such variations for BCIs based on the sensorimotor-rhythm (SMR). We propose a classification of studies according to four categories, and argue that an investigation of the neuro-physiological correlates of within-subject variations is likely to have a large impact on the design of future BCIs. We place a special emphasis on our own work on the neuro-physiological causes of performance variations, and argue that attentional networks in the gamma-range ( Hz) are likely to play a critical role in this context. We conclude the review with a discussion of outstanding problems.",Moritz Grosse-Wentrup and Bernhard Schölkopf,50,8771918622518546615,,,39-51,"Springer, Berlin, Heidelberg",A review of performance variations in SMR-based Brain− Computer interfaces (BCIs),https://link.springer.com/chapter/10.1007/978-3-642-36083-1_5,,2013,/scholar?cites=8771918622518546615,DZ-fHPgAAAAJ:60iIaj97TE0C
14301,"This paper presents an approach to build Sparse Large Margin Classifiers (SLMC) by adding one more constraint to the standard Support Vector Machine (SVM) training problem. The added constraint explicitly controls the sparseness of the classifier and an approach is provided to solve the formulated problem. When considering the dual of this problem. it can be seen that building an SLMC is equivalent to constructing an SVM with a modified kernel function. Further analysis of this kernel function indicates that the proposed approach essentially finds a discriminating subspace that can be spanned by a small number of vectors, and in this subspace different classes of data are linearly well separated. Experimental results over several classification benchmarks show that in most cases the proposed approach outperforms the state-of-art sparse learning algorithms.",Mingrui Wu and Bernhard Schölkopf and Gökhan Bakir,50,16200170391660594629,,,996-1003,,Building sparse large margin classifiers,https://dl.acm.org/doi/abs/10.1145/1102351.1102477,,2005,/scholar?cites=16200170391660594629,DZ-fHPgAAAAJ:uc_IGeMz5qoC
14302,"We show how, and under which conditions, the equilibrium states of a first-order Ordinary Differential Equation (ODE) system can be described with a deterministic Structural Causal Model (SCM). Our exposition sheds more light on the concept of causality as expressed within the framework of Structural Causal Models, especially for cyclic models.",Joris M Mooij and Dominik Janzing and Bernhard Schölkopf,49,13292883235581377123,,,,AUAI Press,From ordinary differential equations to structural causal models: the deterministic case,https://arxiv.org/abs/1304.7920,,2013,/scholar?cites=13292883235581377123,DZ-fHPgAAAAJ:NtCmTCuxid4C
14303,"Due to recent advances in genotyping technologies, mapping phenotypes to single loci in the genome has become a standard technique in statistical genetics. However, one-locus mapping fails to explain much of the phenotypic variance in complex traits. Here, we present GLIDE, which maps phenotypes to pairs of genetic loci and systematically searches for the epistatic interactions expected to reveal part of this missing heritability. GLIDE makes use of the computational power of consumer-grade graphics cards to detect such interactions via linear regression. This enabled us to conduct a systematic two-locus mapping study on seven disease data sets from the Wellcome Trust Case Control Consortium and on in-house hippocampal volume data in 6 h per data set, while current single CPU-based approaches require more than a year’s time to complete the same task.",Tony Kam-Thong and Chloé-Agathe Azencott and Lawrence Cayton and Benno Pütz and André Altmann and Nazanin Karbalai and Philipp G Sämann and Bernhard Schölkopf and Bertram Müller-Myhsok and Karsten M Borgwardt,49,13450376787781968581,Human heredity,4,220-236,Karger Publishers,GLIDE: GPU-based linear regression for detection of epistasis,https://www.karger.com/Article/Abstract/341885,73,2012,/scholar?cites=13450376787781968581,DZ-fHPgAAAAJ:LK8CI43ZvvMC
14304,"We formulate the multiframe blind deconvolution problem in an incremental expectation maximization (EM) framework. Beyond deconvolution, we show how to use the same framework to address: (i) super-resolution despite noise and unknown blurring; (ii) saturation-correction of overexposed pixels that confound image restoration. The abundance of data allows us to address both of these without using explicit image or blur priors. The end result is a simple but effective algorithm with no hyperparameters. We apply this algorithm to real-world images from astronomy and to super resolution tasks: for both, our algorithm yields increased resolution and deconvolved images simultaneously.",Stefan Harmeling and Suvrit Sra and Michael Hirsch and Bernhard Schölkopf,49,4954228233881071097,,,3313-3316,IEEE,"Multiframe blind deconvolution, super-resolution, and saturation correction via incremental EM",https://ieeexplore.ieee.org/abstract/document/5651650/,,2010,/scholar?cites=4954228233881071097,DZ-fHPgAAAAJ:2l5NCbZemmgC
14305,"We describe a method for inferring linear causal relations among multi-dimensional variables. The idea is to use an asymmetry between the distributions of cause and effect that occurs if both the covariance matrix of the cause and the structure matrix mapping cause to the effect are independently chosen. The method works for both stochastic and deterministic causal relations, provided that the dimensionality is sufficiently high (in some experiments, 5 was enough). It is applicable to Gaussian as well as non-Gaussian data.",Dominik Janzing and Patrik O Hoyer and Bernhard Schölkopf,49,13762600130539957540,Arxiv preprint arXiv:0909.4386,,,,Telling cause from effect based on high-dimensional observations,https://arxiv.org/abs/0909.4386,,2009,/scholar?cites=13762600130539957540,DZ-fHPgAAAAJ:D_sINldO8mEC
14306,,Stefan Harmeling and Suvrit Sra and M Hirsch and Bernhard Schölkopf,49,2588800296683084921,,,,,Online blind deconvolution for Astronomy,,,2009,/scholar?cites=2588800296683084921,DZ-fHPgAAAAJ:geHnlv5EZngC
14307,"Many graph-based semi-supervised learning methods can be viewed as imposing smoothness conditions on the target function with respect to a graph representing the data points to be labeled. The smoothness properties of the functions are encoded in terms of Mercer kernels over the graph. The central quantity in such regularization is the spectral decomposition of the graph Laplacian, a matrix derived from the graph’s edge weights. The eigenvectors with small eigenvalues are smooth, and ideally represent large cluster structures within the data. The eigenvectors having large eigenvalues are rugged, and considered noise. Different weightings of the eigenvectors of the graph Laplacian lead to different measures of smoothness. Such weightings can be viewed as spectral transforms, that is, as transformations of the standard eigenspectrum that lead to different regularizers over the graph. Familiar kernels, such as the diffusion kernel resulting by solving a discrete heat equation on the graph, can be seen as simple parametric spectral transforms.The question naturally arises whether one can obtain effective spectral transforms automatically. In this paper we develop an approach to searching over a nonparametric family of spectral transforms by using convex optimization to maximize kernel alignment to the labeled data. Order constraints are imposed to encode a preference for smoothness with respect to the graph structure. This results in a flexible family of kernels that is more data-driven than the standard parametric spectral transforms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally …",Xiaojin Zhu and Jaz S Kandola and John Lafferty and Zoubin Ghahramani,49,18116102567963887096,,,276-291,,Graph Kernels by Spectral Transforms.,http://pages.cs.wisc.edu/~jerryzhu/pub/ssl-book.pdf,,2006,/scholar?cites=18116102567963887096,DZ-fHPgAAAAJ:5aR9HdqQzEIC
14308,"Distributed neural processing likely entails the capability of networks to reconfigure dynamically the directionality and strength of their functional connections. Yet, the neural mechanisms that may allow such dynamic routing of the information flow are not yet fully understood. We investigated the role of gamma band (50–80 Hz) oscillations in transient modulations of communication among neural populations by using measures of direction-specific causal information transfer. We found that the local phase of gamma-band rhythmic activity exerted a stimulus-modulated and spatially-asymmetric directed effect on the firing rate of spatially separated populations within the primary visual cortex. The relationships between gamma phases at different sites (phase shifts) could be described as a stimulus-modulated gamma-band wave propagating along the spatial directions with the largest information transfer. We observed transient stimulus-related changes in the spatial configuration of phases (compatible with changes in direction of gamma wave propagation) accompanied by a relative increase of the amount of information flowing along the instantaneous direction of the gamma wave. These effects were specific to the gamma-band and suggest that the time-varying relationships between gamma phases at different locations mark, and possibly causally mediate, the dynamic reconfiguration of functional connections.",Michel Besserve and Scott C Lowe and Nikos K Logothetis and Bernhard Schölkopf and Stefano Panzeri,48,7816166118590582427,PLoS Biol,9,e1002257,Public Library of Science,Shifts of gamma phase across primary visual cortical sites reflect dynamic stimulus-modulated information transfer,https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002257,13,2015,/scholar?cites=7816166118590582427,DZ-fHPgAAAAJ:QJP_a5eTMeYC
14309,"Many common machine learning methods such as support vector machines or Gaussian process inference make use of positive definite kernels, reproducing kernel Hilbert spaces, Gaussian processes, and regularization operators. In this work these objects are presented in a general, unifying framework and interrelations are highlighted.With this in mind we then show how linear stochastic differential equation models can be incorporated naturally into the kernel framework. And vice versa, many kernel machines can be interpreted in terms of differential equations. We focus especially on ordinary differential equations, also known as dynamical systems, and it is shown that standard kernel inference algorithms are equivalent to Kalman filter methods based on such models.In order not to cloud qualitative insights with heavy mathematical machinery, we restrict ourselves to finite domains, implying that differential …",Florian Steinke and Bernhard Schölkopf,48,12634482239474139514,Pattern Recognition,11,3271-3286,Pergamon,"Kernels, regularization and differential equations",https://www.sciencedirect.com/science/article/pii/S0031320308002367,41,2008,/scholar?cites=12634482239474139514,DZ-fHPgAAAAJ:VN7nJs4JPk0C
14310,"Brain–computer interfaces (BCIs) can be used for communication in writing without muscular activity or for learning to control seizures by voluntary regulation of brain signals such as the electroencephalogram (EEG). Three of five patients with epilepsy were able to spell their names with electrocorticogram (ECoG) signals derived from motor-related areas within only one or two training sessions. Imagery of finger or tongue movements was classified with support-vector classification of autoregressive coefficients derived from the ECoG signals. After training of the classifier, binary classification responses were used to select letters from a computer-generated menu. Offline analysis showed increased theta activity in the unsuccessful patients, whereas the successful patients exhibited dominant sensorimotor rhythms that they could control. The high spatial resolution and increased signal-to-noise ratio in ECoG signals …",Thilo Hinterberger and Guido Widman and Thomas Navin Lal and Jeremy Hill and Michael Tangermann and Wolfgang Rosenstiel and Bernhard Scholkopf and Christian Elger and Niels Birbaumer,47,11837046297703468736,Epilepsy & Behavior,2,300-306,Academic Press,Voluntary brain regulation and communication with electrocorticogram signals,https://www.sciencedirect.com/science/article/pii/S1525505008000760,13,2008,/scholar?cites=11837046297703468736,DZ-fHPgAAAAJ:F2UWTTQJPOcC
14311,"This paper presents a Local Learning Projection (LLP) approach for linear dimensionality reduction. We first point out that the well known Principal Component Analysis (PCA) essentially seeks the projection that has the minimal global estimation error. Then we propose a dimensionality reduction algorithm that leads to the projection with the minimal local estimation error, and elucidate its advantages for classification tasks. We also indicate that LLP keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values. Experimental results are provided to validate the effectiveness of the proposed algorithm.",Mingrui Wu and Kai Yu and Shipeng Yu and Bernhard Schölkopf,47,1923949169980888024,,,1039-1046,,Local learning projections,https://dl.acm.org/doi/abs/10.1145/1273496.1273627,,2007,/scholar?cites=1923949169980888024,DZ-fHPgAAAAJ:oNZyr7d5Mn4C
14312,"We attempt to shed light on the algorithms humans use to classify images of human faces according to their gender. For this, a novel methodology combining human psychophysics and machine learning is introduced. We proceed as follows. First, we apply principal component analysis (PCA) on the pixel information of the face stimuli. We then obtain a data set composed of these PCA eigenvectors combined with the subjects' gender estimates of the corresponding stimuli. Second, we model the gender classification process on this data set using a separating hyperplane (SH) between both classes. This SH is computed using algorithms from machine learning: the support vector machine (SVM), the relevance vector machine, the prototype classifier, and the K-means classifier. The classification behavior of humans and machines is then analyzed in three steps. First, the classification errors of humans and machines …",Arnulf BA Graf and Felix A Wichmann and Heinrich H Bülthoff and Bernhard Schölkopf,47,4988433810870497525,Neural Computation,1,143-165,MIT Press,Classification of faces in man and machine,https://www.mitpressjournals.org/doi/abs/10.1162/089976606774841611,18,2006,/scholar?cites=4988433810870497525,DZ-fHPgAAAAJ:7H_MAutzIkAC
14313,"The last years have witnessed an increasing interest in Support Vector (SV) machines, which use Mercer kernels for efficiently performing computations in high-dimensional spaces. In pattern recognition, the SV algorithm constructs nonlinear decision functions by training a classifier to perform a linear separation in some high-dimensional space which is nonlinearly related to input space. Recently, we have developed a technique for Nonlinear Principal Component Analysis (Kernel PCA) based on the same types of kernels. This way, we can for instance efficiently extract polynomial features of arbitrary order by computing projections onto principal components in the space of all products of n pixels of images. We explain the idea of Mercer kernels and associated feature spaces, and describe connections to the theory of reproducing kernels and to regularization theory, followed by an overview of the above algorithms employing these kernels. 1. Introduction For the case of two-class pattern...",Bernhard Schölkopf and Alex Smola and Klaus-Robert Müller and Chris Burges and Vladimir Vapnik,47,2249917166668243596,,,,,Support vector methods in learning and feature extraction,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.1638,,1998,/scholar?cites=2249917166668243596,DZ-fHPgAAAAJ:XD-gHx7UXLsC
14314,"Many controlled systems suffer from unmodeled nonlinear effects that recur periodically over time. Model-free controllers generally cannot compensate these effects, and good physical models for such periodic dynamics are challenging to construct. We investigate nonparametric system identification for periodically recurring nonlinear effects. Within a Gaussian process (GP) regression framework, we use a locally periodic covariance function to shape the hypothesis space, which allows for a structured extrapolation that is not possible with more widely used covariance functions. We show that hyperparameter estimation can be performed online using the maximum a posteriori point estimate, which provides an accuracy comparable with sampling methods as soon as enough data to cover the periodic structure has been collected. It is also shown how the periodic structure can be exploited in the hyperparameter …",Edgar D Klenske and Melanie N Zeilinger and Bernhard Schölkopf and Philipp Hennig,46,13555022710794783091,IEEE Transactions on Control Systems Technology,1,110-121,IEEE,Gaussian process-based predictive control for periodic error correction,https://ieeexplore.ieee.org/abstract/document/7105398/,24,2015,/scholar?cites=13555022710794783091,DZ-fHPgAAAAJ:Eu5-agfRY4IC
14315,,Scholkopf Bernhard and J SMOLA Alexander,46,17659045910908121798,MIT Press. Cambridge,,24-69,,"Learning with Kernels Support Vector Machines, Regularization, Optimization and Beyond",http://scholar.google.com/scholar?cluster=17659045910908121798&hl=en&oi=scholarr,,2002,/scholar?cites=17659045910908121798,DZ-fHPgAAAAJ:axOtAORPYx0C
14316,"One of the goals of probabilistic inference is to decide whether an empirically observed distribution is compatible with a candidate Bayesian network. However, Bayesian networks with hidden variables give rise to highly non-trivial constraints on the observed distribution. Here, we propose an information-theoretic approach, based on the insight that conditions on entropies of Bayesian networks take the form of simple linear inequalities. We describe an algorithm for deriving entropic tests for latent structures. The well-known conditional independence tests appear as a special case. While the approach applies for generic Bayesian networks, we presently adopt the causal view, and show the versatility of the framework by treating several relevant problems from that domain: detecting common ancestors, quantifying the strength of causal influence, and inferring the direction of causation from two-variable marginals.",Rafael Chaves and Lukas Luft and Thiago O Maciel and David Gross and Dominik Janzing and Bernhard Schölkopf,44,9944067498786278711,arXiv preprint arXiv:1407.2256,,,,Inferring latent structures via information inequalities,https://arxiv.org/abs/1407.2256,,2014,/scholar?cites=9944067498786278711,DZ-fHPgAAAAJ:h9G0ZmjYpDoC
14317,"Astronomical images taken by ground-based telescopes suffer degradation due to atmospheric turbulence. This degradation can be tackled by costly hardware-based approaches such as adaptive optics, or by sophisticated software-based methods such as lucky imaging, speckle imaging, or multi-frame deconvolution. Software-based methods process a sequence of images to reconstruct a deblurred high-quality image. However, existing approaches are limited in one or several aspects: (i) they process all images in batch mode, which for thousands of images is prohibitive; (ii) they do not reconstruct a super-resolved image, even though an image sequence often contains enough information; (iii) they are unable to deal with saturated pixels; and (iv) they are usually non-blind, i.e., they assume the blur kernels to be known. In this paper we present a new method for multi-frame deconvolution called online blind …",Michael Hirsch and S Harmeling and S Sra and B Schölkopf,44,2258778604025131703,Astronomy & Astrophysics,,A9,EDP Sciences,Online multi-frame blind deconvolution with super-resolution and saturation correction,https://www.aanda.org/articles/aa/abs/2011/07/aa13955-09/aa13955-09.html,531,2011,/scholar?cites=2258778604025131703,DZ-fHPgAAAAJ:5ugPr518TE4C
14318,"In a pre-processing step prior to training a learning machine, pre-processing includes reducing the quantity of features to be processed using feature selection methods selected from the group consisting of recursive feature elimination (RFE), minimizing the number of non-zero parameters of the system (l 0-norm minimization), evaluation of cost function to identify a subset of features that are compatible with constraints imposed by the learning set, unbalanced correlation score and transductive feature selection. The features remaining after feature selection are then used to train a learning machine for purposes of pattern classification, regression, clustering and/or novelty detection.",,44,17765253051466401106,,,,,Methods for feature selection in a learning machine,https://patents.google.com/patent/US7624074B2/en,,2009,/scholar?cites=17765253051466401106,DZ-fHPgAAAAJ:CHSYGLWDkRkC
14319,,Bernhard Schölkopf and Alexander Smola,44,16075049356233645565,"The MIT Press, Cambridge, MA",,45,,"Learning with Kernels–Support Vector Machines, Optimization, and Beyond",http://scholar.google.com/scholar?cluster=12401000353600736667&hl=en&oi=scholarr,18,2002,/scholar?cites=16075049356233645565,DZ-fHPgAAAAJ:miWjVyTa_KoC
14320,,Klaus-Robert Müller and Alexander J Smola and Gunnar Rätsch and Bernhard Schökopf and Jens Kohlmorgen and Vladimir Vapnik,44,8751979978363900963,,,243-254,"MIT Press, Cambridge, MA","Using support vector machines for time series prediction, Advances in kernel methods: support vector learning",http://scholar.google.com/scholar?cluster=8751979978363900963&hl=en&oi=scholarr,,1999,/scholar?cites=8751979978363900963,DZ-fHPgAAAAJ:72nuA7Q-vkIC
14321,"Papers from the 2006 flagship meeting on neural computation, with contributions from physicists, neuroscientists, mathematicians, statisticians, and computer scientists. The annual Neural Information Processing Systems (NIPS) conference is the flagship meeting on neural computation and machine learning. It draws a diverse group of attendees--physicists, neuroscientists, mathematicians, statisticians, and computer scientists--interested in theoretical and applied aspects of modeling, simulating, and building neural-like or intelligent systems. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning, and applications. Only twenty-five percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains the papers presented at the December 2006 meeting, held in Vancouver.",Bernhard Schölkopf and John Platt and Thomas Hofmann,43,17350050156995625280,,,,Mit Press,Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference,http://books.google.com/books?hl=en&lr=&id=Tbn1l9P1220C&oi=fnd&pg=PR5&dq=info:QEHverHCx_AJ:scholar.google.com&ots=V5kdDkjtZZ&sig=yiG9LqVLg2_hnxxlfrNgWljdp4Y,19,2007,/scholar?cites=17350050156995625280,DZ-fHPgAAAAJ:pxwZTRgS7OwC
14322,"We propose a new inference rule for estimating causal structure that underlies the observed statistical dependencies among n random variables. Our method is based on comparing the conditional distributions of variables given their direct causes (the so-called Markov kernels"") for all hypothetical causal directions and choosing the most plausible one. We consider those Markov kernels most plausible, which maximize the (conditional) entropies constrained by their observed first moment (expectation) and second moments (variance and covariance with its direct causes) based on their given domain. In this paper, we discuss our inference rule for causal relationships between two variables in detail, apply it to a real-world temperature data set with known causality and show that our method provides a correct result for the example.",Xiaohai Sun and Dominik Janzing and Bernhard Scholkopf,43,13693808989775449315,,,,,Causal inference by choosing graphs with most plausible Markov kernels,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1791171,,2005,/scholar?cites=13693808989775449315,DZ-fHPgAAAAJ:rbm3iO8VlycC
14323,,Schölkopf Bernhard and CJC Burges and A Smola,43,14596491277654402408,"Cambridge, MA, MIT Press",,,,Advanced in kernel methods support vector learning,http://scholar.google.com/scholar?cluster=14596491277654402408&hl=en&oi=scholarr,,1998,/scholar?cites=14596491277654402408,DZ-fHPgAAAAJ:yJjnfzR0HrkC
14324,"Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.",Giambattista Parascandolo and Niki Kilbertus and Mateo Rojas-Carulla and Bernhard Schölkopf,42,470442545911167426,,,4036-4044,PMLR,Learning independent causal mechanisms,http://proceedings.mlr.press/v80/parascandolo18a.html,,2018,/scholar?cites=470442545911167426,DZ-fHPgAAAAJ:n0yAh_twS0EC
14325,"It is commonplace to encounter nonstationary or heterogeneous data, of which the underlying generating process changes over time or across data sets (the data sets may have different experimental conditions or data collection conditions). Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper we develop a principled framework for causal discovery from such data, called Constraint-based causal Discovery from Nonstationary/heterogeneous Data (CD-NOD), which addresses two important questions. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine causal orientations by making use of independence changes in the data distribution implied by the underlying causal model, benefiting …",Kun Zhang and Biwei Huang and Jiji Zhang and Clark Glymour and Bernhard Schölkopf,42,17305702719663191712,IJCAI: Proceedings of the Conference,,1347,NIH Public Access,Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5617646/,2017,2017,/scholar?cites=17305702719663191712,DZ-fHPgAAAAJ:l1aDfRkTpg0C
14326,"Similarity is used as an explanatory construct throughout psychology and multidimensional scaling (MDS) is the most popular way to assess similarity. In MDS, similarity is intimately connected to the idea of a geometric representation of stimuli in a perceptual space. Whilst connecting similarity and closeness of stimuli in a geometric representation may be intuitively plausible, Tversky and Gati [Tversky, A., & Gati, I. (1982). Similarity, separability, and the triangle inequality. Psychological Review, 89(2), 123–154] have reported data which are inconsistent with the usual geometric representations that are based on segmental additivity. We show that similarity measures based on Shepard’s universal law of generalization [Shepard, R. N. (1987). Toward a universal law of generalization for psychologica science. Science, 237(4820), 1317–1323] lead to an inner product representation in a reproducing kernel Hilbert …",Frank Jakel and Bernhard Scholkopf and Felix A Wichmann,42,12089905424968865929,Journal of Mathematical Psychology,5,297-303,Academic Press,"Similarity, kernels, and the triangle inequality",https://www.sciencedirect.com/science/article/pii/S0022249608000278,52,2008,/scholar?cites=12089905424968865929,DZ-fHPgAAAAJ:ziOE8S1-AIUC
14327,"Many real-world machine learning problems are situated on finite discrete sets, including dimensionality reduction, clustering, and transductive inference. A variety of approaches for learning from finite sets has been proposed from different motivations and for different problems. In most of those approaches, a finite set is modeled as a graph, in which the edges encode pairwise relationships among the objects in the set. Consequently many concepts and methods from graph theory are adopted. In particular, the graph Laplacian is widely used. In this chapter we present a systemic framework for learning from a finite set represented as a graph. We develop discrete analogues of a number of differential operators, and then construct a discrete analogue of classical regularization theory based on those discrete differential operators. The graph Laplacian based approaches are special cases of this general discrete regularization framework. An important thing implied in this framework is that we have a wide choices of regularization on graph in addition to the widely-used graph Laplacian based one.",Dengyong Zhou and Bernhard Schölkopf,42,13718315029115091007,,,221-232,"MIT Press, Cambridge, MA",Discrete regularization,https://www.researchgate.net/profile/Dengyong_Zhou/publication/237814563_1_Discrete_Regularization/links/0c96052b4d366d5048000000/1-Discrete-Regularization.pdf,,2006,/scholar?cites=13718315029115091007,DZ-fHPgAAAAJ:VBqjOGLmcYMC
14328,"© 2015 Macmillan Publishers Limited. All rights reserved non linear mapping between inputs and the value of possible actions—for instance, the value of a move in each possible direction when playing Space Invaders (Fig. 1). The system picks output actions on the basis of its current estimate of Q*, thereby exploiting its knowledge of a game’s reward structure, and intersperses the predicted best action with random actions to explore uncharted territory. The game then responds with the next game screen and a reward signal equal to the change in the game score. Periodically, the network uses inputs and rewards to update the DQN parameters, attempting to move closer to Q*. Much thought went into how exactly to do this, given that the agent collects its own training data over time. As such, the data are not independent from a statistical point of view, implying that most of statistical theory does not apply. The …",Bernhard Schölkopf,41,3428743446695574993,Nature,7540,486-487,Nature Publishing Group,Artificial intelligence: Learning to see and act,https://www.nature.com/articles/518486a,518,2015,/scholar?cites=3428743446695574993,DZ-fHPgAAAAJ:tQ8TVJWhmt0C
14329,"In bioinformatics, there exist multiple descriptions of graphs for the same set of genes or proteins. For instance, in yeast systems, graph edges can represent different relationships such as protein–protein interactions, genetic interactions, or co-participation in a protein complex, etc. Relying on similarities between nodes, each graph can be used independently for prediction of protein function. However, since different graphs contain partly independent and partly complementary information about the problem at hand, one can enhance the total information extracted by combining all graphs. In this paper, we propose a method for integrating multiple graphs within a framework of semi-supervised learning. The method alternates between minimizing the objective function with respect to network output and with respect to combining weights. We apply the method to the task of protein functional class prediction in yeast …",Hyunjung Shin and Koji Tsuda and Bernhard Schölkopf,41,11134127175250161858,Expert Systems with Applications,2,3284-3292,Pergamon,Protein functional class prediction with a combined graph,https://www.sciencedirect.com/science/article/pii/S0957417408000870,36,2009,/scholar?cites=11134127175250161858,DZ-fHPgAAAAJ:K3LRdlH-MEoC
14330,"We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a Reproducing Kernel Hilbert Space. We apply this technique to construct a two-sample test, which is used for determining whether two sets of observations arise from the same distribution. We use this test in attribute matching for databases using the Hungarian marriage method, where it performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist.",Arthur Gretton and Karsten M Borgwardt and Malte Rasch and B Scholkopf and Alexander J Smola,41,4408504611969921029,PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE,2,1637,"Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999",A kernel approach to comparing distributions,https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf,22,2007,/scholar?cites=4408504611969921029,DZ-fHPgAAAAJ:mUJArPsKIAAC
14331,"This chapter contains sections titled: Introduction, Incorporating Geometry in Regularization, Algorithms, Data-Dependent Kernels for Semi-Supervised Learning, Linear Methods for Large-Scale Semi-Supervised Learning, Connections to Other Algorithms and Related Work, Future Directions",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,41,9775561257469928050,,,217-235,MIT press,The geometric basis of semi-supervised learning,https://ieeexplore.ieee.org/abstract/document/6280897/,,2006,/scholar?cites=9775561257469928050,DZ-fHPgAAAAJ:02rtHm8L3-kC
14332,"We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (ie, the spike data) as well as of the output (ie, the orientation), and report the results obtained using different kernel algorithms.",Jan Eichhorn and Andreas Tolias and Alexander Zien and Malte Kuss and Jason Weston and Nikos Logothetis and Bernhard Schölkopf and Carl Rasmussen,41,9285047554845866887,Advances in neural information processing systems,,1367-1374,,Prediction on spike data using kernel algorithms,https://proceedings.neurips.cc/paper/2003/file/d095a94d20dcaf7aa07301948549bede-Paper.pdf,16,2003,/scholar?cites=9285047554845866887,DZ-fHPgAAAAJ:zLWjf1WUPmwC
14333,"The presence of noise in the data introduces a trade-o in every learning problem: complex hypotheses can be very accurate on the training set, but have worse predictive power than simpler and slightly inaccurate hypotheses. Hence the right balance between accuracy and simplicity of a hypothesis needs to be sought and this is usually attained by minimizing a cost function formed of two parts, one describing the complexity of the hypothesis, the other measuring its training error. In the case of linear functions this leads to an additional di culty as the problem of minimising the number of training errors is computationally infeasible if we parametrize the problem in terms of the dimension of the inputs (Arora et al., 1997). We avoid this apparent impasse by bounding the generalization in terms of a di erent function of the training set performance, namely one based on the distribution of margin values, but not directly involving training error. We will show in this paper that minimising this new criterion can be performed e ciently. When considering large margin classi ers, where the complexity of a hypothesis is measured by its margin with respect to the data, the presence of noise can lead to further problems, for example datasets may be non-separable, and hence their non-separable data margin would be negative, making application of the non-agnostic result impossible. Moreover solutions found by maximizing the margin are not stable with respect to the training points {slight modi cations in the training set can signi cantly change the hypothesis {a brittleness which makes the maximal margin solution somehow undesirable. These problems have led …",John Shawe-Taylor and Nello Cristianini,41,14377536449472757799,Advances in Large Margin Classifiers,,349-358,MIT Press,Margin distribution and soft margin,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.4783&rep=rep1&type=pdf,,2000,/scholar?cites=14377536449472757799,DZ-fHPgAAAAJ:lhHUEWzJgRwC
14334,"Active learning typically aims at minimizing the number of labeled samples to be included in the training set to reach a certain level of classification accuracy. Standard methods do not usually take into account the real annotation procedures and implicitly assume that all samples require the same effort to be labeled. Here, we consider the case where the cost associated with the annotation of a given sample depends on the previously labeled samples. In general, this is the case when annotating a queried sample is an action that changes the state of a dynamic system, and the cost is a function of the state of the system. In order to minimize the total annotation cost, the active sample selection problem is addressed in the framework of a Markov decision process, which allows one to plan the next labeling action on the basis of an expected long-term cumulative reward. This framework allows us to address the problem …",Claudio Persello and Abdeslam Boularias and Michele Dalponte and Terje Gobakken and Erik Naesset and Bernhard Schoelkopf,40,15620007734766530342,IEEE Transactions on Geoscience and Remote Sensing,10,6652-6664,IEEE,Cost-sensitive active learning with lookahead: Optimizing field surveys for remote sensing data classification,https://ieeexplore.ieee.org/abstract/document/6729084/,52,2014,/scholar?cites=15620007734766530342,DZ-fHPgAAAAJ:eIKNFFVQvJAC
14335,"Learning inverse kinematics of robots with redundant degrees of freedom (DoF) is a difficult problem in robot learning. The difficulty lies in the non-uniqueness of the inverse kinematics function. Existing methods tackle non-uniqueness by segmenting the configuration space and building a global solution from local experts. The usage of local experts implies the definition of an oracle, which governs the global consistency of the local models; the definition of this oracle is difficult. We propose an algorithm suitable to learn the inverse kinematics function in a single global model despite its multivalued nature. Inverse kinematics is approximated from examples using structured output learning methods. Unlike most of the existing methods, which estimate inverse kinematics on velocity level, we address the learning of the direct function on position level. This problem is a significantly harder. To support the proposed …",Botond Bócsi and Duy Nguyen-Tuong and Lehel Csató and Bernhard Schoelkopf and Jan Peters,40,13240005596924004595,,,698-703,IEEE,Learning inverse kinematics with structured prediction,https://ieeexplore.ieee.org/abstract/document/6094666/,,2011,/scholar?cites=13240005596924004595,DZ-fHPgAAAAJ:silx2ntsSuwC
14336,"We review recent methods for learning with positive definite kernels. All these methods formulate learning and estimation problems as linear tasks in a reproducing kernel Hilbert space (RKHS) associated with a kernel. We cover a wide range of methods, ranging from simple classifiers to sophisticated methods for estimation with structured data.",Thomas Hofmann and Bernhard Schölkopf and Alexander J Smola,40,16669167802809212613,"TR, MPI for Biological Cybernetics",,,,A review of kernel methods in machine learning,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.9409&rep=rep1&type=pdf,156,2006,/scholar?cites=16669167802809212613,DZ-fHPgAAAAJ:l7t_Zn2s7bgC
14337,"We present an approach for designing interest operators that are based on human eye movement statistics. In contrast to existing methods which use hand-crafted saliency measures, we use machine learning methods to infer an interest operator directly from eye movement data. That way, the operator provides a measure of biologically plausible interestingness. We describe the data collection, training, and evaluation process, and show that our learned saliency measure significantly accounts for human eye movements. Furthermore, we illustrate connections to existing interest operators, and present a multi-scale interest point detector based on the learned function.",Wolf Kienzle and Felix A Wichmann and Bernhard Scholkopf and Matthias O Franz,40,5847230318212256107,,,24-24,IEEE,Learning an interest operator from human eye movements,https://ieeexplore.ieee.org/abstract/document/1640463/,,2006,/scholar?cites=5847230318212256107,DZ-fHPgAAAAJ:uVUOdF_882EC
14338,"We develop a methodology for solving high dimensional dependency estimation problems between pairs of data types, which is viable in the case where the output of interest has very high dimension, e.g., thousands of dimensions. This is achieved by mapping the objects into continuous or discrete spaces, using joint kernels. Known correlations between input and output can be defined by such kernels, some of which can maintain linearity in the outputs to provide simple (closed form) pre-images. We provide examples of such kernels and empirical results.",Jason Weston and Bernhard Schölkopf and Olivier Bousquet,39,17300134536421055729,,,176-191,"Springer, Berlin, Heidelberg",Joint kernel maps,https://link.springer.com/chapter/10.1007/11494669_23,,2005,/scholar?cites=17300134536421055729,DZ-fHPgAAAAJ:Z5m8FVwuT1cC
14339,"Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network’s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.",Carl-Johann Simon-Gabriel and Yann Ollivier and Léon Bottou and Bernhard Schölkopf and David Lopez-Paz,38,5780158561780944700,,,,,Adversarial vulnerability of neural networks increases with input dimension,https://openreview.net/forum?id=H1MzKs05F7,,2018,/scholar?cites=5780158561780944700,DZ-fHPgAAAAJ:5qzlj8FBphgC
14340,"Learning a complex task such as table tennis is a challenging problem for both robots and humans. Even after acquiring the necessary motor skills, a strategy is needed to choose where and how to return the ball to the opponent’s court in order to win the game. The data-driven identification of basic strategies in interactive tasks, such as table tennis, is a largely unexplored problem. In this paper, we suggest a computational model for representing and inferring strategies, based on a Markov decision problem, where the reward function models the goal of the task as well as the strategic information. We show how this reward function can be discovered from demonstrations of table tennis matches using model-free inverse reinforcement learning. The resulting framework allows to identify basic elements on which the selection of striking movements is based. We tested our approach on data collected from …",Katharina Muelling and Abdeslam Boularias and Betty Mohler and Bernhard Schölkopf and Jan Peters,38,8897076145404329224,Biological cybernetics,5,603-619,Springer Berlin Heidelberg,Learning strategies in table tennis using inverse reinforcement learning,https://link.springer.com/article/10.1007/s00422-014-0599-1,108,2014,/scholar?cites=8897076145404329224,DZ-fHPgAAAAJ:IExZWSxeYXUC
14341,"In a pre-processing step prior to training a learning machine, pre-processing includes reducing the quantity of features to be processed using feature selection methods selected from the group consisting of recursive feature elimination (RFE), minimizing the number of non-zero parameters of the system (l 0-norm minimization), evaluation of cost function to identify a subset of features that are compatible with constraints imposed by the learning set, unbalanced correlation score, transductive feature selection and single feature using margin-based ranking. The features remaining after feature selection are then used to train a learning machine for purposes of pattern classification, regression, clustering and/or novelty detection.",,38,10655485586229960553,,,,,Method for feature selection in a support vector machine using feature ranking,https://patents.google.com/patent/US7805388B2/en,,2010,/scholar?cites=10655485586229960553,DZ-fHPgAAAAJ:9vf0nzSNQJEC
14342,"In kernel methods, all the information about the training data is contained in the Gram matrix. If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well: We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine, which is a common kernel approach for pattern recognition.",Jason Weston and Bernhard Schölkopf and Eleazar Eskin and Christina Leslie and William Stafford Noble,38,7794181664583425299,Annals of the Institute of Statistical Mathematics,2,391-408,Kluwer Academic Publishers,Dealing with large diagonals in kernel matrices,https://link.springer.com/content/pdf/10.1007/BF02530507.pdf,55,2003,/scholar?cites=7794181664583425299,DZ-fHPgAAAAJ:AvfA0Oy_GE0C
14343,"Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine two such techniques on a pattern recognition problem. The method for improving generalization per-formance (the “virtual support vector” method) does so by incor-porating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the “reduced set” method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1% error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine.",Bernhard Schölkopf and Patrice Simard and Vladimir Vapnik and AJ Smola,38,16051352144982663635,Advances in neural information processing systems,,375-381,,Improving the accuracy and speed of support vector machines,http://books.google.com/books?hl=en&lr=&id=QpD7n95ozWUC&oi=fnd&pg=PA375&dq=info:02Flbrvbwd4J:scholar.google.com&ots=iEjvmGSZdx&sig=J4drnJ9GW3JH7TLSwkc0-43z09o,9,1997,/scholar?cites=16051352144982663635,DZ-fHPgAAAAJ:fewDlv083xwC
14344,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",Francesco Locatello and Gabriele Abbati and Thomas Rainforth and Stefan Bauer and Bernhard Schölkopf and Olivier Bachem,37,8529143244518609453,,,14611-14624,,On the fairness of disentangled representations,http://papers.nips.cc/paper/9603-on-the-fairness-of-disentangled-representations,,2019,/scholar?cites=8529143244518609453,DZ-fHPgAAAAJ:oqUQjtHG81IC
14345,"Clinical PET/MR requires the use of patient positioning aids to immobilize and support patients for the duration of the combined examination. Ancillary immobilization devices contribute to overall attenuation of the PET signal, but are not detected with conventional MR sequences and, hence, are ignored in standard MR-based attenuation correction (MR-AC). We report on the quantitative effect of not accounting for the attenuation of patient positioning aids in combined PET/MR imaging.We used phantom and patient data acquired with positioning aids on a PET/CT scanner (Biograph 16, HI-REZ) to mimic PET/MR imaging conditions. Reference CT-based attenuation maps were generated from measured (original) CT transmission images (origCT-AC). We also created MR-like attenuation maps by following the same conversion …",Frederic Mantlik and Matthias Hofmann and Matthias K Werner and Alexander Sauter and Jürgen Kupferschläger and Bernhard Schölkopf and Bernd J Pichler and Thomas Beyer,37,6422252709105410626,European journal of nuclear medicine and molecular imaging,5,920-929,Springer-Verlag,The effect of patient positioning aids on PET quantification in PET/MR imaging,https://link.springer.com/article/10.1007/s00259-010-1721-9,38,2011,/scholar?cites=6422252709105410626,DZ-fHPgAAAAJ:NhqRSupF_l8C
14346,"We propose machine learning methods for the estimation of deformation fields that transform two given objects into each other, thereby establishing a dense point to point correspondence. The fields are computed using a modified support vector machine containing a penalty enforcing that points of one object will be mapped to"" similar"" points on the other one. Our system, which contains little engineering or domain knowledge, delivers state of the art performance. We present application results including close to photorealistic morphs of 3D head models.",Bernhard Schölkopf and Florian Steinke and Volker Blanz,37,14610787856291624132,,,776-783,,Object correspondence as a machine learning problem,https://dl.acm.org/doi/abs/10.1145/1102351.1102449,,2005,/scholar?cites=14610787856291624132,DZ-fHPgAAAAJ:vbGhcppDl1QC
14347,"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. To this end, we propose"" fidelity-weighted learning""(FWL), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels). Both student and teacher are learned from the data. We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.",Mostafa Dehghani and Arash Mehrjou and Stephan Gouws and Jaap Kamps and Bernhard Schölkopf,36,6679593996852506297,arXiv preprint arXiv:1711.02799,,,,Fidelity-weighted learning,https://arxiv.org/abs/1711.02799,,2017,/scholar?cites=6679593996852506297,DZ-fHPgAAAAJ:Id6JH_Uf3zcC
14348,"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.",Matthias Bauer and Mateo Rojas-Carulla and Jakub Bartłomiej Świątkowski and Bernhard Schölkopf and Richard E Turner,36,12518975774456633060,arXiv preprint arXiv:1706.00326,,,,Discriminative k-shot learning using probabilistic models,https://arxiv.org/abs/1706.00326,,2017,/scholar?cites=12518975774456633060,DZ-fHPgAAAAJ:99Pv1cn5vGAC
14349,"We consider the task of inferring causal relations in brain imaging data with latent confounders. Using a priori knowledge that randomized experimental conditions cannot be effects of brain activity, we derive statistical conditions that are sufficient for establishing a causal relation between two neural processes, even in the presence of latent confounders. We provide an algorithm to test these conditions on empirical data, and illustrate its performance on simulated as well as on experimentally recorded EEG data.",Moritz Grosse-Wentrup and Dominik Janzing and Markus Siegel and Bernhard Schölkopf,36,11981377593090342939,NeuroImage,,825-833,Academic Press,Identification of causal relations in neuroimaging data with latent confounders: An instrumental variable approach,https://www.sciencedirect.com/science/article/pii/S1053811915009751,125,2016,/scholar?cites=11981377593090342939,DZ-fHPgAAAAJ:KDIJNi6oDIIC
14350,"Camera lenses are a critical component of optical imaging systems, and lens imperfections compromise image quality. While traditionally, sophisticated lens design and quality control aim at limiting optical aberrations, recent works [1,2,3] promote the correction of optical flaws by computational means. These approaches rely on elaborate measurement procedures to characterize an optical system, and perform image correction by non-blind deconvolution.In this paper, we present a method that utilizes physically plausible assumptions to estimate non-stationary lens aberrations blindly, and thus can correct images without knowledge of specifics of camera and lens. The blur estimation features a novel preconditioning step that enables fast deconvolution. We obtain results that are competitive with state-of-the-art non-blind approaches.",Christian J Schuler and Michael Hirsch and Stefan Harmeling and Bernhard Schölkopf,36,1139307307350284389,,,187-200,"Springer, Berlin, Heidelberg",Blind correction of optical aberrations,https://link.springer.com/chapter/10.1007/978-3-642-33712-3_14,,2012,/scholar?cites=1139307307350284389,DZ-fHPgAAAAJ:wUn16MOA3RoC
14351,"Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.",Mauricio Alvarez and Jan Peters and Neil Lawrence and Bernhard Schölkopf,36,5808626008200367507,Advances in neural information processing systems,,55-63,,Switched latent force models for movement segmentation,https://papers.nips.cc/paper/2010/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html,23,2010,/scholar?cites=5808626008200367507,DZ-fHPgAAAAJ:QYdC8u9Cj1oC
14352,"Learning machines, such as support vector machines, are used to analyze datasets to recognize patterns within the dataset using kernels that are selected according to the nature of the data to be analyzed. Where the datasets possesses structural characteristics, locational kernels can be utilized to provide measures of similarity among data points within the dataset. The locational kernels are then combined to generate a decision function, or kernel, that can be used to analyze the dataset. Where invariance transformations or noise is present, tangent vectors are defined to identify relationships between the invariance or noise and the data points. A covariance matrix is formed using the tangent vectors, then used in generation of the kernel for recognizing patterns in the dataset.",,36,3355899643005322425,,,,,Kernels and methods for selecting kernels for use in learning machines,https://patents.google.com/patent/US7353215B2/en,,2008,/scholar?cites=3355899643005322425,DZ-fHPgAAAAJ:NXb4pA-qfm4C
14353,"We derive new bounds for the generalization error of feature space machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs are based on a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite dimensional unit ball in feature space into a finite dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator, can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence we are able to theoretically explain the effect of the choice of kernel functions on the generalization performance of support vector …",Robert C Williamson and Alex J Smola and Bernhard Schölkopf,36,15130074667307841832,,,285-299,"Springer, Berlin, Heidelberg","Entropy numbers, operators and support vector kernels",https://link.springer.com/chapter/10.1007/3-540-49097-3_23,,1999,/scholar?cites=15130074667307841832,DZ-fHPgAAAAJ:ipzZ9siozwsC
14354,Model selection in support vector machines is usually carried out by minimizing the quotient of the radius of the smallest enclosing sphere of the data and the observed margin on the training set. We provide a new criterion taking the distribution within that sphere into account by considering the eigenvalue distribution of the Gram matrix of the data. Experimental results on real world data show that this new criterion provides a good prediction of the shape of the curve relating generalization error to kernel width.,Bernhard Scholkopf and John Shawe-Taylor and Alex J Smola and Robert C Williamson,36,176209563648506642,,,103-108,IET Digital Library,Kernel-dependent support vector error bounds,https://digital-library.theiet.org/content/conferences/10.1049/cp_19991092,,1999,/scholar?cites=176209563648506642,DZ-fHPgAAAAJ:1DsIQWDZLl8C
14355,"In the last few years, machine learning techniques, in particular convolutional neural networks, have been investigated as a method to replace or complement traditional matched filtering techniques that are used to detect the gravitational-wave signature of merging black holes. However, to date, these methods have not yet been successfully applied to the analysis of long stretches of data recorded by the Advanced LIGO and Virgo gravitational-wave observatories. In this work, we critically examine the use of convolutional neural networks as a tool to search for merging black holes. We identify the strengths and limitations of this approach, highlight some common pitfalls in translating between machine learning and gravitational-wave astronomy, and discuss the interdisciplinary challenges. In particular, we explain in detail why convolutional neural networks alone cannot be used to claim a statistically significant …",Timothy D Gebhard and Niki Kilbertus and Ian Harry and Bernhard Schölkopf,35,7619404683272374568,Physical Review D,6,063015,American Physical Society,Convolutional neural networks: A magic bullet for gravitational-wave detection?,https://journals.aps.org/prd/abstract/10.1103/PhysRevD.100.063015,100,2019,/scholar?cites=7619404683272374568,DZ-fHPgAAAAJ:XfHLGV2CmsEC
14356,"A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a ""large d, small n"" paradigm.",Krikamol Muandet and Bharath Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf,35,12473900160038848555,The Journal of Machine Learning Research,1,1656-1696,JMLR. org,Kernel mean shrinkage estimators,https://dl.acm.org/doi/abs/10.5555/2946645.2946693,17,2016,/scholar?cites=12473900160038848555,DZ-fHPgAAAAJ:dMpQl7XwOw4C
14357,"We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.",Samori Kpotufe and Eleni Sgouritsa and Dominik Janzing and Bernhard Schölkopf,35,18269698956495115540,,,,,Consistency of causal inference under the additive noise model,http://www.jmlr.org/proceedings/papers/v32/kpotufe14.pdf,,2014,/scholar?cites=18269698956495115540,DZ-fHPgAAAAJ:uoRD4RTSUPoC
14358,"We study nonparametric regression between Riemannian manifolds based on regularized empirical risk minimization. Regularization functionals for mappings between manifolds should respect the geometry of input and output manifold and be independent of the chosen parametrization of the manifolds. We define and analyze the three most simple regularization functionals with these properties and present a rather general scheme for solving the resulting optimization problem. As application examples we discuss interpolation on the sphere, fingerprint processing, and correspondence computations between three-dimensional surfaces. We conclude with characterizing interesting and sometimes counterintuitive implications and new open problems that are specific to learning between Riemannian manifolds and are not encountered in multivariate regression in Euclidean space.",Florian Steinke and Matthias Hein and Bernhard Schölkopf,35,10379020281271897609,SIAM Journal on Imaging Sciences,3,527-563,Society for Industrial and Applied Mathematics,Nonparametric regression between general Riemannian manifolds,https://epubs.siam.org/doi/abs/10.1137/080744189,3,2010,/scholar?cites=10379020281271897609,DZ-fHPgAAAAJ:JoZmwDi-zQgC
14359,"Motivated by the particular problems involved in communicating with ""locked-in"" paralysed patients, we aim to develop a brain-computer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using support vector machine classification and recursive channel elimination on the independent components of averaged event-related potentials, we show that an untrained user's EEG data can be classified with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI.",N Jeremy Hill and Thomas Navin Lal and Karin Bierig and Niels Birbaumer and B Scholkopf,35,6192050924672347266,,,S3/5/INV-S3/17,IEEE,Attention modulation of auditory event-related potentials in a brain-computer interface,https://ieeexplore.ieee.org/abstract/document/1454156/,,2004,/scholar?cites=6192050924672347266,DZ-fHPgAAAAJ:w0F2JDEymm0C
14360,"Compared to constraint-based causal discovery, causal discovery based on functional causal models is able to identify the whole causal model under appropriate assumptions [Shimizu et al. 2006; Hoyer et al. 2009; Zhang and Hyvärinen 2009b]. Functional causal models represent the effect as a function of the direct causes together with an independent noise term. Examples include the linear non-Gaussian acyclic model (LiNGAM), nonlinear additive noise model, and post-nonlinear (PNL) model. Currently, there are two ways to estimate the parameters in the models: dependence minimization and maximum likelihood. In this article, we show that for any acyclic functional causal model, minimizing the mutual information between the hypothetical cause and the noise term is equivalent to maximizing the data likelihood with a flexible model for the distribution of the noise term. We then focus on estimation of the PNL …",Kun Zhang and Zhikun Wang and Jiji Zhang and Bernhard Schölkopf,34,6240939294900408101,ACM Transactions on Intelligent Systems and Technology (TIST),2,1-22,ACM,On estimation of functional causal models: general results and application to the post-nonlinear causal model,https://dl.acm.org/doi/abs/10.1145/2700476,7,2015,/scholar?cites=6240939294900408101,DZ-fHPgAAAAJ:JCGcFKGLtV4C
14361,"We propose a method that detects the true direction of time series, by fitting an autoregressive moving average model to the data. Whenever the noise is independent of the previous samples for one ordering of the observations, but dependent for the opposite ordering, we infer the former direction to be the true one. We prove that our method works in the population case as long as the noise of the process is not normally distributed (for the latter case, the direction is not identifiable). A new and important implication of our result is that it confirms a fundamental conjecture in causal reasoning---if after regression the noise is independent of signal for one direction and dependent for the other, then the former represents the true causal direction---in the case of time series. We test our approach on two types of data: simulated data sets conforming to our modeling assumptions, and real world EEG time series. Our method …",Jonas Peters and Dominik Janzing and Arthur Gretton and Bernhard Schölkopf,34,14683239690996943497,,,801-808,,Detecting the direction of causal time series,https://dl.acm.org/doi/abs/10.1145/1553374.1553477,,2009,/scholar?cites=14683239690996943497,DZ-fHPgAAAAJ:2VqYfGB8ITEC
14362,"We describe a method to perform functional operations on probability distributions of random variables. The method uses reproducing kernel Hilbert space representations of probability distributions, and it is applicable to all operations which can be applied to points drawn from the respective distributions. We refer to our approach as kernel probabilistic programming. We illustrate it on synthetic data and show how it can be used for nonparametric structural equation models, with an application to causal inference.",Bernhard Schölkopf and Krikamol Muandet and Kenji Fukumizu and Stefan Harmeling and Jonas Peters,33,17877857802404779142,Statistics and Computing,4,755-766,Springer US,Computing functions of random variables via reproducing kernel Hilbert space representations,https://link.springer.com/article/10.1007/s11222-015-9558-5,25,2015,/scholar?cites=17877857802404779142,DZ-fHPgAAAAJ:LOUAI8Maf04C
14363,"Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the “cause” and the “mechanism generating the effect from the cause” independently of each other. Specifically we postulate that the power spectrum of the “cause” time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series.",Naji Shajarisales and Dominik Janzing and Bernhard Schölkopf and Michel Besserve,33,3691344447344898558,,,285-294,,Telling cause from effect in deterministic linear dynamical systems,http://www.jmlr.org/proceedings/papers/v37/shajarisales15.pdf,,2015,/scholar?cites=3691344447344898558,DZ-fHPgAAAAJ:OQ9zOo_0HogC
14364,"This chapter contains sections titled: Introduction, Transductive Support Vector Machines, Why Use Margin on the Test Set?, Experiments and Applications of TSVMs, Solving the TSVM Optimization Problem, Connection to Related Approaches, Summary and Conclusions",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,33,3138097877145174002,,,105-117,MIT Press,Transductive support vector machines,https://ieeexplore.ieee.org/abstract/document/6280903/,,2006,/scholar?cites=3138097877145174002,DZ-fHPgAAAAJ:Zqvzy0Y0RycC
14365,"This paper presents a view-based approach to map learning and navigation in mazes. By means of graph theory we have shown that the view-graph is a sufficient representation for map behaviour such as path planning. A neural network for unsupervised learning of the view-graph from sequences of views is constructed. We use a modified Kohonen (1988) learning rule that transforms temporal sequence (rather than featural similarity) into connectedness. In the main part of the paper, we present a robot implementation of the scheme. The results show that the proposed network is able to support map behaviour in simple environments.",Hanspeter A Mallot and Heinrich H Bülthoff and Philipp Georg and Bernhard Schölkopf and Ken Yasuhara,33,13676252985110369212,,,381-386,EC2,View-based cognitive map learning by an autonomous robot,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1794450,,1995,/scholar?cites=13676252985110369212,DZ-fHPgAAAAJ:nrtMV_XWKgEC
14366,"The goal in extreme multi-label classification (XMC) is to learn a classifier which can assign a small subset of relevant labels to an instance from an extremely large set of target labels. The distribution of training instances among labels in XMC exhibits a long tail, implying that a large fraction of labels have a very small number of positive training instances. Detecting tail-labels, which represent diversity of the label space and account for a large fraction (upto 80%) of all the labels, has been a significant research challenge in XMC. In this work, we pose the tail-label detection task in XMC as robust learning in the presence of worst-case perturbations. This viewpoint is motivated by a key observation that there is a significant change in the distribution of the feature composition of instances of these labels from the training set to test set. For shallow classifiers, our robustness perspective to XMC naturally motivates the …",Rohit Babbar and Bernhard Schölkopf,32,7920109726905287406,Machine Learning,8-9,1329-1351,Springer US,"Data scarcity, robustness and extreme multi-label classification",https://link.springer.com/article/10.1007/s10994-019-05791-5,108,2019,/scholar?cites=7920109726905287406,DZ-fHPgAAAAJ:ESN8wY4Sm6AC
14367,"The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.",Raphael Suter and Djordje Miladinovic and Bernhard Schölkopf and Stefan Bauer,32,16555829466650284291,,,6056-6065,PMLR,Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness,http://proceedings.mlr.press/v97/suter19a.html,,2019,/scholar?cites=16555829466650284291,DZ-fHPgAAAAJ:2uihPNn1UBIC
14368,"Objective. Brain–computer interface (BCI) systems are often based on motor-and/or sensory processes that are known to be impaired in late stages of amyotrophic lateral sclerosis (ALS). We propose a novel BCI designed for patients in late stages of ALS that only requires high-level cognitive processes to transmit information from the user to the BCI. Approach. We trained subjects via EEG-based neurofeedback to self-regulate the amplitude of gamma-oscillations in the superior parietal cortex (SPC). We argue that parietal gamma-oscillations are likely to be associated with high-level attentional processes, thereby providing a communication channel that does not rely on the integrity of sensory-and/or motor-pathways impaired in late stages of ALS. Main results. Healthy subjects quickly learned to self-regulate gamma-power in the SPC by alternating between states of focused attention and relaxed wakefulness …",Moritz Grosse-Wentrup and Bernhard Schölkopf,32,8323873358304095066,Journal of neural engineering,5,056015,IOP Publishing,A brain–computer interface based on self-regulation of gamma-oscillations in the superior parietal cortex,https://iopscience.iop.org/article/10.1088/1741-2560/11/5/056015/meta,11,2014,/scholar?cites=8323873358304095066,DZ-fHPgAAAAJ:J6OZcwVsj5AC
14369,"A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is an important part of many algorithms ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given a finite sample, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved due to a well-known phenomenon in statistics called Stein’s phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard one. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator.",Krikamol Muandet and Kenji Fukumizu and Bharath Sriperumbudur and Arthur Gretton and Bernhard Schölkopf,32,6292961417655864146,,,10-18,,Kernel mean estimation and stein effect,http://www.jmlr.org/proceedings/papers/v32/muandet14.pdf,,2014,/scholar?cites=6292961417655864146,DZ-fHPgAAAAJ:oursBaop5wYC
14370,"A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques.",David Lopez-Paz and Jose Hernández-lobato and Bernhard Schölkopf,32,4753151448784736814,Advances in neural information processing systems,,665-673,,Semi-supervised domain adaptation with non-parametric copulas,https://papers.nips.cc/paper/4802-semi-supervised-domain-adaptation-with-non-parametric-copulas,25,2012,/scholar?cites=4753151448784736814,DZ-fHPgAAAAJ:R-LXmdHK_14C
14371,"The NIPS 2008 workshop on causality provided a forum for researchers from different horizons to share their view on causal modeling and address the difficult question of assessing causal models. There has been a vivid debate on properly separating the notion of causality from particular models such as graphical models, which have been dominating the field in the past few years. Part of the workshop was dedicated to discussing the results of a challenge, which offered a wide variety of applications of causal modeling. We have regrouped in these proceedings the best papers presented. Most lectures were videotaped or recorded. All information regarding the challenge and the lectures are found at http://www. clopinet. com/isabelle/Projects/NIPS2008/. This introduction provides a synthesis of the findings and a gentle introduction to causality topics, which are the object of active research.",Isabelle Guyon and Dominik Janzing and Bernhard Schölkopf,32,13708662158550783256,,,1-42,,Causality: Objectives and assessment,http://www.jmlr.org/proceedings/papers/v6/guyon10a/guyon10a.pdf,,2010,/scholar?cites=13708662158550783256,DZ-fHPgAAAAJ:rmuvC79q63oC
14372,,Matthias W Seeger and Hannes Nickisch and Rolf Pohmann and Bernhard Schölkopf,32,209886169976489126,,,,,Bayesian experimental design of magnetic resonance imaging sequences,,,2009,/scholar?cites=209886169976489126,DZ-fHPgAAAAJ:MLfJN-KU85MC
14373,"This chapter presents a fictitious discussion inspired by real discussions between the editors of this book and a number of people, including Vladimir Vapnik. It involves three researchers; for simplicity, called here A, B, and C, without implying any one-to-one mapping to real persons. The topic of the discussion is: What is the Difference between Semi-Supervised Learning and Transductive Learning?",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,32,8518060179569459666,,,473-478,MIT Press,A discussion of semi-supervised learning and transduction,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3158958,,2006,/scholar?cites=8518060179569459666,DZ-fHPgAAAAJ:nNdRi0dBS0cC
14374,"Beim Menschen ist der Sehsinn der differenzierteste aller Sinne. Über zwei Drittel des Cortex dienen der Verarbeitung visueller Information. Die schnelle und zuverlässige Erkennung von Objekten und Gesichtern spielt dabei eine zentrale Rolle. Durch eine interdisziplinäre Vorgehensweise wurde es in den letzten Jahrzenten möglich, die Erkennung und Detektion von Objekten besser zu verstehen und psychophysisch plausible Modelle zu entwickeln. Im Folgenden werden zunächst die wichtigsten Prozesse und Repräsentationen dargestellt, welche von Mensch und Maschine für die Erkennung von Objekten unter verschiedenen Wahrnehmungsbedingungen eingesetzt werden können (Kapitel 2). Anschliessend wird in Kapitel 3 die Signaldetektionstheorie (SDT) besprochen, welche interdisziplinär angewandte Methoden zur Messung von Detektions-und Erkennungsprozessen zur Verfügung stellt. In Kapitel 4 wird anhand ausgewählter Beispiele illustriert, wie theoretische Ansätze der Objekterkennung und psychophysische Methoden der SDT angewendet werden können. Dabei wird am Beispiel der Gepäckkontrolle an Flughäfen veranschaulicht, wie die SDT zur Messung der Erkennungsleistung verbotener Gegenstände in Röntgenbildern angewendet werden kann. Am Beispiel der Gesichtserkennung wird gezeigt, wie mittels SDT verschiedene Computeralgorithmen mit der menschlichen Erkennungsleistung verglichen werden können.",Bernhard Schölkopf and Alex Smola,32,4680335667041382422,,,5328-5335,Wiley,Support vector machines and kernel algorithms,https://pure.mpg.de/rest/items/item_1791668/component/file_3177433/content,,2005,/scholar?cites=4680335667041382422,DZ-fHPgAAAAJ:cT3ycOdf_ycC
14375,"We study gender discrimination of human faces using a combination of psychophysical classification and discrimination experiments together with methods from machine learning. We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classifiers on this reduced representation (linear support vector machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classifiers) using human classification data. Because we combine a linear preprocessor with linear classifiers, the entire system acts as a linear classifier, allowing us to visualise the decision-image corresponding to the normal vector of the separating hyperplanes (SH) of each classifier. We predict that the female-tomaleness transition along the normal vector for classifiers closely mimicking human classification (SVM and RVM [1]) should be faster than the transition along any other direction. A psychophysical discrimination experiment using the decision images as stimuli is consistent with this prediction.",Felix A Wichmann and Arnulf Graf and Heinrich Bülthoff and Eero Simoncelli and Bernhard Schölkopf,32,10592549350826196493,Advances in neural information processing systems,,1489-1496,,Machine learning applied to perception: Decision images for gender classification,https://papers.nips.cc/paper/2004/file/1b113258af3968aaf3969ca67e744ff8-Paper.pdf,17,2004,/scholar?cites=10592549350826196493,DZ-fHPgAAAAJ:o0B9MCxaBXUC
14376,"The computation of classical higher-order statistics such as higher-order moments or spectra is difficult for images due to the huge number of terms to be estimated and interpreted. We propose an alternative approach in which multiplicative pixel interactions are described by a series of Wiener functionals. Since the functionals are estimated implicitly via polynomial kernels, the combinatorial explosion associated with the classical higher-order statistics is avoided. First results show that image structures such as lines or corners can be predicted correctly, and that pixel interactions up to the order of five play an important role in natural images.Most of the interesting structure in a natural image is characterized by its higher-order statistics. Arbitrarily oriented lines and edges, for instance, cannot be described by the usual pairwise statistics such as the power spectrum or the autocorrelation function: From knowing the intensity of one point on a line alone, we cannot predict its neighbouring intensities. This would require knowledge of a second point on the line, ie, we have to consider some third-order statistics which describe the interactions between triplets of points. Analogously, the prediction of a corner neighbourhood needs at least fourth-order statistics, and so on.",Matthias Franz and Bernhard Schölkopf,32,17447226392342369817,Advances in neural information processing systems,,465-472,,Implicit Wiener series for higher-order image analysis,https://proceedings.neurips.cc/paper/2004/file/321cf86b4c9f5ddd04881a44067c2a5a-Paper.pdf,17,2004,/scholar?cites=17447226392342369817,DZ-fHPgAAAAJ:BJrgspguQaEC
14377,"We study gender discrimination of human faces using a combination of psychophysical classification and discrimination experiments together with methods from machine learning. We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classifiers on this reduced representation (linear support vector machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classifiers) using human classification data. Because we combine a linear preprocessor with linear classifiers, the entire system acts as a linear classifier, allowing us to visualise the decision-image corresponding to the normal vector of the separating hyperplanes (SH) of each classifier. We predict that the female-tomaleness transition along the normal vector for classifiers closely mimicking human classification (SVM and RVM [1]) should be faster than the transition along any other direction. A psychophysical discrimination experiment using the decision images as stimuli is consistent with this prediction.",Felix A Wichmann and Arnulf Graf and Heinrich Bülthoff and Eero Simoncelli and Bernhard Schölkopf,32,10592549350826196493,Advances in neural information processing systems,,1489-1496,,Machine learning applied to perception: Decision images for gender classification,https://papers.nips.cc/paper/2004/file/1b113258af3968aaf3969ca67e744ff8-Paper.pdf,17,2004,/scholar?cites=10592549350826196493,DZ-fHPgAAAAJ:gsN89kCJA0AC
14378,"電子情報通信学会技術研究報告. IBISML, 情報論的学習理論と機械学習= IEICE technical report. IBISML, Information-based induction sciences and machine learning 111 (275), 243-249, 2011-11-02",M Tipping,32,10243367413149601994,"Proc. of AISTATS, 2001",,,,A kernel approach for vector quantization with guaranteed distortion bounds,https://ci.nii.ac.jp/naid/10031100466/,,2001,/scholar?cites=10243367413149601994,DZ-fHPgAAAAJ:TIZ-Mc8IlK0C
14379,"Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.",Anirudh Goyal and Alex Lamb and Jordan Hoffmann and Shagun Sodhani and Sergey Levine and Yoshua Bengio and Bernhard Schölkopf,31,10756182227982355233,arXiv preprint arXiv:1909.10893,,,,Recurrent independent mechanisms,https://arxiv.org/abs/1909.10893,,2019,/scholar?cites=10756182227982355233,DZ-fHPgAAAAJ:HoRspIuzbTgC
14380,"We generalize traditional goals of clustering towards distinguishing components in a non-parametric mixture model. The clusters are not necessarily based on point locations, but on higher order criteria. This framework can be implemented by embedding probability distributions in a Hilbert space. The corresponding clustering objective is very general and relates to a range of common clustering concepts.",Stefanie Jegelka and Arthur Gretton and Bernhard Schölkopf and Bharath K Sriperumbudur and Ulrike Von Luxburg,31,15971053680199295491,,,144-152,"Springer, Berlin, Heidelberg",Generalized clustering via kernel embeddings,https://link.springer.com/chapter/10.1007/978-3-642-04617-9_19,,2009,/scholar?cites=15971053680199295491,DZ-fHPgAAAAJ:hkOj_22Ku90C
14381,"Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al.(2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01--0.5\% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.",Francesco Locatello and Michael Tschannen and Stefan Bauer and Gunnar Rätsch and Bernhard Schölkopf and Olivier Bachem,30,6110804235668339123,arXiv preprint arXiv:1905.01258,,,,Disentangling factors of variation using few labels,https://arxiv.org/abs/1905.01258,,2019,/scholar?cites=6110804235668339123,DZ-fHPgAAAAJ:NKamU9JBio0C
14382,"Spaced repetition is a technique for efficient memorization which uses repeated review of content following a schedule determined by a spaced repetition algorithm to improve long-term retention. However, current spaced repetition algorithms are simple rule-based heuristics with a few hard-coded parameters. Here, we introduce a flexible representation of spaced repetition using the framework of marked temporal point processes and then address the design of spaced repetition algorithms with provable guarantees as an optimal control problem for stochastic differential equations with jumps. For two well-known human memory models, we show that, if the learner aims to maximize recall probability of the content to be learned subject to a cost on the reviewing frequency, the optimal reviewing schedule is given by the recall probability itself. As a result, we can then develop a simple, scalable online spaced repetition …",Behzad Tabibian and Utkarsh Upadhyay and Abir De and Ali Zarezade and Bernhard Schölkopf and Manuel Gomez-Rodriguez,30,5019479710088363145,Proceedings of the National Academy of Sciences,10,3988-3993,National Academy of Sciences,Enhancing human learning via spaced repetition optimization,https://www.pnas.org/content/116/10/3988.short,116,2019,/scholar?cites=5019479710088363145,DZ-fHPgAAAAJ:9U6a4ZeTB_UC
14383,"Density estimation is a fundamental problem in statistical learning. This problem is especially challenging for complex high-dimensional data due to the curse of dimensionality. A promising solution to this problem is given here in an inference-free hierarchical framework that is built on score matching. We revisit the Bayesian interpretation of the score function and the Parzen score matching, and construct a multilayer perceptron with a scalable objective for learning the energy (ie the unnormalized log-density), which is then optimized with stochastic gradient descent. In addition, the resulting deep energy estimator network (DEEN) is designed as products of experts. We present the utility of DEEN in learning the energy, the score function, and in single-step denoising experiments for synthetic and high-dimensional data. We also diagnose stability problems in the direct estimation of the score function that had been observed for denoising autoencoders.",Saeed Saremi and Arash Mehrjou and Bernhard Schölkopf and Aapo Hyvärinen,30,804586678207297504,arXiv preprint arXiv:1805.08306,,,,Deep energy estimator networks,https://arxiv.org/abs/1805.08306,,2018,/scholar?cites=804586678207297504,DZ-fHPgAAAAJ:PAUcJbK1cpwC
14384,"Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their contents. These explicit feedback mechanisms can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy measurements, often biased, to distill a robust, unbiased and interpretable measure of both notions?",Behzad Tabibian and Isabel Valera and Mehrdad Farajtabar and Le Song and Bernhard Schölkopf and Manuel Gomez-Rodriguez,30,1847415597885131451,,,847-855,,Distilling information reliability and source trustworthiness from digital traces,https://dl.acm.org/doi/abs/10.1145/3038912.3052672,,2017,/scholar?cites=1847415597885131451,DZ-fHPgAAAAJ:kM62dKQefFYC
14385,"We describe a method for removing the effect of confounders to reconstruct a latent quantity of interest. The method, referred to as “half-sibling regression,” is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification, discussing both independent and identically distributed as well as time series data, respectively, and illustrate the potential of the method in a challenging astronomy application.",Bernhard Schölkopf and David W Hogg and Dun Wang and Daniel Foreman-Mackey and Dominik Janzing and Carl-Johann Simon-Gabriel and Jonas Peters,30,2429561747341807338,Proceedings of the National Academy of Sciences,27,7391-7398,National Academy of Sciences,Modeling confounding by half-sibling regression,https://www.pnas.org/content/113/27/7391.short,113,2016,/scholar?cites=2429561747341807338,DZ-fHPgAAAAJ:NNPL2XyPKO8C
14386,"Causal discovery via the asymmetry between the cause and the effect has proved to be a promising way to infer the causal direction from observations. The basic idea is to assume that the mechanism generating the cause distribution p(x) and that generating the conditional distribution p(y|x) correspond to two independent natural processes and thus p(x) and p(y|x) fulfill some sort of independence condition. However, in many situations, the independence condition does not hold for the anticausal direction; if we consider p(x, y) as generated via p(y)p(x|y), then there are usually some contrived mutual adjustments between p(y) and p(x|y). This kind of asymmetry can be exploited to identify the causal direction. Based on this postulate, in this letter, we define an uncorrelatedness criterion between p(x) and p(y|x) and, based on this uncorrelatedness, show asymmetry between the cause and the effect in terms that a …",Zhitang Chen and Kun Zhang and Laiwan Chan and Bernhard Schölkopf,30,4821282517701140399,Neural computation,7,1484-1517,MIT Press,Causal discovery via reproducing kernel hilbert space embeddings,https://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00599,26,2014,/scholar?cites=4821282517701140399,DZ-fHPgAAAAJ:9tXw7Op4-u0C
14387,,Peter J Bartlett and Bernhard Schölkopf and Dale Schuurmans and Alexander J Smola,30,12288702141990906182,,,,MIT Press,Advances in Large-Margin Classifiers,http://scholar.google.com/scholar?cluster=12288702141990906182&hl=en&oi=scholarr,,2000,/scholar?cites=12288702141990906182,DZ-fHPgAAAAJ:BGuocZfU5CQC
14388,"We address the problem of inferring the causal relation between two variables by comparing the least-squares errors of the predictions in both possible causal directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable method that only requires a regression in both possible causal directions. The performance of this method is compared with different related causal inference methods in various artificial and real-world data sets.",Patrick Blöbaum and Dominik Janzing and Takashi Washio and Shohei Shimizu and Bernhard Schölkopf,29,1986845674977853709,,,900-909,,Cause-effect inference by comparing regression errors,http://proceedings.mlr.press/v84/bloebaum18a.html,,2018,/scholar?cites=1986845674977853709,DZ-fHPgAAAAJ:JhFzC59B_DkC
14389,"232 Kernel-Based Integration of Genomic Data Using Semidefinite Programming predictions on a variety of sources of information, including amino acid sequence, hydropathy profiles, gene expression data, known protein-protein interactions, and known protein complexes. We show that a support vector machine (SVM) trained from all of these data, using the combined kernel, performs significantly better than the same algorithm trained on any single type of data, and better than previously described approaches.",Gert RG Lanckriet and Nello Cristianini and Michael I Jordan and William Stafford Noble,29,1575463990239505313,Kernel Methods in Computational Biology,,231,MIT Press,1 kernel-based integration of genomic data using semidefinite programming,http://books.google.com/books?hl=en&lr=&id=SwAooknaMXgC&oi=fnd&pg=PA231&dq=info:oSul5iYs3RUJ:scholar.google.com&ots=rMwgCvR7Eo&sig=f7SVsU59TZfRseMA4zK6h7igRRI,,2004,/scholar?cites=1575463990239505313,DZ-fHPgAAAAJ:vXriF0DZYm8C
14390,"Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the L1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension-dependence persists after either usual or robust training, but gets attenuated with higher regularization.",Carl-Johann Simon-Gabriel and Yann Ollivier and Leon Bottou and Bernhard Schölkopf and David Lopez-Paz,28,926683092554330851,,,5809-5817,PMLR,First-order adversarial vulnerability of neural networks and input dimension,http://proceedings.mlr.press/v97/simon-gabriel19a.html,,2019,/scholar?cites=926683092554330851,DZ-fHPgAAAAJ:9nyh0FOz-90C
14391,"Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1- regularized likelihood maximization framework. We show that, as long as the cascade …",Manuel Gomez-Rodriguez and Le Song and Hadi Daneshmand and Bernhard Schölkopf,28,13429345025545386697,The Journal of Machine Learning Research,1,3092-3120,JMLR. org,"Estimating diffusion networks: Recovery conditions, sample complexity & soft-thresholding algorithm",https://dl.acm.org/doi/abs/10.5555/2946645.3007043,17,2016,/scholar?cites=13429345025545386697,DZ-fHPgAAAAJ:Uett3gagOK8C
14392,"Physiological nonrigid motion is inevitable when imaging, e.g., abdominal viscera, and can lead to serious deterioration of the image quality. Prospective techniques for motion correction can handle only special types of nonrigid motion, as they only allow global correction. Retrospective methods developed so far need guidance from navigator sequences or external sensors. We propose a fully retrospective nonrigid motion correction scheme that only needs raw data as an input.Our method is based on a forward model that describes the effects of nonrigid motion by partitioning the image into patches with locally rigid motion. Using this forward model, we construct an objective function that we can optimize with respect to both unknown motion parameters per patch and the underlying sharp image.We evaluate our method on both synthetic and real data in 2D and 3D. In vivo data was …",Alexander Loktyushin and Hannes Nickisch and Rolf Pohmann and Bernhard Schölkopf,28,12221739518523546752,Magnetic resonance in medicine,4,1457-1468,,Blind multirigid retrospective motion correction of MR images,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.25266,73,2015,/scholar?cites=12221739518523546752,DZ-fHPgAAAAJ:Ec25XKZgu8sC
14393,"Information-Geometric Causal                                                                Inference (IGCI) is a new approach to distinguish between cause and effect for two variables. It is based on an independence assumption between input distribution and causal mechanism that can be phrased in terms of orthogonality in information space. We describe two intuitive reinterpretations of this approach that make IGCI more accessible to a broader audience. Moreover, we show that the described independence is related to the hypothesis that                                                               unsupervised learning and                                                               semi-supervised learning only work for predicting the cause from the effect and not vice versa.",Dominik Janzing and Bastian Steudel and Naji Shajarisales and Bernhard Schölkopf,28,10256669543187321904,,,253-265,"Springer, Cham",Justifying information-geometric causal inference,https://link.springer.com/chapter/10.1007/978-3-319-21852-6_18,,2015,/scholar?cites=10256669543187321904,DZ-fHPgAAAAJ:s2G-WRnXBicC
14394,"Identification of a determinative subset of features from within a group of features is performed by training a support vector machine using training samples with class labels to determine a value of each feature, where features are removed based on their the value. One or more features having the smallest values are removed and an updated kernel matrix is generated using the remaining features. The process is repeated until a predetermined number of features remain which are capable of accurately separating the data into different classes.",,28,15504390436702532389,,,,,Support vector machine—Recursive feature elimination (SVM-RFE),https://patents.google.com/patent/US8095483B2/en,,2012,/scholar?cites=15504390436702532389,DZ-fHPgAAAAJ:qlbHnvxaeWYC
14395,"Cyclic graphical models are unnecessary for accurate representation of joint probability distributions, but are often indispensable when a causal representation of variable relationships is desired. For variables with a cyclic causal dependence structure, DAGs are guaranteed not to recover the correct causal structure, and therefore may yield false predictions about the outcomes of perturbations (and even inference.) In this paper, we introduce an approach to generalize Bayesian Network structure learning to structures with cyclic dependence. We introduce a structure learning algorithm, prove its performance given reasonable assumptions, and use simulated data to compare its results to the results of standard Bayesian network structure learning. We then propose a modified, heuristic algorithm with more modest data requirements, and test its performance on a real-life dataset from molecular biology, containing causal, cyclic dependencies. c○ 2010 S. Itani and M. Ohannessian",Sleiman Itani and Mesrob Ohannessian and Karen Sachs and Garry P Nolan and Munther A Dahleh,28,16402832802214627957,,,165-176,,Structure learning in causal cyclic networks,http://www.jmlr.org/proceedings/papers/v6/itani10a/itani10a.pdf,,2010,/scholar?cites=16402832802214627957,DZ-fHPgAAAAJ:zdX0sdgBH_kC
14396,"In computational biology, it is common to represent domain knowledge using graphs. Frequently there exist multiple graphs for the same set of nodes, representing information from different sources, and no single graph is sufficient to predict class labels of unlabelled nodes reliably. One way to enhance reliability is to integrate multiple graphs, since individual graphs are partly independent and partly complementary to each other for prediction. In this chapter, we describe an algorithm to assign weights to multiple graphs within graph-based semi-supervised learning. Both predicting class labels and searching for weights for combining multiple graphs are formulated into one convex optimization problem. The graph-combining method is applied to functional class prediction of yeast proteins. When compared with individual graphs, the combined graph with optimized weights performs significantly better than any single graph. When compared with the semidefinite programming-based support vector machine (SDP/SVM), it shows comparable accuracy in a remarkably short time. Compared with a combined graph with equal-valued weights, our method could select important graphs without loss of accuracy, which implies the desirable property of integration with selectivity.",Hyunjung Shin and Koji Tsuda,28,12601225571972754108,,,361-376,MIT press,Prediction of protein function from networks,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790721,,2006,/scholar?cites=12601225571972754108,DZ-fHPgAAAAJ:5p9vMBpPSXYC
14397,"Systems and methods for object or pattern detection that use a nonlinear support vector (SV) machine are described. In the illustrated and described embodiment, objects or patterns comprising faces are detected. The decision surface is approximated in terms of a reduced set of expansion vectors. In order to determine the presence of a face, the kernelized inner product of the expansion vectors with the input pattern are sequentially evaluated and summed, such that if at any point the pattern can be rejected as not comprising a face, no more expansion vectors are used. The sequential application of the expansion vectors produces a substantial saving in computational time.",,28,9813886116385778011,,,,,"Pattern detection methods and systems, and face detection methods and systems",https://patents.google.com/patent/US6804391B1/en,,2004,/scholar?cites=9813886116385778011,DZ-fHPgAAAAJ:NJ774b8OgUMC
14398,"This paper presents a method for single-frame image superresolution using an unsupervised learning technique. The required prior knowledge about the high-resolution images is obtained from Kernel Principal Component Analysis (KPCA). The original form of KPCA, however, can be only applied to strongly restricted image classes due to the limited number of training examples that can be processed. We therefore propose a new iterative method for performing KPCA, the Kernel Hebbian Algorithm. By kernelizing the Generalized Hebbian Algorithm, one can iteratively estimate the Kernel Principal Components with only linear order memory complexity. The resulting super-resolution algorithm shows a comparable performance to the existing supervised methods on images containing faces and natural scenes.",Kwang In Kim and Matthias Franz and Bernhard Schölkopf,28,16965592068365034443,,,,,Kernel hebbian algorithm for single-frame super-resolution,https://eprints.lancs.ac.uk/id/eprint/69832/,,2004,/scholar?cites=16965592068365034443,DZ-fHPgAAAAJ:M7yex6snE4oC
14399,"A method is proposed which computes a direction in a dataset such that a specified fraction of a particular class of all examples is separated from the overall mean by a maximal margin. The projector onto that direction can be used for class-specific feature extraction. The algorithm is carried out in a feature space associated with a support vector kernel function, hence it can be used to construct a large class of nonlinear feature extractors. In the particular case where there exists only one class, the method can be thought of as a robust form of principal component analysis, where instead of variance we maximize percentile thresholds. Finally, we generalize it to also include the possibility of specifying negative examples. 1 Introduction and Notation Suppose we are given two sets of data: a set of points Z= fz 1;:::; ztg (1) which we think of as representative of the kind of data that we typically encounter in some problem of interest, and a second set X= f...",Bernhard Schölkopf and John C Platt and Alex J Smola,28,3325051936366531709,,,,,Kernel method for percentile feature extraction,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.4689,,2000,/scholar?cites=3325051936366531709,DZ-fHPgAAAAJ:g3aElNc5_aQC
14400,,Schölkopf Bernhard and Smola Alexander,28,3019783727665677834,,,,,Kernel principal component analysis,http://scholar.google.com/scholar?cluster=3019783727665677834&hl=en&oi=scholarr,,1997,/scholar?cites=3019783727665677834,DZ-fHPgAAAAJ:joaEdbMnRSAC
14401,"Causality is a fundamental notion in science, and plays an important role in explanation, prediction, decision making and control. Recently, with the rapid accumulation of huge volumes of data, it is even more desirable to abstract causal knowledge from data. Furthermore, such data are usually time series measured over a relatively long time period or aggregated data from multiple data sets collected in different environments or under different experimental conditions, leading to the issue of data heterogeneity. Causality also provides a way to understand and tackle data heterogeneity, while traditional machine learning",Kun Zhang and Bernhard Schölkopf and Peter Spirtes and Clark Glymour,27,15179850672529769732,National science review,1,26-29,Oxford University Press,Learning causality and causality-related learning: some recent progress,https://academic.oup.com/nsr/article-abstract/5/1/26/4638533,5,2018,/scholar?cites=15179850672529769732,DZ-fHPgAAAAJ:nFsND8bGYWMC
14402,"Complex systems can be modelled at various levels of detail. Ideally, causal models of the same system should be consistent with one another in the sense that they agree in their predictions of the effects of interventions. We formalise this notion of consistency in the case of Structural Equation Models (SEMs) by introducing exact transformations between SEMs. This provides a general language to consider, for instance, the different levels of description in the following three scenarios:(a) models with large numbers of variables versus models in which theirrelevant'or unobservable variables have been marginalised out;(b) micro-level models versus macro-level models in which the macro-variables are aggregate features of the micro-variables;(c) dynamical time series models versus models of their stationary behaviour. Our analysis stresses the importance of well specified interventions in the causal modelling process and sheds light on the interpretation of cyclic SEMs.",Paul K Rubenstein and Sebastian Weichwald and Stephan Bongers and Joris M Mooij and Dominik Janzing and Moritz Grosse-Wentrup and Bernhard Schölkopf,27,4933968458719174010,arXiv preprint arXiv:1707.00819,,,,Causal consistency of structural equation models,https://arxiv.org/abs/1707.00819,,2017,/scholar?cites=4933968458719174010,DZ-fHPgAAAAJ:ysAkk8fzSpoC
14403,"We present a neural network model approach for multi-frame blind deconvolution. The discriminative approach adopts and combines two recent techniques for image deblurring into a single neural network architecture. Our proposed hybrid-architecture combines the explicit prediction of a deconvolution filter and non-trivial averaging of Fourier coefficients in the frequency domain. In order to make full use of the information contained in all images in one burst, the proposed network embeds smaller networks, which explicitly allow the model to transfer information between images in early layers. Our system is trained end-to-end using standard backpropagation on a set of artificially generated training examples, enabling competitive performance in multi-frame blind deconvolution, both with respect to quality and runtime.",Patrick Wieschollek and Bernhard Schölkopf and Hendrik PA Lensch and Michael Hirsch,27,18289039093001917868,,,35-51,"Springer, Cham",End-to-end learning for image burst deblurring,https://link.springer.com/chapter/10.1007/978-3-319-54190-7_3,,2016,/scholar?cites=18289039093001917868,DZ-fHPgAAAAJ:ql9EF6h3GboC
14404,"Recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case. For that purpose, however, one has to impose substantial structural constraints or smoothness assumptions on the functional causal models. In this paper, we consider the problem of determining the causal direction from a related but different point of view, and propose a new framework for causal direction determination. We show that it is possible to perform causal inference based on the condition that the cause is"" exogenous"" for the parameters involved in the generating process from the cause to the effect. In this way, we avoid the structural constraints required by the SEM-based approaches. In particular, we exploit nonparametric methods to estimate marginal and conditional distributions, and propose a bootstrap-based approach to test for the exogeneity condition; the testing results indicate the causal direction between two variables. The proposed method is validated on both synthetic and real data.",Kun Zhang and Jiji Zhang and Bernhard Schölkopf,27,14200793739984711011,arXiv preprint arXiv:1504.05651,,,,Distinguishing cause from effect based on exogeneity,https://arxiv.org/abs/1504.05651,,2015,/scholar?cites=14200793739984711011,DZ-fHPgAAAAJ:n1hJTA2faNgC
14405,"State-of-the-art video restoration methods integrate optical flow estimation networks to utilize temporal information. However, these networks typically consider only a pair of consecutive frames and hence are not capable of capturing long-range temporal dependencies and fall short of establishing correspondences across several timesteps. To alleviate these problems, we propose a novel Spatio-temporal Transformer Network (STTN) which handles multiple frames at once and thereby manages to mitigate the common nuisance of occlusions in optical flow estimation. Our proposed STTN comprises a module that estimates optical flow in both space and time and a resampling layer that selectively warps target frames using the estimated flow. In our experiments, we demonstrate the efficiency of the proposed network and show state-of-the-art restoration results in video super-resolution and video deblurring.",Tae Hyun Kim and Mehdi SM Sajjadi and Michael Hirsch and Bernhard Schölkopf,26,3787176442761009203,,,111-127,"Springer, Cham",Spatio-temporal transformer network for video restoration,https://link.springer.com/chapter/10.1007/978-3-030-01219-9_7,,2018,/scholar?cites=3787176442761009203,DZ-fHPgAAAAJ:QdBzNGYzwCUC
14406,,Henry Horng-Shing LU and Bernhard SCHöLKOPF and Hongyu ZHAO,26,8728040453436050392,,,02,,Handbook of statistical bioinformatics (series: springer handbooks of computational statistics),,67,2011,/scholar?cites=8728040453436050392,DZ-fHPgAAAAJ:L_l9e5I586QC
14407,"The increasing complexity of modern robots makes it prohibitively hard to accurately model such systems as required by many applications. In such cases, machine learning methods offer a promising alternative for approximating such models using measured data. To date, high computational demands have largely restricted machine learning techniques to mostly offline applications. However, making the robots adaptive to changes in the dynamics and to cope with unexplored areas of the state space requires online learning. In this paper, we propose an approximation of the support vector regression (SVR) by sparsification based on the linear independency of training data. As a result, we obtain a method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques, such as v-SVR, Gaussian process regression (GPR) and locally …",Duy Nguyen-Tuong and Bernhard Schölkopf and Jan Peters,26,6557909331451881767,,,3121-3126,IEEE,Sparse online model learning for robot control with support vector regression,https://ieeexplore.ieee.org/abstract/document/5354609/,,2009,/scholar?cites=6557909331451881767,DZ-fHPgAAAAJ:5bg8sr1QxYwC
14408,"This book constitutes the refereed post-proceedings of the First PASCAL Machine Learning Challenges Workshop, MLCW 2005. 25 papers address three challenges: finding an assessment base on the uncertainty of predictions using classical statistics, Bayesian inference, and statistical learning theory; second, recognizing objects from a number of visual object classes in realistic scenes; third, recognizing textual entailment addresses semantic analysis of language to form a generic framework for applied semantic inference in text understanding.",Joaquin Quiñonero-Candela and Ido Dagan and Bernardo Magnini and Florence D'Alché-Buc,26,9350852143941361368,,,,Springer,"Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognizing Textual Entailment, First Pascal Machine Learning Challenges …",http://books.google.com/books?hl=en&lr=&id=TT73BwAAQBAJ&oi=fnd&pg=PA1&dq=info:2B7TYnvmxIEJ:scholar.google.com&ots=cuSYJ1Xqe5&sig=KM4DsiqoZzzdi9KZwspB43ia5OM,3944,2006,/scholar?cites=9350852143941361368,DZ-fHPgAAAAJ:mel-f30kHHgC
14409,"We have studied the application of different classification algorithms in the analysis of simulated high energy physics data. Whereas Neural Network algorithms have become a standard tool for data analysis, the performance of other classifiers such as Support Vector Machines has not yet been tested in this environment. We chose two different problems to compare the performance of a Support Vector Machine and a Neural Net trained with back-propagation: tagging events of the type e+ e--> ccbar and the identification of muons produced in multihadronic e+ e-annihilation events.",Ph Vannerem and K-R Müller and B Schölkopf and A Smola and S Soldner-Rembold,26,418173942318429007,arXiv preprint hep-ex/9905027,,,,Classifying LEP data with support vector algorithms,https://arxiv.org/abs/hep-ex/9905027,,1999,/scholar?cites=418173942318429007,DZ-fHPgAAAAJ:L7CI7m0gUJcC
14410,"Despite intensive efforts, no significant benefit of rehabilitation robotics in post-stroke motor-recovery has yet been demonstrated in large-scale clinical trials. The present work is based on the premise that future advances in rehabilitation robotics require an enhanced understanding of the neural processes involved in motor learning after stroke. We present a system that combines a Barret WAM™seven degree-of-freedom robot arm with neurophysiological recordings for the purpose of studying post-stroke motor learning. We used this system to conduct a pilot study on motor learning during reaching movements with two stroke patients. Preliminary results indicate that pre-trial brain activity in ipsilesional sensorimotor areas may be a neural correlate of the current state of motor learning. These results are discussed in terms of their relevance for future rehabilitation strategies that combine rehabilitation robotics with …",Timm Meyer and Jan Peters and Doris Brötz and Thorsten O Zander and Bernhard Schölkopf and Surjo R Soekadar and Moritz Grosse-Wentrup,25,12965232419263800881,,,4078-4083,IEEE,A brain-robot interface for studying motor learning after stroke,https://ieeexplore.ieee.org/abstract/document/6385646/,,2012,/scholar?cites=12965232419263800881,DZ-fHPgAAAAJ:oPLKW5k6eA4C
14411,"Genome-wide association studies (GWAS) have not been able to discover strong associations between many complex human diseases and single genetic loci. Mapping these phenotypes to pairs of genetic loci is hindered by the huge number of candidates leading to enormous computational and statistical problems. In GWAS on single nucleotide polymorphisms (SNPs), one has to consider in the order of 10 10 to 10 14 pairs, which is infeasible in practice. In this article, we give the first algorithm for 2-locus genome-wide association studies that is subquadratic in the number, n, of SNPs. The running time of our algorithm is data-dependent, but large experiments over real genomic data suggest that it scales empirically as n 3/2. As a result, our algorithm can easily cope with n~ 10 7, ie, it can efficiently search all pairs of SNPs in the human genome.",Panagiotis Achlioptas and Bernhard Schölkopf and Karsten Borgwardt,25,1952001411934470607,,,726-734,,Two-locus association mapping in subquadratic time,https://dl.acm.org/doi/abs/10.1145/2020408.2020521,,2011,/scholar?cites=1952001411934470607,DZ-fHPgAAAAJ:J-pR_7NvFogC
14412,"In this paper, we develop and analyze a nonparametric method for estimating the class of integral probability metrics (IPMs), examples of which include the Wasserstein distance, Dudley metric, and maximum mean discrepancy (MMD). We show that these distances can be estimated efficiently by solving a linear program in the case of Wasserstein distance and Dudley metric, while MMD is computable in a closed form. All these estimators are shown to be strongly consistent and their convergence rates are analyzed. Based on these results, we show that IPMs are simple to estimate and the estimators exhibit good convergence behavior compared to ø-divergence estimators.",Bharath K Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Gert RG Lanckriet,25,3564797729191557784,,,1428-1432,IEEE,Non-parametric estimation of integral probability metrics,https://ieeexplore.ieee.org/abstract/document/5513626/,,2010,/scholar?cites=3564797729191557784,DZ-fHPgAAAAJ:wvYxNZNCP7wC
14413,"The causal Markov condition (CMC) is a postulate that links observations to causality. It describes the conditional independences among the observations that are entailed by a causal hypothesis in terms of a directed acyclic graph. In the conventional setting, the observations are random variables and the independence is a statistical one, ie, the information content of observations is measured in terms of Shannon entropy. We formulate a generalized CMC for any kind of observations on which independence is defined via an arbitrary submodular information measure. Recently, this has been discussed for observations in terms of binary strings where information is understood in the sense of Kolmogorov complexity. Our approach enables us to find computable alternatives to Kolmogorov complexity, eg, the length of a text after applying existing data compression schemes. We show that our CMC is justified if one restricts the attention to a class of causal mechanisms that is adapted to the respective information measure. Our justification is similar to deriving the statistical CMC from functional models of causality, where every variable is a deterministic function of its observed causes and an unobserved noise term.",Bastian Steudel and Dominik Janzing and Bernhard Schölkopf,25,6209837975319001362,arXiv preprint arXiv:1002.4020,,,,Causal markov condition for submodular information measures,https://arxiv.org/abs/1002.4020,,2010,/scholar?cites=6209837975319001362,DZ-fHPgAAAAJ:Mojj43d5GZwC
14414,This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian flow field which we compute using ideas from kernel methods. We demonstrate the efficacy of our approach on various real world data sets.,Christian Walder and Bernhard Schölkopf,25,9341714341113447399,Advances in Neural Information Processing Systems,,1713-1720,,Diffeomorphic dimensionality reduction,https://papers.nips.cc/paper/2008/hash/647bba344396e7c8170902bcf2e15551-Abstract.html,21,2008,/scholar?cites=9341714341113447399,DZ-fHPgAAAAJ:PELIpwtuRlgC
14415,"We introduce a game-theoretic model for network formation inspired by earlier stochastic models that mix localized and long-distance connectivity. In this model, players may purchase edges at distance d at a cost of dα, and wish to minimize the sum of their edge purchases and their average distance to other players. In this model, we show there is a striking “small world” threshold phenomenon: in two dimensions, if α< 2 then every Nash equilibrium results in a network of constant diameter (independent of network size), and if α> 2 then every Nash equilibrium results in a network whose diameter grows as a root of the network size, and thus is unbounded. We contrast our results with those of Kleinberg [8] in a stochastic model, and empirically investigate the “navigability” of equilibrium networks. Our theoretical results all generalize to higher dimensions.",Eyal Even-Dar and Michael Kearns,25,16164139540952734992,Advances in Neural Information Processing Systems,,385-392,,A small world threshold for economic network formation,https://proceedings.neurips.cc/paper/2006/file/6917ff2a7b53421ff4066020e2d89eec-Paper.pdf,19,2006,/scholar?cites=16164139540952734992,DZ-fHPgAAAAJ:5p-b37PSfY4C
14416,"Bayesian methods allow for a simple and intuitive representation of the function spaces used by kernel methods. This chapter describes the basic principles of Gaussian Processes, their implementation and their connection to other kernel-based Bayesian estimation methods, such as the Relevance Vector Machine.",AJ Smola and B Schölkopf,25,3831962491103163392,Proceedings of the Machine Learning Summer School,,,Springer,Bayesian Kernel Methods,https://link.springer.com/chapter/10.1007/3-540-36434-X_3,,2003,/scholar?cites=3831962491103163392,DZ-fHPgAAAAJ:a0OBvERweLwC
14417,"This paper proposes a combined learning framework for a table tennis robot. In a typical robot table tennis setup, a single striking point is predicted for the robot on the basis of the ball's initial state. Subsequently, the desired Cartesian racket state and the desired joint states at the striking time are determined. Finally, robot joint trajectories are generated. Instead of predicting a single striking point, we propose to construct a ball trajectory prediction map, which predicts the ball's entire rebound trajectory using the ball's initial state. We construct as well a robot trajectory generation map, which predicts the robot joint movement pattern and the movement duration using the Cartesian racket trajectories without the need of inverse kinematics, where a correlation function is used to adapt these joint movement parameters according to the ball flight trajectory. With joint movement parameters, we can directly generate joint …",Yanlong Huang and Dieter Büchler and Okan Koç and Bernhard Schölkopf and Jan Peters,24,6516789189308975599,,,650-655,IEEE,Jointly learning trajectory generation and hitting point prediction in robot table tennis,https://ieeexplore.ieee.org/abstract/document/7803343/,,2016,/scholar?cites=6516789189308975599,DZ-fHPgAAAAJ:1desnc-wk2oC
14418,"Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on $\R^ d $ and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its -statistic variant, which are usually employed in applications.",Ilya O Tolstikhin and Bharath K Sriperumbudur and Bernhard Schölkopf,24,6696836250651675537,,,1930-1938,,Minimax estimation of maximum mean discrepancy with radial kernels,http://papers.nips.cc/paper/6483-minimax-estimation-of-maximum-mean-discrepancy-with-radial-kernels,,2016,/scholar?cites=6696836250651675537,DZ-fHPgAAAAJ:hRqfHK2Mmv8C
14419,"Obtaining reproducible fiber direction estimates from diffusion MRI is crucial for successful fiber tracking. Modeling and visualizing the probability distribution of the inferred fiber directions is an important step in evaluating and comparing different acquisition schemes and fiber models. However, this distribution is usually strongly dominated by its main direction, which makes it difficult to examine when plotted naively.In this work, we propose a new visualization of the fiber probability distribution. It is based on embedding the probability measure into a particular reproducing kernel Hilbert space. This permits a decomposition into an embedded delta peak, representing the main direction, and a non‐negative residual. They are then combined into a new glyph representation which visually enhances the residual, in order to highlight even subtle differences. Moreover, the magnitude of the delta peak component …",Thomas Schultz and Lara Schlaffke and Bernhard Schölkopf and Tobias Schmidt‐Wilcke,24,3161026062211828885,Computer Graphics Forum,3pt1,121-130,Blackwell Publishing Ltd,HiFiVE: a hilbert space embedding of fiber variability estimates for uncertainty modeling and visualization,https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12099,32,2013,/scholar?cites=3161026062211828885,DZ-fHPgAAAAJ:AFmTUeZ1pmEC
14420,"We compare Karl Popper’s ideas concerning the falsifiability of a theory with similar notions from the part of statistical learning theory known as VC-theory. Popper’s notion of the dimension of a theory is contrasted with the apparently very similar VC-dimension. Having located some divergences, we discuss how best to view Popper’s work from the perspective of statistical learning theory, either as a precursor or as aiming to capture a different learning activity.",David Corfield and Bernhard Schölkopf and Vladimir Vapnik,24,10028237063963240109,Journal for General Philosophy of Science,1,51-58,Springer Netherlands,Falsificationism and statistical learning theory: Comparing the Popper and Vapnik-Chervonenkis dimensions,https://link.springer.com/content/pdf/10.1007/s10838-009-9091-3.pdf,40,2009,/scholar?cites=10028237063963240109,DZ-fHPgAAAAJ:P5F9QuxV20EC
14421,"Multiclass support vector machines (SVMs) have already proved efficient in protein secondary structure prediction as ensemble methods, to combine the outputs of sets of classifiers based on different principles. In this chapter, their implementation as basic prediction methods, processing the primary structure or the profile of multiple alignments, is investigated. A kernel devoted to the task is introduced, which incorporates high-level pieces of knowledge. Initial experimental results illustrate the potential of this approach.",Yann Guermeur and Alain Lifchitz and Régis Vert,24,17509090568552605443,Kernel methods in computational biology,,193-206,"The MIT Press, Cambridge, Massachussetts",A kernel for protein secondary structure prediction,http://books.google.com/books?hl=en&lr=&id=SwAooknaMXgC&oi=fnd&pg=PA193&dq=info:A2e43hrJ_PIJ:scholar.google.com&ots=rMwgCvR8Ao&sig=ZMFUdlxm9dAgnrlXQVLvHBS-QZU,,2004,/scholar?cites=17509090568552605443,DZ-fHPgAAAAJ:iObmAvv4XHcC
14422,"Invariance to nuisance transformations is one of the desirable properties of effective representations. We consider transformations that form a group and propose an approach based on kernel methods to derive local group invariant representations. Locality is achieved by defining a suitable probability distribution over the group which in turn induces distributions in the input feature space. We learn a decision function over these distributions by appealing to the powerful framework of kernel methods and generate local invariant random feature maps via kernel approximations. We show uniform convergence bounds for kernel approximation and provide generalization bounds for learning with these features. We evaluate our method on three real datasets, including Rotated MNIST and CIFAR-10, and observe that it outperforms competing kernel based approaches. The proposed method also outperforms deep CNN on Rotated MNIST and performs comparably to the recently proposed group-equivariant CNN.",Anant Raj and Abhishek Kumar and Youssef Mroueh and Tom Fletcher and Bernhard Schölkopf,23,6687216933805981717,,,1225-1235,PMLR,Local group invariant representations via orbit embeddings,http://proceedings.mlr.press/v54/raj17a.html,,2017,/scholar?cites=6687216933805981717,DZ-fHPgAAAAJ:HkEcjpUCB68C
14423,"Research on the neurophysiological correlates of visuomotor integration and learning (VMIL) has largely focused on identifying learning-induced activity changes in cortical areas during motor execution. While such studies have generated valuable insights into the neural basis of VMIL, little is known about the processes that represent the current state of VMIL independently of motor execution. Here, we present empirical evidence that a subject’s performance in a 3D reaching task can be predicted on a trial-to-trial basis from pre-trial electroencephalographic (EEG) data. This evidence provides novel insights into the brain states that support successful VMIL.Six healthy subjects, attached to a seven degrees-of-freedom (DoF) robot with their right arm, practiced 3D reaching movements in a virtual space, while an EEG recorded their …",Timm Meyer and Jan Peters and Thorsten O Zander and Bernhard Schölkopf and Moritz Grosse-Wentrup,23,16427838418246960431,Journal of neuroengineering and rehabilitation,1,24,BioMed Central,Predicting motor learning performance from electroencephalographic data,https://link.springer.com/article/10.1186/1743-0003-11-24,11,2014,/scholar?cites=16427838418246960431,DZ-fHPgAAAAJ:QKtdBID3u5MC
14424,"Everyday inductive reasoning draws on many kinds of knowledge, including knowledge about relationships between properties and knowledge about relationships between objects. Previous accounts of inductive reasoning generally focus on just one kind of knowledge: models of causal reasoning often focus on relationships between properties, and models of similarity-based reasoning often focus on similarity relationships between objects. We present a Bayesian model of inductive reasoning that incorporates both kinds of knowledge, and show that it accounts well for human inferences about the properties of biological species.",Charles Kemp and Patrick Shafto and Allison Berke and Joshua Tenenbaum,23,7072020238719825050,Advances in neural information processing systems,,681-688,,Combining causal and similarity-based reasoning,https://proceedings.neurips.cc/paper/2006/file/1aa057313c28fa4a40c5bc084b11d276-Paper.pdf,19,2006,/scholar?cites=7072020238719825050,DZ-fHPgAAAAJ:o1WnbgQmKUcC
14425,"Recently the so called Fisher kernel was proposed by 6] to construct discriminative kernel techniques by using generative models. We provide a regularization-theoretic analysis of this approach and extend the set of kernels to a class of natural kernels, all based on generative models with density p (xj), like the original Fisher kernel. This allows us to incorporate distribution dependent smoothness criteria in a general way.As a result of this analyis we show that the Fisher kernel corresponds to a L2 (p) norm regularization. Moreover it allows us to derive explicit representations of the eigensystem of the kernel, give an analysis of the spectrum of the integral operator, and give experimental evidence that this may be used for model selection purposes.",Nuria Oliver and Bernhard Schölkopf and AJ Smola,23,8599258641147682281,Advances in large margin classifiers,,51-60,MIT Press,Natural regularization in SVMs,https://www.researchgate.net/profile/Nuria_Oliver2/publication/2870707_Natural_Regularization_in_SVMs/links/5524d8950cf2b123c5175a0c.pdf,,2000,/scholar?cites=8599258641147682281,DZ-fHPgAAAAJ:BwyfMAYsbu0C
14426,"In visual homing tasks, animals as well as robots can compute their movements from the current view and a snapshot taken at a home position. Solving this problem exactly would require knowledge about the distances to visible landmarks, information, which is not directly available to passive vision systems. We propose a homing scheme that dispenses with accurate distance information by using parameterized disparity fields. These are obtained from an approximation that incorporates prior knowledge about perspective distortions of the visual environment. A mathematical analysis proves that the approximation does not prevent the scheme from approaching the goal with arbitrary accuracy. Mobile robot experiments are used to demonstrate the practical feasibility of the approach.",Matthias O Franz and Bernhard Schölkopf and Heinrich H Bülthoff,23,1198364604294826760,,,236-245,MIT Press,Homing by parameterized scene matching,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1794157,,1997,/scholar?cites=1198364604294826760,DZ-fHPgAAAAJ:hMsQuOkrut0C
14427,"Anticipation can enhance the capability of a robot in its interaction with humans, where the robot predicts the humans' intention for selecting its own action. We present a novel framework of anticipatory action selection for human–robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty. The presented framework is generic and can be used in many human–robot interaction scenarios, for example, in navigation and human–robot co-manipulation. In this article, we conduct a case study on human–robot table tennis. Due to the limited amount of time for executing hitting movements, a robot usually needs to initiate its hitting movement before the opponent hits the ball, which requires the robot to be anticipatory based on visual observation of …",Zhikun Wang and Abdeslam Boularias and Katharina Mülling and Bernhard Schölkopf and Jan Peters,22,13619851125066365121,Artificial Intelligence,,399-414,Elsevier,Anticipatory action selection for human–robot table tennis,https://www.sciencedirect.com/science/article/pii/S0004370214001398,247,2017,/scholar?cites=13619851125066365121,DZ-fHPgAAAAJ:RmcNAhKkducC
14428,"We discuss the problem of fitting an implicit shape model to a set of points sampled from a co-dimension one manifold of arbitrary topology. The method solves a non-convex optimisation problem in the embedding function that defines the implicit by way of its zero level set. By assuming that the solution is a mixture of radial basis functions of varying widths we attain the globally optimal solution by way of an equivalent eigenvalue problem, without using or constructing as an intermediate step the normal vectors of the manifold at each data point. We demonstrate the system on two and three dimensional data, with examples of missing data interpolation and set operations on the resultant shapes.",Christian Walder and Olivier Chapelle and Bernhard Schölkopf,22,2816683468347450062,,,936-939,ACM,Implicit surface modelling as an eigenvalue problem,https://dl.acm.org/doi/abs/10.1145/1102351.1102469,,2005,/scholar?cites=2816683468347450062,DZ-fHPgAAAAJ:FPJr55Dyh1AC
14429,"We review several families of string kernels designed in particular for use with support vector machines (SVMs) for classification of protein sequence data, mismatch kernels, and three newer related models: restricted gappy kernels, substitution kernels, and wildcard kernels. These kernels are based on feature spaces indexed by/-length subsequences or""/-mers"" from the string alphabet S (or the alphabet augmented by a wildcard character) and incorporate various notions of inexact string matching. All the kernels can be computed efficiently with a recursive function based on a trie-based data structure, with computation time that scales linearly with sequence length: the kernel value k (x, y) can be computed in 0 (ck (\x\+\y\)) time, where the constant cfc depends on the parameters of the kernel. In particular, for the newer kernel models, the constant Cfc is independent of the size| E| of the alphabet, which significantly speeds up computation time. Moreover, when used with an SVM classifier, all the kernel models allow linear time prediction on test sequences. Finally, we report protein classification experiments on a benchmark SCOP (structural classification of proteins) data set, where we show that inexact matching kernels achieve SVM classification performance comparable to the best competing methods.",Christina Leslie and Rui Kuang and Eleazar Eskin,22,1349537144173275966,Kernel Methods in Computational Biology,,95-112,"MIT Press, Cambridge, MA",Inexact matching string kernels for protein classification,http://books.google.com/books?hl=en&lr=&id=SwAooknaMXgC&oi=fnd&pg=PA95&dq=info:Pquk6eGEuhIJ:scholar.google.com&ots=rMwgCvR8Cj&sig=THaaEFXmfFHlukzlUrtcScii4-w,,2004,/scholar?cites=1349537144173275966,DZ-fHPgAAAAJ:KG0AXYY1ICoC
14430,"We introduce a novel modeling framework for studying epidemics that is specifically designed to make use of fine-grained spatiotemporal data. Motivated by the current COVID-19 outbreak and the availability of data from contact or location tracing technologies, our model uses marked temporal point processes to represent individual mobility patterns and the course of the disease for each individual in a population. We design an efficient sampling algorithm for our model that can be used to predict the spread of infectious diseases such as COVID-19 under different testing and tracing strategies, social distancing measures, and business restrictions, given location or contact histories of individuals. Building on this algorithm, we use Bayesian optimization to estimate the risk of exposure of each individual at the sites they visit, the percentage of symptomatic individuals, and the difference in transmission rate between asymptomatic and symptomatic individuals from historical longitudinal testing data. Experiments using measured COVID-19 data and mobility patterns from Tübingen, a town in the southwest of Germany, demonstrate that our model can be used to quantify the effects of tracing, testing, and containment strategies at an unprecedented spatiotemporal resolution. To facilitate research and informed policy-making, particularly in the context of the current COVID-19 outbreak, we are releasing an open-source implementation of our framework at this https URL.",Lars Lorch and William Trouleau and Stratis Tsirtsis and Aron Szanto and Bernhard Schölkopf and Manuel Gomez-Rodriguez,21,13827675019921664033,arXiv preprint arXiv:2004.07641,,,,"A spatiotemporal epidemic model to quantify the effects of contact tracing, testing, and containment",https://arxiv.org/abs/2004.07641,,2020,/scholar?cites=13827675019921664033,DZ-fHPgAAAAJ:t02MBfyJvUkC
14431,"Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.",Michel Besserve and Arash Mehrjou and Rémy Sun and Bernhard Schölkopf,21,9392882601140010159,arXiv preprint arXiv:1812.03253,,,,Counterfactuals uncover the modular structure of deep generative models,https://arxiv.org/abs/1812.03253,,2018,/scholar?cites=9392882601140010159,DZ-fHPgAAAAJ:68pgvIssxt4C
14432,"The postulate of independence of cause and mechanism (ICM) has recently led to several new causal discovery algorithms. The interpretation of independence and the way it is utilized, however, varies across these methods. Our aim in this paper is to propose a group theoretic framework for ICM to unify and generalize these approaches. In our setting, the cause-mechanism relationship is assessed by perturbing it with random group transformations. We show that the group theoretic view encompasses previous ICM approaches and provides a very general tool to study the structure of data generating mechanisms with direct applications to machine learning.",Michel Besserve and Naji Shajarisales and Bernhard Schölkopf and Dominik Janzing,21,2221751005076742644,,,557-565,PMLR,Group invariance principles for causal generative models,http://proceedings.mlr.press/v84/besserve18a.html,,2018,/scholar?cites=2221751005076742644,DZ-fHPgAAAAJ:uFXbyyTG2PgC
14433,"Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels.The contributions of this paper are three-fold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can …",Carl-Johann Simon-Gabriel and Bernhard Schölkopf,21,13735259738371120654,The Journal of Machine Learning Research,1,1708-1736,JMLR. org,"Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions",https://dl.acm.org/doi/abs/10.5555/3291125.3309606,19,2018,/scholar?cites=13735259738371120654,DZ-fHPgAAAAJ:_z8dl2W72C8C
14434,"We describe a method that infers whether statistical dependences between two observed variables X and Y are due to a"" direct"" causal link or only due to a connecting causal path that contains an unobserved variable of low complexity, eg, a binary variable. This problem is motivated by statistical genetics. Given a genetic marker that is correlated with a phenotype of interest, we want to detect whether this marker is causal or it only correlates with a causal one. Our method is based on the analysis of the location of the conditional distributions P (Y| x) in the simplex of all distributions of Y. We report encouraging results on semi-empirical data.",Dominik Janzing and Eleni Sgouritsa and Oliver Stegle and Jonas Peters and Bernhard Schölkopf,21,16438063663422105440,arXiv preprint arXiv:1202.3737,,,,Detecting low-complexity unobserved causes,https://arxiv.org/abs/1202.3737,,2012,/scholar?cites=16438063663422105440,DZ-fHPgAAAAJ:Se3iqnhoufwC
14435,"We propose a method to quantify the complexity of conditional probability measures by a Hilbert space seminorm of the logarithm of its density. The concept of reproducing kernel Hilbert spaces (RKHSs) is a flexible tool to define such a seminorm by choosing an appropriate kernel. We present several examples with artificial data sets where our kernel-based complexity measure is consistent with our intuitive understanding of complexity of densities. The intention behind the complexity measure is to provide a new approach to inferring causal directions. The idea is that the factorization of the joint probability measure P (effect, cause) into P (effect| cause) P (cause) leads typically to “simpler” and “smoother” terms than the factorization into P (cause| effect) P (effect). Since the conventional constraint-based approach of causal discovery is not able to determine the causal direction between only two variables, our …",Xiaohai Sun and Dominik Janzing and Bernhard Schölkopf,21,14987743742521562158,Neurocomputing,7-9,1248-1256,Elsevier,Causal reasoning by evaluating the complexity of conditional densities with kernel methods,https://www.sciencedirect.com/science/article/pii/S092523120800060X,71,2008,/scholar?cites=14987743742521562158,DZ-fHPgAAAAJ:9pM33mqn1YgC
14436,"A new method for performing a kernel principal component analysis is proposed. By kernelizing the generalized Hebbian algorithm, one can iteratively estimate the principal components in a reproducing kernel Hilbert space with only linear order memory complexity. The derivation of the method and preliminary applications in image hyperresolution are presented. In addition, we discuss the extension of the method to the online learning of kernel principal components.",Kwang In Kim and Matthias O Franz and Bernhard Schölkopf,21,4367856493956495688,,,1-13,Max Planck Institute for Biological Cybernetics,Kernel Hebbian algorithm for iterative kernel principal component analysis,https://eprints.lancs.ac.uk/id/eprint/69829/,,2003,/scholar?cites=4367856493956495688,DZ-fHPgAAAAJ:HbR8gkJAVGIC
14437,"Although 3D structure of a protein is valuable to predict its function, it is still far more difficult and costly to measure coordinates of atoms in a protein than sequencing its amino acids. We often do not know the 3D structures of all the proteins at hand. Let us consider a kernel matrix that consists of kernel values representing protein similarities in terms of their 3D structures where some of the entries are missing because structure information of some proteins are unavailable whereastheir amino acid sequences are readily available. We proposes to estimate the missing entries by means of another kernel matrix derived from amino acid sequences. Basically a parametric model is created from the sequence kernel matrix, and the missing entries of the structure kernel matrix areestimated by fitting this model to existing entries. For model fitting, we adopt two algorithms: e-projection and em algorithm based on the information geometry of kernel matrices. We performed protein classification experiments by using support vector machines. Our results show that these algorithms can effectively estimate the missing entries.",Taishin Kin and Tsuyoshi Kato and Koji Tsuda and Kiyoshi Asai,21,1608579450079270074,Genome Informatics,,516-517,Japanese Society for Bioinformatics,Protein classification via kernel matrix completion,https://www.jstage.jst.go.jp/article/gi1990/14/0/14_0_516/_article/-char/ja/,14,2003,/scholar?cites=1608579450079270074,DZ-fHPgAAAAJ:sZOHfsbWRCYC
14438,This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the “maximum-margin” algorithm due to Vapnik and others. The key feature of the paper is its operator-theoretic viewpoint. New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension. Specific contents of the paper include:• a new and self-contained proof of Maurey’s theorem and some generalizations with small explicit values of constants;• bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance;• the extension of such classes to those induced by balls in quasi-Banach spaces (such as p-norms with 0 Ô o).• extension of results on the covering numbers of convex hulls of basis functions to Ô-convex hulls (0 Ô< 1);• an appendix containing the tightest known bounds on the entropy numbers of the identity operator between n p1 and n p2 (0 Ô½ Ô¾< o).,Robert C Williamson and Alexander J Smola and Bernhard Schölkopf,21,5121846760813985902,,,309-319,,Entropy Numbers of Linear Function Classes.,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.9499&rep=rep1&type=pdf,,2000,/scholar?cites=5121846760813985902,DZ-fHPgAAAAJ:PoWvk5oyLR8C
14439,"It is known that the covering numbers of a function class on a double sample (length 2m, where m is the number of points in the sample) can be used to bound the generalization performance of a classi er by using a margin based analysis. Traditionally this has been done using a\Sauer-like"" relationship involving a combinatorial dimension such as the fat-shattering dimension. In this paper we show that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points). The signi cance of this is that for certain interesting classes of functions, such as support vector machines, one can readily estimate the empirical covering numbers quite well. We show how to do so in terms of the eigenvalues of the Gram matrix created from the data. These covering numbers can be much less than a priori bounds indicate in situations where the particular data received is\easy"". The work can be considered an extension of previous results which provided generalization performance bounds in terms of the VC-dimension of the class of hypotheses restricted to the sample, with the considerable advantage that the covering numbers can be readily computed, and they often are small.",Robert C Williamson and John Shawe-Taylor and Bernhard Schölkopf and Alexander J Smola,21,5904921757805417197,IEEE Transactions on Information Theory,,,,Sample based generalization bounds,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.8826&rep=rep1&type=pdf,,1999,/scholar?cites=5904921757805417197,DZ-fHPgAAAAJ:epqYDVWIO7EC
14440,"Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.",Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stefan Bauer and Hugo Larochelle and Bernhard Schölkopf and Michael C Mozer and Chris Pal and Yoshua Bengio,20,17856737708526146743,arXiv preprint arXiv:1910.01075,,,,Learning neural causal models from unknown interventions,https://arxiv.org/abs/1910.01075,,2019,/scholar?cites=17856737708526146743,DZ-fHPgAAAAJ:5PnCxSpcA8MC
14441,"We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.Subjects: Machine Learning (stat. ML); Machine Learning (cs. LG)Cite as: arXiv: 1802.03761 [stat. ML](or arXiv: 1802.03761 v1 [stat. ML] for this version)Submission historyFrom: Paul Rubenstein [view email][v1] Sun, 11 Feb 2018 16: 10: 41 UTC (991 KB)Full-text links:Download:",Paul K Rubenstein and Bernhard Schoelkopf and Ilya Tolstikhin,20,2908233974993835818,arXiv preprint arXiv:1802.03761,,,,On the latent space of wasserstein auto-encoders,https://arxiv.org/abs/1802.03761,,2018,/scholar?cites=2908233974993835818,DZ-fHPgAAAAJ:JNz6qP6hL3UC
14442,"Discovering causal structure of a dynamical system from observed time series is a traditional and important problem. In many practical applications, observed data are obtained by applying subsampling or temporally aggregation to the original causal processes, making it difficult to discover the underlying causal relations. Subsampling refers to the procedure that for every k consecutive observations, one is kept, the rest being skipped, and recently some advances have been made in causal discovery from such data. With temporal aggregation, the local averages or sums of k consecutive, non-overlapping observations in the causal process are computed as new observations, and causal discovery from such data is even harder. In this paper, we investigate how to recover causal relations at the original causal frequency from temporally aggregated data when k is known. Assuming the time series at the causal …",Mingming Gong and Kun Zhang and Bernhard Schölkopf and Clark Glymour and Dacheng Tao,20,17176311905710037686,Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence,,,NIH Public Access,Causal discovery from temporally aggregated time series,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5995575/,2017,2017,/scholar?cites=17176311905710037686,DZ-fHPgAAAAJ:XZpvIhjRgrsC
14443,"Most approaches to causal discovery assume a fixed (or time-invariant) causal model; however, in practical situations, especially in neuroscience and economics, causal relations might be timedependent for various reasons. This paper aims to identify the time-dependent causal relations from observational data. We consider general formulations for time-varying causal modeling on stochastic processes, which can also capture the causal influence from a certain type of unobserved confounders. We focus on two issues: one is whether such a causal model, including the causal direction, is identifiable from observational data; the other is how to estimate such a model in a principled way. We show that under appropriate assumptions, the causal structure is identifiable according to our formulated model. We then propose a principled way for its estimation by extending Gaussian Process regression, which enables an automatic way to learn how the causal model changes over time. Experimental results on both artificial and real data demonstrate the practical usefulness of time-dependent causal modeling and the effectiveness of the proposed approach for estimation.",Biwei Huang and Kun Zhang and Bernhard Schölkopf,20,10669337451633363752,,,,,Identification of time-dependent causal model: A gaussian process treatment,https://www.ijcai.org/Proceedings/15/Papers/501.pdf,,2015,/scholar?cites=10669337451633363752,DZ-fHPgAAAAJ:w8fV-8NuKYgC
14444,"Screws and gears are a source of periodically recurring nonlinear effects in mechanical dynamical systems. Unless the state sampling frequency is much higher than the periodic effect, model-free controllers cannot always compensate these effects, and good physical models for such periodic dynamics are challenging to construct. We investigate nonparametric system identification with an explicit focus on periodically recurring nonlinear effects. Within a Gaussian process regression framework, we design a locally periodic covariance function to shape the hypothesis space, which allows for a structured extrapolation that is not possible with more widely used covariance functions. These predictions are then used in model predictive control to construct a control signal correcting for the predicted external effect. We show that this approach is beneficial for state sampling times that are smaller than, but comparable to …",Edgar D Klenske and Melanie N Zeilinger and Bernhard Scholkopf and Philipp Hennig,20,142281866432880443,,,486-493,IEEE,Nonparametric dynamics estimation for time periodic systems,https://ieeexplore.ieee.org/abstract/document/6736564/,,2013,/scholar?cites=142281866432880443,DZ-fHPgAAAAJ:KIRwYnRZzWQC
14445,"Playing table tennis is a difficult task for robots, especially due to their limitations of acceleration. A key bottleneck is the amount of time needed to reach the desired hitting position and velocity of the racket for returning the incoming ball. Here, it often does not suffice to simply extrapolate the ball's trajectory after the opponent returns it but more information is needed. Humans are able to predict the ball's trajectory based on the opponent's moves and, thus, have a considerable advantage. Hence, we propose to incorporate an anticipation system into robot table tennis players, which enables the robot to react earlier while the opponent is performing the striking movement. Based on visual observation of the opponent's racket movement, the robot can predict the aim of the opponent and adjust its movement generation accordingly. The policies for deciding how and when to react are obtained by reinforcement learning …",Zhikun Wang and Christoph H Lampert and Katharina Mülling and Bernhard Schölkopf and Jan Peters,20,10108758715335930542,,,332-337,IEEE,Learning anticipation policies for robot table tennis,https://ieeexplore.ieee.org/abstract/document/6094892/,,2011,/scholar?cites=10108758715335930542,DZ-fHPgAAAAJ:UuEBAcK4md4C
14446,"A group of features that has been identified as “significant” in being able to separate data into classes is evaluated using a support vector machine which separates the dataset into classes one feature at a time. After separation, an extremal margin value is assigned to each feature based on the distance between the lowest feature value in the first class and the highest feature value in the second class. Separately, extremal margin values are calculated for a normal distribution within a large number of randomly drawn example sets for the two classes to determine the number of examples within the normal distribution that would have a specified extremal margin value. Using p-values calculated for the normal distribution, a desired p-value is selected. The specified extremal margin value corresponding to the selected p-value is compared to the calculated extremal margin values for the group of features. The features …",,20,17730824481033580583,,,,,Method for feature selection and for evaluating features identified as significant for classifying data,https://patents.google.com/patent/US7970718B2/en,,2011,/scholar?cites=17730824481033580583,DZ-fHPgAAAAJ:Tyk-4Ss8FVUC
14447,"The annual Neural Information Processing Systems (NIPS) conference is the flagship meeting on neural computation. It draws a diverse group of attendees--physicists, neuroscientists, mathematicians, statisticians, and computer scientists. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning and control, emerging technologies, and applications. Only twenty-five percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains the papers presented at the December 2005 meeting, held in Vancouver.",Yair Weiss and Bernhard Schölkopf and John Platt,20,12668510261551645815,,,,MIT Press,Advances in Neural Information Processing Systems 18: Proceedings of the 2005 Conference,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790998,,2006,/scholar?cites=12668510261551645815,DZ-fHPgAAAAJ:tBlTYpvFGQIC
14448,"Establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difficult to capture in an a priori criterion. While previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. By optimizing this criterion, we are then able to compute correspondence and morphs for novel heads.",Florian Steinke and Volker Blanz and Bernhard Schölkopf,20,18358146740809033533,Advances in Neural Information Processing Systems,,1313-1320,,Learning dense 3D correspondence,https://proceedings.neurips.cc/paper/2006/file/e22cb9d6bbb4c290a94e4fff4d68a831-Paper.pdf,19,2006,/scholar?cites=18358146740809033533,DZ-fHPgAAAAJ:YsrPvlHIBpEC
14449,"We study a model where one target variable  is correlated with a vector  of predictor variables being potential causes of . We describe a method that infers to what extent the statistical dependences between  and  are due to the influence of  on  and to what extent due to a hidden common cause (confounder) of  and . The method relies on concentration of measure results for large dimensions  and an independence assumption stating that, in the absence of confounding, the vector of regression coefficients describing the influence of each  on  typically has ‘generic orientation’ relative to the eigenspaces of the covariance matrix of . For the special case of a scalar confounder we show that confounding typically spoils this generic orientation in a characteristic way that can be used to quantitatively estimate the amount of confounding (subject to our idealized model assumptions).",Dominik Janzing and Bernhard Schölkopf,19,2567224768372135073,Journal of Causal Inference,1,,De Gruyter,Detecting confounding in multivariate linear models via spectral analysis,https://www.degruyter.com/view/journals/jci/6/1/article-20170013.xml,6,2018,/scholar?cites=2567224768372135073,DZ-fHPgAAAAJ:bZOjAe2D9b4C
14450,"The analysis of n-ary relations receives attention in many different fields, for instance biology, web mining, and social studies. In the basic setting, there are n sets of instances, and each observation associates n instances, one from each set. A common approach to explore these n-way data is the search for n-set patterns, the n-way equivalent of itemsets. More precisely, an n-set pattern consists of specific subsets of the n instance sets such that all possible associations between the corresponding instances are observed in the data. In contrast, traditional itemset mining approaches consider only two-way data, namely items versus transactions. The n-set patterns provide a higher-level view of the data, revealing associative relationships between groups of instances. Here, we generalize this approach in two respects. First, we tolerate missing observations to a certain degree, that means we are also …",Elisabeth Georgii and Koji Tsuda and Bernhard Schölkopf,19,516315450378273000,Machine Learning,2,123-155,Springer US,Multi-way set enumeration in weight tensors,https://link.springer.com/content/pdf/10.1007/s10994-010-5210-y.pdf,82,2011,/scholar?cites=516315450378273000,DZ-fHPgAAAAJ:vRqMK49ujn8C
14451,"This report summarizes the theory and some main applications of a new non-monotonic algorithm for maximizing a Poisson Likelihood, which for Positron Emission Tomography (PET) is equivalent to minimizing the associated Kullback-Leibler Divergence, and for Transmission Tomography is similar to maximizing the dual of a maximum entropy problem. We call our method non-monotonic maximum likelihood (NMML) and show its application to different problems such as tomography and image restoration. We discuss some theoretical properties such as convergence for our algorithm. Our experimental results indicate that speedups obtained via our non-monotonic methods are substantial.",Suvrit Sra and Dongmin Kim and Bernhard Schölkopf,19,8613165098771994322,,,,Max Planck Institute for Biological Cybernetics,Non-monotonic poisson likelihood maximization,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1789886,,2008,/scholar?cites=8613165098771994322,DZ-fHPgAAAAJ:4hFrxpcac9AC
14452,,Arthur Gretton and KM Borgwardt and Malte Rasch and Bernhard Schölkopf and Alex J Smola,19,8252070277830522429,,,513-520,MIT Press,Advances in neural information processing systems,http://scholar.google.com/scholar?cluster=8252070277830522429&hl=en&oi=scholarr,19,2007,/scholar?cites=8252070277830522429,DZ-fHPgAAAAJ:_5wVgMG9DcwC
14453,"Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over 1 million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.",Muhammad Waleed Gondal and Manuel Wuthrich and Djordje Miladinovic and Francesco Locatello and Martin Breidt and Valentin Volchkov and Joel Akpo and Olivier Bachem and Bernhard Schölkopf and Stefan Bauer,18,17192262335599006209,,,15740-15751,,On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset,http://papers.nips.cc/paper/9704-on-the-transfer-of-inductive-bias-from-simulation-to-the-real-world-a-new-disentanglement-dataset,,2019,/scholar?cites=17192262335599006209,DZ-fHPgAAAAJ:GPPi9ZHr0VQC
14454,We propose a kernel method to identify finite mixtures of nonparametric product distributions. It is based on a Hilbert space embedding of the joint distribution. The rank of the constructed tensor is equal to the number of mixture components. We present an algorithm to recover the components by partitioning the data points into clusters such that the variables are jointly conditionally independent given the cluster. This method can be used to identify finite confounders.,Eleni Sgouritsa and Dominik Janzing and Jonas Peters and Bernhard Schölkopf,18,11585401132558946328,arXiv preprint arXiv:1309.6860,,,,Identifying finite mixtures of nonparametric product distributions and causal inference of confounders,https://arxiv.org/abs/1309.6860,,2013,/scholar?cites=11585401132558946328,DZ-fHPgAAAAJ:hefNtdE4IMkC
14455,"We shed light on the discrimination between patterns belonging to two different classes by casting this decoding problem into a generalized prototype framework. The discrimination process is then separated into two stages: a projection stage that reduces the dimensionality of the data by projecting it on a line and a threshold stage where the distributions of the projected patterns of both classes are separated. For this, we extend the popular mean-of-class prototype classification using algorithms from machine learning that satisfy a set of invariance properties. We report a simple yet general approach to express different types of linear classification algorithms in an identical and easy-to-visualize formal framework using generalized prototypes where these prototypes are used to express the normal vector and offset of the hyperplane. We investigate non-margin classifiers such as the classical prototype classifier, the …",Arnulf BA Graf and Olivier Bousquet and Gunnar Rätsch and Bernhard Schölkopf,18,16377026703266167738,Neural computation,1,272-300,MIT Press,Prototype classification: Insights from machine learning,https://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.01-07-443,21,2009,/scholar?cites=16377026703266167738,DZ-fHPgAAAAJ:JQOojiI6XY0C
14456,"Electrophysiological signals of the developing fetal brain and heart can be investigated by fetal magnetoencephalography (fMEG). During such investigations, the fetal heart activity and that of the mother should be monitored continuously to provide an important indication of current well-being. Due to physical constraints of an fMEG system, it is not possible to use clinically established heart monitors for this purpose. Considering this constraint, we developed a real-time heart monitoring system for biomagnetic measurements and showed its reliability and applicability in research and for clinical examinations. The developed system consists of real-time access to fMEG data, an algorithm based on independent component analysis (ICA), and a graphical user interface (GUI). The algorithm extracts the current fetal and maternal heart signal from a noisy and artifact-contaminated data stream in real-time and is able to …",Stephan Waldert and Michael Bensch and Martin Bogdan and Wolfgang Rosenstiel and Bernhard Scholkopf and Curtis L Lowery and Hari Eswaran and Hubert Preissl,18,8724219814366741898,IEEE Transactions on Biomedical Engineering,10,1867-1874,IEEE,Real-time fetal heart monitoring in biomagnetic measurements using adaptive real-time ICA,https://ieeexplore.ieee.org/abstract/document/4303280/,54,2007,/scholar?cites=8724219814366741898,DZ-fHPgAAAAJ:raTqNPD5sRQC
14457,"To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.",Susanne Still and Bernhard Schölkopf and Klaus Hepp and Rodney J Douglas,18,6290657182828953583,,,741-747,,Four-legged walking gait control using a neuromorphic chip interfaced to a support vector learning algorithm,https://papers.nips.cc/paper/1824-four-legged-walking-gait-control-using-a-neuromorphic-chip-interfaced-to-a-support-vector-learning-algorithm.pdf,,2001,/scholar?cites=6290657182828953583,DZ-fHPgAAAAJ:-95Q15plzcUC
14458,"While implicit generative models such as GANs have shown impressive results in high quality image reconstruction and manipulation using a combination of various losses, we consider a simpler approach leading to surprisingly strong results. We show that texture loss [1] alone allows the generation of perceptually high quality images. We provide a better understanding of texture constraining mechanism and develop a novel semantically guided texture constraining method for further improvement. Using a recently developed perceptual metric employing “deep features” and termed LPIPS [2], the method obtains state-of-the-art results. Moreover, we show that a texture representation of those deep features better capture the perceptual quality of an image than the original deep features. Using texture information, off-the-shelf deep classification networks (without training) perform as well as the best …",Muhammad Waleed Gondal and Bernhard Schölkopf and Michael Hirsch,17,18035902138430865581,,,80-97,"Springer, Cham",The unreasonable effectiveness of texture transfer for single image super-resolution,https://link.springer.com/chapter/10.1007/978-3-030-11021-5_6,,2018,/scholar?cites=18035902138430865581,DZ-fHPgAAAAJ:fvf8D2X3ffoC
14459,"Brain-Computer Interfaces (BCI) that rely upon epidural electrocorticographic signals may become a promising tool for neurorehabilitation of patients with severe hemiparatic syndromes due to cerebrovascular, traumatic or tumor-related brain damage. Here, we show in a patient-based feasibility study that online classification of arm movement intention is possible. The intention to move or to rest can be identified with high accuracy (~90 %), which is sufficient for BCI-guided neurorehabilitation. The observed spatial distribution of relevant features on the motor cortex indicates that cortical reorganization has been induced by the brain lesion. Low- and high-frequency components of the electrocorticographic power spectrum provide complementary information towards classification of arm movement intention.",M Gomez-Rodriguez and M Grosse-Wentrup and J Peters and G Naros and J Hill and B Scholkopf and A Gharabaghi,17,5982313548893701246,,,36-39,IEEE,Epidural ECoG online decoding of arm movement intention in hemiparesis,https://ieeexplore.ieee.org/abstract/document/5581409/,,2010,/scholar?cites=5982313548893701246,DZ-fHPgAAAAJ:SGW5VrABaM0C
14460,"We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at this https URL.",Chaochao Lu and Bernhard Schölkopf and José Miguel Hernández-Lobato,16,9271066941467023390,arXiv preprint arXiv:1812.10576,,,,Deconfounding reinforcement learning in observational settings,https://arxiv.org/abs/1812.10576,,2018,/scholar?cites=9271066941467023390,DZ-fHPgAAAAJ:fo_iOo8pkCEC
14461,"A computer-implemented method for recovering a digital image (x) from a sequence of observed digital images (y 1,..., y T), includes: obtaining an observed digital image (y t); estimating a point spread function (f t) based on the observed image (y t); estimating the recovered digital image (x), based on the estimated point spread function (f t) and the observed image (y t); and repeating the above steps. In order to correct optical aberrations of a lens, a point spread function of the lens may be used.",,16,8761809652136533693,,,,,Method and device for recovering a digital image from a sequence of observed digital images,https://patents.google.com/patent/US10032254B2/en,,2018,/scholar?cites=8761809652136533693,DZ-fHPgAAAAJ:EsEWqaRxkBgC
14462,"We postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law. We discuss the implications of this principle and argue that they link thermodynamics and causal inference. On the one hand, they entail behavior that is similar to the usual arrow of time. On the other hand, they motivate a statistical asymmetry between cause and effect that has recently been postulated in the field of causal inference, namely, that the probability distribution  contains no information about the conditional distribution  and vice versa, while  may contain information about .",Dominik Janzing and Rafael Chaves and Bernhard Schölkopf,16,15761438675109413037,New Journal of Physics,9,093052,IOP Publishing,Algorithmic independence of initial condition and dynamical law in thermodynamics and causal inference,https://iopscience.iop.org/article/10.1088/1367-2630/18/9/093052/meta,18,2016,/scholar?cites=15761438675109413037,DZ-fHPgAAAAJ:zl-nTW-_jeEC
14463,"Objective. Patients in the completely locked-in state (CLIS), due to, for example, amyotrophic lateral sclerosis (ALS), no longer possess voluntary muscle control. Assessing attention and cognitive function in these patients during the course of the disease is a challenging but essential task for both nursing staff and physicians. Approach. An electrophysiological cognition test battery, including auditory and semantic stimuli, was applied in a late-stage ALS patient at four different time points during a six-month epidural electrocorticography (ECoG) recording period. Event-related cortical potentials (ERP), together with changes in the ECoG signal spectrum, were recorded via 128 channels that partially covered the left frontal, temporal and parietal cortex. Main results. Auditory but not semantic stimuli induced significant and reproducible ERP projecting to specific temporal and parietal cortical areas. N1/P2 responses …",Michael Bensch and Suzanne Martens and Sebastian Halder and Jeremy Hill and Femke Nijboer and Ander Ramos and Niels Birbaumer and Martin Bogdan and Boris Kotchoubey and Wolfgang Rosenstiel and Bernhard Schölkopf and Alireza Gharabaghi,16,8830776965358057130,Journal of neural engineering,2,026006,IOP Publishing,Assessing attention and cognitive function in completely locked-in state with event-related brain potentials and epidural electrocorticography,https://iopscience.iop.org/article/10.1088/1741-2560/11/2/026006/meta,11,2014,/scholar?cites=8830776965358057130,DZ-fHPgAAAAJ:I-2NeQpV75MC
14464,"This book honours the outstanding contributions of Vladimir Vapnik, a rare example of a scientist for whom the following statements hold true simultaneously: his work led to the inception of a new field of research, the theory of statistical learning and empirical inference; he has lived to see the field blossom; and he is still as active as ever. He started analyzing learning algorithms in the 1960s and he invented the first version of the generalized portrait algorithm. He later developed one of the most successful methods in machine learning, the support vector machine (SVM)–more than just an algorithm, this was a new approach to learning problems, pioneering the use of functional analysis and convex optimization in machine learning. Part I of this book contains three chapters describing and witnessing some of Vladimir Vapnik's contributions to science. In the first chapter, Léon Bottou discusses the seminal paper published in 1968 by Vapnik and Chervonenkis that lay the foundations of statistical learning theory, and the second chapter is an English-language translation of that original paper. In the third chapter, Alexey Chervonenkis presents a first-hand account of the early history of SVMs and valuable insights into the first steps in the development of the SVM in the framework of the generalised portrait method. The remaining chapters, by leading scientists in domains such as statistics, theoretical computer science, and mathematics, address substantial topics in the theory and practice of statistical learning theory, including SVMs and other kernel-based methods, boosting, PAC-Bayesian theory, online and transductive learning, loss functions …",Bernhard Schölkopf and Zhiyuan Luo and Vladimir Vovk,16,528233090426304493,,,,Springer Science & Business Media,Empirical inference: Festschrift in honor of Vladimir N. Vapnik,http://books.google.com/books?hl=en&lr=&id=3Hi8BAAAQBAJ&oi=fnd&pg=PR7&dq=info:7Xd3cTKpVAcJ:scholar.google.com&ots=Q387dHssDy&sig=8mWQa19Po7K9U-y2xgP6uFZIohw,,2013,/scholar?cites=528233090426304493,DZ-fHPgAAAAJ:k8to_Y4Q4_EC
14465,"We consider the problem of learning in the case where an underlying causal model can be inferred. Causal knowledge may facilitate some approaches for a given problem, and rule out others. We formulate the hypothesis that semi-supervised learning can help in an anti-causal setting, but not in a causal setting, and corroborate it with empirical results.",Bernhard Schölkopf and Dominik Janzing and Jonas Peters and Eleni Sgouritsa and Kun Zhang and Joris Mooij,16,17859746430977271826,,,129-141,"Springer, Berlin, Heidelberg",Semi-supervised learning in causal and anticausal settings,https://link.springer.com/chapter/10.1007/978-3-642-41136-6_13,,2013,/scholar?cites=17859746430977271826,DZ-fHPgAAAAJ:VjBpw8Hezy4C
14466,"We present a novel algorithm for the markerless tracking of deforming surfaces such as faces. We acquire a sequence of 3D scans along with color images at 40Hz. The data is then represented by implicit surface and color functions, using a novel partition-of-unity type method of efficiently combining local regressors using nearest neighbor searches. Both these functions act on the 4D space of 3D plus time, and use temporal information to handle the noise in individual scans. After interactive registration of a template mesh to the first frame, it is then automatically deformed to track the scanned surface, using the variation of both shape and color as features in a dynamic energy minimization problem. Our prototype system yields high-quality animated 3D models in correspondence, at a rate of approximately twenty seconds per timestep. Tracking results for faces and other objects are presented.",Christian Walder and Martin Breidt and Heinrich Bülthoff and Bernhard Schölkopf and Cristóbal Curio,16,15340846811870542483,,,41-50,"Springer, Berlin, Heidelberg",Markerless 3d face tracking,https://link.springer.com/chapter/10.1007/978-3-642-03798-6_5,,2009,/scholar?cites=15340846811870542483,DZ-fHPgAAAAJ:anf4URPfarAC
14467,"We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O(m 2 ) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist.",Bernhard Schölkopf and John Platt and Thomas Hofmann,16,4838344505876932174,,,513-520,MIT Press,A kernel method for the two-sample-problem,https://ieeexplore.ieee.org/abstract/document/6287330/,,2007,/scholar?cites=4838344505876932174,DZ-fHPgAAAAJ:4NAErq3LqyIC
14468,"This paper investigates architectures for generating local semantic labels within an image. The motivation for this problem is in content-based image retrieval. Once the spatial layout of a number of semantic features can be extracted, combinations of these can be used to formulate complex image retrieval queries based on the spatial semantics of the underlying images. Semantic query formulation leads to the representation of a much richer class of concepts than those of current retrieval systems, whose queries are based on the local outputs of low-level image features—for instance colour histograms or texture [1]. Finally note that, since the proposed method generates posterior probabilities, diverse semantic outputs can be combined in a principled manner. There are a number of papers that address the issue of determining the semantic content of images, all of which do so at a global scale (ie they result in one output per image). The papers most similar to the work presented here are those of Torralba et al.[13] and Vailaya et al.[14]. The former paper describes an algorithm that attempts to determine a set of real-valued ‘semantic axes’ in a particular feature space. They recognise the importance of being able to assign real-values to each image in relation to each semantic label, rather than the more common binary classification approach, but do not extend these real-values to a probabilistic representation. The latter paper, by Vailaya et al. describes a system that performs a hierarchical categorisation of images using a Bayesian framework which results in probabilistic labels for the images. All of the systems referenced above output only one …",Ben Bradshaw and Bernhard Schölkopf and J Platt,16,12973736235707490946,Tech. Report MSR-TR-2001-99,,,Microsoft Research,Kernel methods for extracting local image semantics,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2001-99.pdf,,2001,/scholar?cites=12973736235707490946,DZ-fHPgAAAAJ:foquWX3nUaYC
14469,"We lay theoretical foundations for new database release mechanisms that allow third-parties to construct consistent estimators of population statistics, while ensuring that the privacy of each individual contributing to the database is protected. The proposed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of the data generating random variable instead of the database itself still allows third-parties to construct consistent estimators of a wide class of population statistics. Second, the algorithm can satisfy the definition of differential privacy by basing the released kernel mean embedding on entirely synthetic data points, while controlling accuracy through the metric available in a Reproducing Kernel Hilbert Space. We describe two instantiations of the proposed framework, suitable under different scenarios, and prove theoretical results guaranteeing differential privacy of the resulting algorithms and the consistency of estimators constructed from their outputs.",Matej Balog and Ilya Tolstikhin and Bernhard Schölkopf,15,4917291882284156906,,,414-422,PMLR,Differentially private database release via kernel mean embeddings,http://proceedings.mlr.press/v80/balog18a.html,,2018,/scholar?cites=4917291882284156906,DZ-fHPgAAAAJ:y8WI7XcMcUUC
14470,"We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.",Alexander Neitz and Giambattista Parascandolo and Stefan Bauer and Bernhard Schölkopf,15,7596677518342590575,Advances in Neural Information Processing Systems,,9816-9826,,Adaptive skip intervals: Temporal abstraction for recurrent dynamical models,https://proceedings.neurips.cc/paper/2018/hash/0f0ee3310223fe38a989b2c818709393-Abstract.html,31,2018,/scholar?cites=7596677518342590575,DZ-fHPgAAAAJ:tCmJmvnmGv4C
14471,"Objective. Electroencephalographic (EEG) brain–computer interfaces (BCIs) hold promise in restoring communication for patients with completely locked-in stage amyotrophic lateral sclerosis (ALS). However, these patients cannot use existing EEG-based BCIs, arguably because such systems rely on brain processes that are impaired in the late stages of ALS. In this work, we introduce a novel BCI designed for patients in late stages of ALS based on high-level cognitive processes that are less likely to be affected by ALS. Approach. We trained two ALS patients via EEG-based neurofeedback to use self-regulation of theta or gamma oscillations in the precuneus for basic communication. Because there is a tight connection between the precuneus and consciousness, precuneus oscillations are arguably generated by high-level cognitive processes, which are less likely to be affected by ALS than processes linked to …",Tatiana Fomina and Gabriele Lohmann and Michael Erb and Thomas Ethofer and Bernhard Schölkopf and Moritz Grosse-Wentrup,15,9181237345373870739,Journal of Neural Engineering,6,066021,IOP Publishing,Self-regulation of brain rhythms in the precuneus: a novel BCI paradigm for patients with ALS,https://iopscience.iop.org/article/10.1088/1741-2560/13/6/066021/meta,13,2016,/scholar?cites=9181237345373870739,DZ-fHPgAAAAJ:3uvcc_vglMYC
14472,"Astronomical observations are affected by several kinds of noise, each with its own causal source; there is photon noise, stochastic source variability, and residuals coming from imperfect calibration of the detector or telescope. The precision of NASA Kepler photometry for exoplanet science—the most precise photometric measurements of stars ever made—appears to be limited by unknown or untracked variations in spacecraft pointing and temperature, and unmodeled stellar variability. Here, we present the causal pixel model (CPM) for Kepler data, a data-driven model intended to capture variability but preserve transit signals. The CPM works at the pixel level so that it can capture very fine-grained information about the variation of the spacecraft. The CPM models the systematic effects in the time series of a pixel using the pixels of many other stars and the assumption that any shared signal in these causally …",Dun Wang and David W Hogg and Daniel Foreman-Mackey and Bernhard Schölkopf,15,11856069122988403930,Publications of the Astronomical Society of the Pacific,967,094503,IOP Publishing,"A causal, data-driven approach to modeling the Kepler data",https://iopscience.iop.org/article/10.1088/1538-3873/128/967/094503/meta,128,2016,/scholar?cites=11856069122988403930,DZ-fHPgAAAAJ:I_iJP3g3-xYC
14473,"In this paper, an approach for learning optimal striking points is proposed. Based on a ball-flight model and a rebound model, a set of reachable striking points within the robot's workspace can be obtained. However, while these striking points are geometrically reachable, their success probability differs substantially due to the robot's nonlinear dynamics, the distance to the ball, the need to reach sufficient velocity as well as the right angle at interception and non-uniform sensitivity to errors. Thus, it is crucial for a ping-pong robotic system to select striking points well. As a successful ball interception is the result of various factors that cannot be modeled straightforwardly, we suggest determining optimal striking points based on a reward function that measures how well the ping-pong ball's trajectory and the racket's movement coincidence. In this approach, we propose to learn a stochastic policy over the reward given …",Yanlong Huang and Bernhard Schölkopf and Jan Peters,15,8192675279039400212,,,4587-4592,IEEE,Learning optimal striking points for a ping-pong playing robot,https://ieeexplore.ieee.org/abstract/document/7354030/,,2015,/scholar?cites=8192675279039400212,DZ-fHPgAAAAJ:xXQF3ZFlQEcC
14474,"In nonlinear latent variable models or dynamic models, if we consider the latent variables as confounders (common causes), the noise dependencies imply further relations between the observed variables. Such models are then closely related to causal discovery in the presence of nonlinear confounders, which is a challenging problem. However, generally in such models the observation noise is assumed to be independent across data dimensions, and consequently the noise dependencies are ignored. In this paper we focus on the Gaussian process latent variable model (GPLVM), from which we develop an extended model called invariant GPLVM (IGPLVM), which can adapt to arbitrary noise covariances. With the Gaussian process prior put on a particular transformation of the latent nonlinear functions, instead of the original ones, the algorithm for IGPLVM involves almost the same computational loads as that for the original GPLVM. Besides its potential application in causal discovery, IGPLVM has the advantage that its estimated latent nonlinear manifold is invariant to any nonsingular linear transformation of the data. Experimental results on both synthetic and realworld data show its encouraging performance in nonlinear manifold learning and causal discovery.",Kun Zhang and Bernhard Schölkopf and Dominik Janzing,15,13823776926212872396,arXiv preprint arXiv:1203.3534,,,,Invariant gaussian process latent variable models and application in causal discovery,https://arxiv.org/abs/1203.3534,,2012,/scholar?cites=13823776926212872396,DZ-fHPgAAAAJ:dTyEYWd-f8wC
14475,"Kernel learning algorithms are currently becoming a standard tool in the area of machine learning and pattern recognition. In this chapter we review the fundamental theory of kernel learning. As the basic building block we introduce the kernel function, which provides an elegant and general way to compare possibly very complex objects. We then review the concept of a reproducing kernel Hilbert space and state the representer theorem. Finally we give an overview of the most prominent algorithms, which are support vector classification and regression, Gaussian Processes and kernel principal analysis. With multiple kernel learning and structured output prediction we also introduce some more recent advancements in the field.",Peter V Gehler and Bernhard Schölkopf and G Camps-Valls and L Bruzzone,15,4166826397268194191,Kernel methods for remote sensing data analysis,,25-45,Wiley,An introduction to kernel learning algorithms,http://books.google.com/books?hl=en&lr=&id=_KhUMXQQkmQC&oi=fnd&pg=PA25&dq=info:jzPkEYiK0zkJ:scholar.google.com&ots=IZzVc7uELx&sig=XueYXyq-aF199UtS65uwDueCp58,,2009,/scholar?cites=4166826397268194191,DZ-fHPgAAAAJ:MXK_kJrjxJIC
14476,,A Gretton and AJ Smola and J Huang and M Schmittfull and KM Borgwardt and B Schölkopf,15,10472693011768010717,,,,MIT Press,Dataset Shift in Machine Learning,http://scholar.google.com/scholar?cluster=10472693011768010717&hl=en&oi=scholarr,,2009,/scholar?cites=10472693011768010717,DZ-fHPgAAAAJ:f2IySw72cVMC
14477,"The study of effective connectivity by means of neuroimaging depends on the measurement of similarity between activity patterns at different locations in the brain, without necessarily presupposing a particular model for this dependence. When these interactions are measured using functional magnetic resonance imaging (fMRI) techniques, however, imaging and physiological artifacts create patterns of dependence that may be unrelated to cortical activity. We demonstrate some of these effects through the measurement of short-range dependencies present in fMRI scans of the primary visual cortex (V1) in the anaesthetized macaque monkey. High-field (4.7 T) fMRI scans were conducted to measure responses based on the blood oxygen level-dependent contrast mechanism, during periods of no sensory stimulation and of visual stimulation with rotating polar-transformed checkerboard gratings. Dependence …",Arthur Gretton and Andrei Belitski and Yusuke Murayama and Bernhard Schölkopf and Nikos Logothetis,15,18010080271151304820,Magnetic resonance imaging,4,401-409,Elsevier,The effect of artifacts on dependence measurement in fMRI,https://www.sciencedirect.com/science/article/pii/S0730725X06000488,24,2006,/scholar?cites=18010080271151304820,DZ-fHPgAAAAJ:86PQX7AUzd4C
14478,"This chapter contains sections titled: Introduction, Background and Results, Top Level Description of the Boosting Strategy, Generation of Weak Learners, Overall Algorithm, Experiments, Conclusions",Alexander J Smola and Peter Bartlett and Bernhard Schölkopf and Dale Schuurmans,15,5183976175162582024,,,247-258,MIT Press,Towards a strategy for boosting regressors,https://ieeexplore.ieee.org/abstract/document/6274994/,,2000,/scholar?cites=5183976175162582024,DZ-fHPgAAAAJ:sTbExtt6UvsC
14479," Dieser Beitrag erläutert neue Ansätze und Ergebnisse der statistischen Lerntheorie. Nach einer Einleitung wird zunächst das Lernen aus Beispielen vorgestellt und erklärt, dass neben dem Erklären der Trainingdaten die Komplexität von Lernmaschinen wesentlich für den Lernerfolg ist. Weiterhin werden Kern-Algorithmen in Merkmalsräumen eingeführt, die eine elegante und effiziente Methode darstellen, verschiedene Lernmaschinen mit kontrollierbarer Komplexität durch Kernfunktionen zu realisieren. Beispiele für solche Algorithmen sind Support-Vektor-Maschinen (SVM), die Kernfunktionen zur Schätzung von Funktionen verwenden, oder Kern-PCA (principal component analysis), die Kernfunktionen zur Extraktion von nichtlinearen Merkmalen aus Datensätzen verwendet. Viel wichtiger als jedes einzelne Beispiel ist jedoch die Einsicht, dass jeder Algorithmus, der sich anhand von Skalarprodukten …",Bernhard Schölkopf and Klaus-Robert Müller and Alexander J Smola,15,18392566128815316230,Informatik Forschung und Entwicklung,3,154-163,Springer-Verlag,Lernen mit Kernen,https://link.springer.com/content/pdf/10.1007/s004500050135.pdf,14,1999,/scholar?cites=18392566128815316230,DZ-fHPgAAAAJ:XoXfffV-tXoC
14480,"Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-iid images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.",Francesco Locatello and Ben Poole and Gunnar Rätsch and Bernhard Schölkopf and Olivier Bachem and Michael Tschannen,14,17730117604231114120,arXiv preprint arXiv:2002.02886,,,,Weakly-Supervised Disentanglement Without Compromises,https://arxiv.org/abs/2002.02886,,2020,/scholar?cites=17730117604231114120,DZ-fHPgAAAAJ:IjboPtdrXSsC
14481,"The ability to learn and act in novel situations is still a prerogative of animate intelligence, as current machine learning methods mostly fail when moving beyond the standard iid setting. What is the reason for this discrepancy? Most machine learning tasks are anti-causal, ie, we infer causes (labels) from effects (observations). Typically, in supervised learning we build systems that try to directly invert causal mechanisms. Instead, in this paper we argue that strong generalization capabilities crucially hinge on searching and validating meaningful hypotheses, requiring access to a causal model. In such a framework, we want to find a cause that leads to the observed effect. Anti-causal models are used to drive this search, but a causal model is required for validation. We investigate the fundamental differences between causal and anti-causal tasks, discuss implications for topics ranging from adversarial attacks to disentangling factors of variation, and provide extensive evidence from the literature to substantiate our view. We advocate for incorporating causal models in supervised learning to shift the paradigm from inference only, to search and validation.",Niki Kilbertus and Giambattista Parascandolo and Bernhard Schölkopf,14,4143095364174076528,arXiv preprint arXiv:1812.00524,,,,Generalization in anti-causal learning,https://arxiv.org/abs/1812.00524,,2018,/scholar?cites=4143095364174076528,DZ-fHPgAAAAJ:RpHT6yXJGWcC
14482,"Discovery of causal relationships from observational data is a fundamental problem. Roughly speaking, there are two types of methods for causal discovery, constraint-based ones and score-based ones. Score-based methods avoid the multiple testing problem and enjoy certain advantages compared to constraint-based ones. However, most of them need strong assumptions on the functional forms of causal mechanisms, as well as on data distributions, which limit their applicability. In practice the precise information of the underlying model class is usually unknown. If the above assumptions are violated, both spurious and missing edges may result. In this paper, we introduce generalized score functions for causal discovery based on the characterization of general (conditional) independence relationships between random variables, without assuming particular model classes. In particular, we exploit regression in …",Biwei Huang and Kun Zhang and Yizhu Lin and Bernhard Schölkopf and Clark Glymour,14,7526242449026068932,,,1551-1560,,Generalized score functions for causal discovery,https://dl.acm.org/doi/abs/10.1145/3219819.3220104,,2018,/scholar?cites=7526242449026068932,DZ-fHPgAAAAJ:BLjlhAx4sdcC
14483,"Two popular examples of first-order optimization methods over linear spaces are coordinate descent and matching pursuit algorithms, with their randomized variants. While the former targets the optimization by moving along coordinates, the latter considers a generalized notion of directions. Exploiting the connection between the two algorithms, we present a unified analysis of both, providing affine invariant sublinear  rates on smooth objectives and linear convergence on strongly convex objectives. As a byproduct of our affine invariant analysis of matching pursuit, our rates for steepest coordinate descent are the tightest known. Furthermore, we show the first accelerated convergence rate  for matching pursuit and steepest coordinate descent on convex objectives.",Francesco Locatello and Anant Raj and Sai Praneeth Karimireddy and Gunnar Rätsch and Bernhard Schölkopf and Sebastian U Stich and Martin Jaggi,14,14447450939980198854,arXiv preprint arXiv:1803.09539,,,,On matching pursuit and coordinate descent,https://arxiv.org/abs/1803.09539,,2018,/scholar?cites=14447450939980198854,DZ-fHPgAAAAJ:--QqW9QaJxsC
14484,"Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.",Wittawat Jitkrittum and Heishiro Kanagawa and Patsorn Sangkloy and James Hays and Bernhard Schölkopf and Arthur Gretton,14,962836959160034441,Advances in Neural Information Processing Systems,,808-819,,Informative features for model comparison,https://proceedings.neurips.cc/paper/2018/hash/550a141f12de6341fba65b0ad0433500-Abstract.html,31,2018,/scholar?cites=962836959160034441,DZ-fHPgAAAAJ:VxhEzruSBC0C
14485,"We aimed to precisely estimate intra-tumoral heterogeneity using spatially regularized spectral clustering (SRSC) on multiparametric MRI data and compare the efficacy of SRSC with the previously reported segmentation techniques in MRI studies.Six NMRI nu/nu mice bearing subcutaneous human glioblastoma U87 MG tumors were scanned using a dedicated small animal 7T magnetic resonance imaging (MRI) scanner. The data consisted of T2 weighted images, apparent diffusion coefficient maps, and pre- and post-contrast T2 and T2* maps. Following each scan, the tumors were excised into 2–3-mm thin slices parallel to the axial field of view and processed for histological staining. The MRI data were segmented using SRSC, K-means, fuzzy C-means, and Gaussian mixture modeling to estimate the fractional population of …",Prateek Katiyar and Mathew R Divine and Ursula Kohlhofer and Leticia Quintanilla-Martinez and Bernhard Schölkopf and Bernd J Pichler and Jonathan A Disselhorst,14,8388147341160308541,Molecular imaging and biology,3,391-397,Springer US,A novel unsupervised segmentation approach quantifies tumor tissue populations using multiparametric MRI: first results with histological validation,https://link.springer.com/content/pdf/10.1007/s11307-016-1009-y.pdf,19,2017,/scholar?cites=8388147341160308541,DZ-fHPgAAAAJ:Dp6ZCYcE31YC
14486,"Diffusion MRI (dMRI) provides rich information on the white matter of the human brain, enabling insight into neurological disease, normal aging, and neuroplasticity. We present BundleMAP, an approach to extracting features from dMRI data that can be used for supervised classification, regression, and hypothesis testing. Our features are based on aggregating measurements along nerve fiber bundles, enabling visualization and anatomical interpretation. The main idea behind BundleMAP is to use the ISOMAP manifold learning technique to jointly parametrize nerve fiber bundles. We combine this idea with mechanisms for outlier removal and feature selection to obtain a practical machine learning pipeline. We demonstrate that it increases accuracy of disease detection and estimation of disease activity, and that it improves the power of statistical tests.",Mohammad Khatami and Tobias Schmidt-Wilcke and Pia C Sundgren and Amin Abbasloo and Bernhard Schölkopf and Thomas Schultz,14,5798832242464284128,Pattern Recognition,,593-600,Pergamon,"BundleMAP: Anatomically localized classification, regression, and hypothesis testing in diffusion MRI",https://www.sciencedirect.com/science/article/pii/S0031320316302849,63,2017,/scholar?cites=5798832242464284128,DZ-fHPgAAAAJ:Y2FWPFNJqs4C
14487,"We propose a new framework for deriving screening rules for convex optimization problems. Our approach covers a large class of constrained and penalized optimization formulations, and works in two steps. First, given any approximate point, the structure of the objective function and the duality gap is used to gather information on the optimal solution. In the second step, this information is used to produce screening rules, ie safely identifying unimportant weight variables of the optimal solution. Our general framework leads to a large variety of useful existing as well as new screening rules for many applications. For example, we provide new screening rules for general simplex and -constrained problems, Elastic Net, squared-loss Support Vector Machines, minimum enclosing ball, as well as structured norm regularized problems, such as group lasso.",Anant Raj and Jakob Olbrich and Bernd Gärtner and Bernhard Schölkopf and Martin Jaggi,14,16073334560174941803,arXiv preprint arXiv:1609.07478,,,,Screening rules for convex problems,https://arxiv.org/abs/1609.07478,,2016,/scholar?cites=16073334560174941803,DZ-fHPgAAAAJ:cdudMjVSFfUC
14488,"Kernel mean embeddings have recently attracted the attention of the machine learning community. They map measures  from some set  to functions in a reproducing kernel Hilbert space (RKHS) with kernel . The RKHS distance of two mapped measures is a semi-metric  over . We study three questions.(I) For a given kernel, what sets  can be embedded?(II) When is the embedding injective over (in which case  is a metric)?(III) How does the -induced topology compare to other topologies on ? The existing machine learning literature has addressed these questions in cases where  is (a subset of) the finite regular Borel measures. We unify, improve and generalise those results. Our approach naturally leads to continuous and possibly even injective embeddings of (Schwartz-) distributions, ie, generalised measures, but the reader is free to focus on measures only. In particular, we systemise and extend various (partly known) equivalences between different notions of universal, characteristic and strictly positive definite kernels, and show that on an underlying locally compact Hausdorff space,  metrises the weak convergence of probability measures if and only if  is continuous and characteristic.",Carl-Johann Simon-Gabriel and Bernhard Schölkopf,14,12168648573535465292,arXiv preprint arXiv:1604.05251,,,,"Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions",https://arxiv.org/abs/1604.05251,,2016,/scholar?cites=12168648573535465292,DZ-fHPgAAAAJ:UpNrVCuNAQgC
14489,"Locked-in syndrome (LIS) as a result of brainstem lesions or progressive neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS), is a severe medical condition in which a person is fully conscious but unable to move or talk. LIS can transition into complete locked-in syndrome (CLIS) in which residual abilities to communicate through muscle twitches are entirely lost. It is unknown how CLIS affects circadian rhythm and sleep/wake patterns. Here we report a 39-year-old ALS patient who transitioned from LIS to CLIS while brain activity was continuously recorded using electrocorticography (ECoG) over one month. While we found no circadian rhythm in heart rate and body temperature, transition into CLIS was associated with increased fragmentation of slow wave sleep (SWS) across the day. Total time in SWS did not change. SWS fragmentation might reflect progressive circadian system impairment …",Surjo R Soekadar and Jan Born and Niels Birbaumer and Michael Bensch and Sebastian Halder and Ander Ramos Murguialday and Alireza Gharabaghi and Femke Nijboer and Bernhard Schölkopf and Suzanne Martens,14,80111658787754023,Journal of clinical sleep medicine,9,951-953,American Academy of Sleep Medicine,Fragmentation of slow wave sleep after onset of complete locked-in state,https://jcsm.aasm.org/doi/abs/10.5664/jcsm.3002,9,2013,/scholar?cites=80111658787754023,DZ-fHPgAAAAJ:pUxgyZctzPYC
14490,"Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these interactions when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal we analyze the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using iid assumptions, showing improvement in terms of detection errors as well as the suitability of this approach for testing dependency in complex dynamical systems. Finally, we use this approach to characterize complex interactions in electrophysiological neural time series.",Michel Besserve and Nikos K Logothetis and Bernhard Schölkopf,14,11266196394865326395,Advances in Neural Information Processing Systems,,2535-2543,,Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.,https://proceedings.neurips.cc/paper/2013/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html,26,2013,/scholar?cites=11266196394865326395,DZ-fHPgAAAAJ:nU66GSXDKhoC
14491,"For digital photographs of astronomical objects, where exposure times are usually long and ISO settings high, the so-called dark-current is a significant source of noise. Dark-current refers to thermally generated electrons and is therefore present even in the absence of light. This paper presents a novel approach for denoising astronomical images that have been corrupted by dark-current noise. Our method relies on a probabilistic description of the dark-current of each pixel of a given camera. The noise model is then combined with an image prior which is adapted to astronomical images. In a laboratory environment, we use a black and white CCD camera containing a cooling unit and show that our method is superior to existing methods in terms of root mean squared error. Furthermore, we show that our method is practically relevant by providing visually more appealing results on astronomical photographs taken …",Harold Christopher Burger and Bernhard Schölkopf and Stefan Harmeling,14,10400088801409986672,,,1-8,IEEE,Removing noise from astronomical images using a pixel-specific noise model,https://ieeexplore.ieee.org/abstract/document/5753128/,,2011,/scholar?cites=10400088801409986672,DZ-fHPgAAAAJ:LXmCCkuhhTsC
14492,"We present a graphical model framework for decoding in the visual ERP-based speller system. The proposed framework allows researchers to build generative models from which the decoding rules are obtained in a straightforward manner. We suggest two models for generating brain signals conditioned on the stimulus events. Both models incorporate letter frequency information but assume different dependencies between brain signals and stimulus events. For both models, we derive decoding rules and perform a discriminative training. We show on real visual speller data how decoding performance improves by incorporating letter frequency information and using a more realistic graphical model for the dependencies between the brain signals and the stimulus events. Furthermore, we discuss how the standard approach to decoding can be seen as a special case of the graphical model framework. The letter …",SMM Martens and Joris M Mooij and N Jeremy Hill and Jason Farquhar and Bernhard Schölkopf,14,3541415065365560982,Neural computation,1,160-182,MIT Press,A graphical model framework for decoding in the visual ERP-based BCI speller,https://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00066,23,2011,/scholar?cites=3541415065365560982,DZ-fHPgAAAAJ:olpn-zPbct0C
14493,"An overview of different approaches to brain-computer interfaces (BCIs) developed in our laboratory is given. An important clinical application of BCIs is to enable communication or environmental control in severely paralyzed patients. The BCI “Thought-Translation Device (TTD)” allows verbal communication through the voluntary self-regulation of brain signals (eg, slow cortical potentials (SCPs)), which is achieved by operant feedback training. Humans’ ability to self-regulate their SCPs is used to move a cursor toward a target thatcontains a selectable letter set. Two differentapproaches were followed todevelop Web browsers that could be controlled with binary brain responses. Implementing more powerful classiﬁcation methods including different signal parameters such as oscillatory features improved our BCI considerably. It was also tested on signals with implanted electrodes. Most BCIs provide the user with a visual feedback interface. Visually impaired patients require an auditory feedback mode. A procedure using auditory (soniﬁed) feedback of multiple EEG parameters was evaluated. Properties of the auditory systems are reported and the results of two experiments with auditory feedback are presented. Clinical data of eightALS patients demonstrated that allpatients were able toacquire efﬁcient brain control of one of the three available BCI systems (SCP, μ-rhythm, and P300), most of them used the SCP-BCI. A controlled comparison of the three systems in a group of ALS patients, however, showed that P300-BCI and the μ-BCI are faster and more easily acquired than SCP-BCI, at least in patients with some rudimentary motor control left …",Thilo Hinterberger and Femke Nijboer and Andrea Kübler and Tamara Matuz and Adrian Furdea and Ursula Mochty and Miguel Jordan and Thomas Navin Lal and N Jeremy Hill and Jürgen Mellinger and Michael Bensch and Michael Tangermann and Guido Widman and Christian E Elger and Wolfgang Rosenstiel and Bernhard Schölkopf and Niels Birbaumer,14,5124564300788793086,Towards Brain-Computer Interfacing,,43-64,"MIT Press Cambridge, Massachussetts, London",Brain-computer interfaces for communication in paralysis: A clinical experimental approach,http://books.google.com/books?hl=en&lr=&id=V88swGX83ecC&oi=fnd&pg=PA43&dq=info:_v4pmRQcHkcJ:scholar.google.com&ots=mZWIuu7IbP&sig=sd_Z-aihTKYMmNO5V2Kpww71Ve4,,2007,/scholar?cites=5124564300788793086,DZ-fHPgAAAAJ:RYcK_YlVTxYC
14494,"This chapter contains sections titled: Introduction, A Framework for Structured/Interdependent Output Learning, A Maximum-Margin Formulation, Cutting-Plane Algorithm, Alternative Margin Formulations, Experiments, Conclusions, Proof of Proposition 37",Gökhan BakIr and Thomas Hofmann and Bernhard Schölkopf and Alexander J Smola and Ben Taskar and SVN Vishwanathan,14,10378759251723254287,,,85-103,MIT press,Support vector machine learning for interdependent and structured output spaces,https://ieeexplore.ieee.org/abstract/document/6270217/,,2007,/scholar?cites=10378759251723254287,DZ-fHPgAAAAJ:l15IaQQ0f8kC
14495,"Our goal for the competition was to evaluate the usefulness of simple machine learning techniques. We decided to use the Fisher criterion (see Chapter 2) as a feature selection method and Support Vector Machines (see Chapter 1) for the classification part. Here we explain how we chose the regularization parameter C of the SVM, how we determined the kernel parameter σ and how we estimated the number of features used for each data set. All analyzes were carried out on the training sets of the competition data. We choose the data set Arcene as an example to explain the approach step by step. In our view the point of this competition was the construction of a well performing classifier rather than the systematic analysis of a specific approach. This is why our search for the best classifier was only guided by the described methods and that we deviated from the road map at several occasions. All calculations were …",Thomas Lal and Olivier Chapelle and Bernhard Schölkopf,14,15363517474180581900,Feature Extraction,,439-445,Springer Berlin/Heidelberg,Combining a filter method with SVMs,https://link.springer.com/chapter/10.1007/978-3-540-35488-8_21,,2006,/scholar?cites=15363517474180581900,DZ-fHPgAAAAJ:IRz6iEL74y4C
14496,"In this paper we present a primal-dual decomposition algorithm for support vector machine training. As with existing methods that use very small working sets (such as Sequential Minimal Optimization (SMO), Successive Over-Relaxation (SOR) or the Kernel Adatron (KA)), our method scales well, is straightforward to implement, and does not require an external QP solver. Unlike SMO, SOR and KA, the method is applicable to a large number of SVM formulations regardless of the number of equality constraints involved. The effectiveness of our algorithm is demonstrated on a more difficult SVM variant in this respect, namely semi-parametric support vector regression.",Wolf Kienzle and Bernhard Schölkopf,14,15307804821174002280,,,182-193,"Springer, Berlin, Heidelberg",Training support vector machines with multiple equality constraints,https://link.springer.com/chapter/10.1007/11564096_21,,2005,/scholar?cites=15307804821174002280,DZ-fHPgAAAAJ:GtLg2Ama23sC
14497,"Recently, several discriminative learning approaches have been proposed for effective image restoration, achieving convincing tradeoff between image quality and computational efficiency. However, these methods require separate training for each restoration task (e.g., denoising, deblurring, and demosaicing) and problem condition (e.g., noise level of input images). This makes it time-consuming and difficult to encompass all tasks and conditions during training. In this paper, we propose a discriminative transfer learning method that incorporates formal proximal optimization and discriminative learning for general image restoration. The method requires a single-pass discriminative training and allows for reuse across various problems and conditions while achieving an efficiency comparable to previous discriminative approaches. Furthermore, after being trained, our model can be easily transferred to new …",Lei Xiao and Felix Heide and Wolfgang Heidrich and Bernhard Schölkopf and Michael Hirsch,13,8468491238966008522,IEEE Transactions on Image Processing,8,4091-4104,IEEE,Discriminative transfer learning for general image restoration,https://ieeexplore.ieee.org/abstract/document/8352765/,27,2018,/scholar?cites=8468491238966008522,DZ-fHPgAAAAJ:o84qWK9PV5cC
14498,We apply Wasserstein auto-encoders (WAEs) to the problem of disentangled representation learning. We highlight the potential of WAEs with promising results on a benchmark disentanglement task.,Paul K Rubenstein and Bernhard Schölkopf and Ilya Tolstikhin,13,4630645081599241398,,,,,Learning disentangled representations with wasserstein auto-encoders,https://openreview.net/pdf?id=Hy79-UJPM,,2018,/scholar?cites=4630645081599241398,DZ-fHPgAAAAJ:yzDjLyU8EikC
14499,"We address two important issues in causal discovery from nonstationary or heterogeneous data, where parameters associated with a causal structure may change over time or across data sets. First, we investigate how to efficiently estimate the ""driving force"" of the nonstationarity of a causal mechanism. That is, given a causal mechanism that varies over time or across data sets and whose qualitative structure is known, we aim to extract from data a low-dimensional and interpretable representation of the main components of the changes. For this purpose we develop a novel kernel embedding of nonstationary conditional distributions that does not rely on sliding windows. Second, the embedding also leads to a measure of dependence between the changes of causal modules that can be used to determine the directions of many causal arrows. We demonstrate the power of our methods with experiments on both …",Biwei Huang and Kun Zhang and Jiji Zhang and Ruben Sanchez-Romero and Clark Glymour and Bernhard Schölkopf,13,16012548668759843747,,,913-918,IEEE,Behind distribution shift: Mining driving forces of changes and causal arrows,https://ieeexplore.ieee.org/abstract/document/8215577/,,2017,/scholar?cites=16012548668759843747,DZ-fHPgAAAAJ:KAhDssNlFq0C
14500,"We prove that a time series satisfying a (linear) multivariate autoregressive moving average (VARMA) model satisfies the same model assumption in the reversed time direction, too, if all innovations are normally distributed. This reversibility breaks down if the innovations are non-Gaussian. This means that under the assumption of a VARMA process with non-Gaussian noise, the arrow of time becomes detectable. Our work thereby provides a theoretic justification of an algorithm that has been used for inferring the direction of video snippets. We present a slightly modified practical algorithm that estimates the time direction for a given sample and prove its consistency. We further investigate how the performance of the algorithm depends on sample size, number of dimensions of the time series and the order of the process. An application to real world data from economics shows that considering multivariate processes instead of univariate processes can be beneficial for estimating the time direction. Our result extends earlier work on univariate time series. It relates to the concept of causal inference, where recent methods exploit non-Gaussianity of the error terms for causal structure learning.",Stefan Bauer and Bernhard Schölkopf and Jonas Peters,13,11590743133541880915,,,2043-2051,,The arrow of time in multivariate time series,http://www.jmlr.org/proceedings/papers/v48/bauer16.pdf,,2016,/scholar?cites=11590743133541880915,DZ-fHPgAAAAJ:xWdBTjN8KC0C
14501,"A system and process for creating an interactive digital image, which allows a viewer to interact with a displayed image so as to change it with regard to a desired effect, such as exposure, focus or color, among others. An interactive image includes representative images which depict a scene with some image parameter varying between them. The interactive image also includes an index image, whose pixels each identify the representative image that exhibits the desired effect related to the varied image parameter at a corresponding pixel location. For example, a pixel of the index image might identify the representative image having a correspondingly-located pixel that depicts a portion of the scene at the sharpest focus. One primary form of interaction involves selecting a pixel of a displayed image whereupon the representative image identified in the index image at a corresponding pixel location is displayed in …",,13,5983492147243697196,,,,,Interactive images,https://patents.google.com/patent/US7421115B2/en,,2008,/scholar?cites=5983492147243697196,DZ-fHPgAAAAJ:t4tNIAV9hF4C
14502,"The final properties of sophisticated products can be affected by many unapparent dependencies within the manufacturing process, and the products' integrity can often only be checked in a final measurement. Troubleshooting can therefore be very tedious if not impossible in large assembly lines. In this paper, we show that feature selection is an efficient tool for serial-grouped lines to reveal causes for irregularities in product attributes.",Tobias Pfingsten and Daniel JL Herrmann and Thomas Schnitzler and Andreas Feustel and Bernhard Scholkopf,13,3851973658528871425,IEEE transactions on automation science and engineering,3,465-469,IEEE,Feature selection for troubleshooting in complex assembly lines,https://ieeexplore.ieee.org/abstract/document/4266824/,4,2007,/scholar?cites=3851973658528871425,DZ-fHPgAAAAJ:kF1pexMAQbMC
14503,"We present a new approximation scheme for support vector decision functions in object detection. In the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller so-called reduced set of synthetic points. Instead of finding the reduced set via unconstrained optimization, we impose a structural constraint on the synthetic vectors such that the resulting approximation can be evaluated via separable filters. Applications that require scanning an entire image can benefit from this representation: when using separable filters, the average computational complexity for evaluating a reduced set vector on a test patch of size h× w drops from O(h· w) to O(h+w). We show experimental results on handwritten digits and face detection.",Wolf Kienzle and Gökhan Bakır and Matthias Franz and Bernhard Schölkopf,13,10134893882678490064,,,54-61,"Springer, Berlin, Heidelberg",Efficient approximations for support vector machines in object detection,https://link.springer.com/chapter/10.1007/978-3-540-28649-3_7,,2004,/scholar?cites=10134893882678490064,DZ-fHPgAAAAJ:bz8QjSJIRt4C
14504,"We introduce a learning technique for regression between high-dimensional spaces. Standard methods typically reduce this task to many one-dimensional problems, with each output dimension considered independently. By contrast, in our approach the feature construction and the regression estimation are performed jointly, directly minimizing a loss function that we specify, subject to a rank constraint. A major advantage of this approach is that the loss is no longer chosen according to the algorithmic requirements, but can be tailored to the characteristics of the task at hand; the features will then be optimal with respect to this objective, and dependence between the outputs can be exploited.",Gökhan Bakır and Arthur Gretton and Matthias Franz and Bernhard Schölkopf,13,4597483648834348951,Pattern Recognition,,262-269,Springer Berlin/Heidelberg,Multivariate regression via stiefel manifold constraints,https://link.springer.com/chapter/10.1007/978-3-540-28649-3_32,,2004,/scholar?cites=4597483648834348951,DZ-fHPgAAAAJ:HE397vMXCloC
14505,"Author: Weston, J et al.; Genre: Report; Published in Print: 2001; Title: Use
of the $ell_0$-norm with linear models and kernel methods.
",J Weston and A Elisseeff and B Schölkopf,13,5134826431702904680,,,,Biowulf Technologies,Use of the -norm with linear models and kernel methods,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1793352,,2001,/scholar?cites=5134826431702904680,DZ-fHPgAAAAJ:d6JCS5z0ckYC
14506,,G Ratsch and B Scholkopf and A Smola and KR Miiller and T Onoda and S Mika,13,6523262319981421349,Advances in Neural Information Processing Systems,,,,nu-Arc - ensemble learning in the presence of outliers,,12,2000,/scholar?cites=6523262319981421349,DZ-fHPgAAAAJ:mWEH9CqjF64C
14507,"Suppose you are given some dataset drawn from an underlying probability dis-tributionPand you want to estimate a subsetSof input space such that theprobability that a test point drawn fromPlies outside ofSis bounded by somea priori specified 0< ν≤ 1. We propose an algorithm to deal with this problem by trying toestimate afunctionfwhich is positive onSand negative on the complement ofS. Thefunctional form offis given by a kernel expansion in terms of a potentially smallsubset of the training data; it is regularized by controlling the length of the weightvector in an associated feature space. We can prove thatνupper bounds the fraction of outliers (training points outsideofS) and lower bounds the fraction of support vectors. Asymptotically, undersome mild condition onP, both become equalities. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.",B Schölkopf and R Williamson and AJ Smola and J Shawe-Taylor,13,683342598508400672,,,19-20,"Schloss Dagstuhl, Leibniz-Zentrum für Informatik",Single-class support vector machines,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1793845,,1999,/scholar?cites=683342598508400672,DZ-fHPgAAAAJ:DwFgw5hZUzMC
14508,"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, ie, flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, ie the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. We also propose an additional loss function to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains competitive samples in terms of perceptual quality with improved temporal consistency.",Eduardo Pérez-Pellitero and Mehdi SM Sajjadi and Michael Hirsch and Bernhard Schölkopf,12,16099198363005232480,Workshop and Challenge on Perceptual Image Restoration and Manipulation (PIRM) at the 15th European Conference on Computer Vision (ECCV),,,,Photorealistic video super resolution,https://www.researchgate.net/profile/Mehdi_S_M_Sajjadi/publication/326550012_Photorealistic_Video_Super_Resolution/links/5ca4c00292851c8e64afba3a/Photorealistic-Video-Super-Resolution.pdf,,2018,/scholar?cites=16099198363005232480,DZ-fHPgAAAAJ:zjq2cL48SfsC
14509,"Brain–computer interfaces (BCIs) are often based on the control of sensorimotor processes, yet sensorimotor processes are impaired in patients suffering from amyotrophic lateral sclerosis (ALS). We devised a new paradigm that targets higher-level cognitive processes to transmit information from the user to the BCI. We instructed five ALS patients and twelve healthy subjects to either activate self-referential memories or to focus on a process without mnemonic content while recording a high-density electroencephalogram (EEG). Both tasks are designed to modulate activity in the default mode network (DMN) without involving sensorimotor pathways. We find that the two tasks can be distinguished after only one experimental session from the average of the combined bandpower modulations in the theta- (4–7 Hz) and alpha-range (8–13 Hz), with an average accuracy of 62.5% and 60.8% for healthy subjects and ALS …",Matthias R Hohmann and Tatiana Fomina and Vinay Jayaram and Natalie Widmann and Christian Förster and Jennifer Just and Matthis Synofzik and Bernhard Schölkopf and Ludger Schöls and Moritz Grosse-Wentrup,12,14122468179469876679,,,221-239,Elsevier,A cognitive brain–computer interface for patients with amyotrophic lateral sclerosis,https://www.sciencedirect.com/science/article/pii/S0079612316300486,228,2016,/scholar?cites=14122468179469876679,DZ-fHPgAAAAJ:UqWtYh3YZQwC
14510,"According to a recently stated 'independence postulate', the distribution Pcause contains no information about the conditional Peffect|cause while Peffect may contain information about Pcause|effect. Since semi-supervised learning (SSL) attempts to exploit information from PX to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f(X) as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of PX and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the …",Dominik Janzing and Bernhard Schölkopf,12,16155044835041591592,The Journal of Machine Learning Research,1,1923-1948,JMLR. org,Semi-supervised interpolation in an anticausal learning scenario,https://dl.acm.org/doi/abs/10.5555/2789272.2912103,16,2015,/scholar?cites=16155044835041591592,DZ-fHPgAAAAJ:eThWPngL6FsC
14511,"The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (eg, when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (eg, kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing “better” estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.",Krikamol Muandet and Bharath Sriperumbudur and Bernhard Schölkopf,12,463268871781302766,,,1-9,,Kernel mean estimation via spectral filtering,http://papers.nips.cc/paper/5239-kernel-mean-estimation-via-spectral,,2014,/scholar?cites=463268871781302766,DZ-fHPgAAAAJ:4uoR24qA-WYC
14512,"We propose a method to infer causal structures containing both discrete and continuous variables. The idea is to select causal hypotheses for which the conditional density of every variable, given its causes, becomes smooth. We define a family of smooth densities and conditional densities by second order exponential models, ie, by maximizing conditional entropy subject to first and second statistical moments. If some of the variables take only values in proper subsets of R^ n, these conditionals can induce different families of joint distributions even for Markov-equivalent graphs.We consider the case of one binary and one real-valued variable where the method can distinguish between cause and effect. Using this example, we describe that sometimes a causal hypothesis must be rejected because P (effect| cause) and P (cause) share algorithmic information (which is untypical if they are chosen independently). This way, our method is in the same spirit as faithfulness-based causal inference because it also rejects non-generic mutual adjustments among DAG-parameters.",Dominik Janzing and Xiaohai Sun and Bernhard Schölkopf,12,11829899638643192893,arXiv preprint arXiv:0910.5561,,,,Distinguishing cause and effect via second order exponential models,https://arxiv.org/abs/0910.5561,,2009,/scholar?cites=11829899638643192893,DZ-fHPgAAAAJ:eq2jaN3J8jMC
14513,"We discuss reproducing kernel Hilbert space (RKHS)-based measures of statistical dependence, with emphasis on constrained covariance (COCO), a novel criterion to test dependence of random variables. We show that COCO is a test for independence if and only if the associated RKHSs are universal. That said, no independence test exists that can distinguish dependent and independent random variables in all circumstances. Dependent random variables can result in a COCO which is arbitrarily close to zero when the source densities are highly non-smooth, which can make dependence hard to detect empirically. All current kernel-based independence tests share this behaviour. Finally, we demonstrate exponential convergence between the population and empirical COCO, which implies that COCO does not suffer from slow learning rates when used as a dependence test.",Arthur Gretton and Alexander Smola and Olivier Bousquet and Ralf Herbrich and Bernhard Schölkopf and NK Logothetis,12,2981888544124479005,,,,Max Planck Institute for Biological Cybernetics,Behaviour and convergence of the constrained covariance,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792168,,2004,/scholar?cites=2981888544124479005,DZ-fHPgAAAAJ:_axFR9aDTf0C
14514,"The Wiener series is one of the standard methods to systematically characterize the nonlinearity of a system. The classical estimation method of the expansion coefficients via cross-correlation suffers from severe problems that prevents its application to high-dimensional and strongly nonlinear systems. We propose an implicit estimation method based on regression in a reproducing kernel Hubert space that alleviates these problems. Experiments show performance advantages in terms of convergence, interpretability, and system sizes that can be handled",Matthias O Franz and B Scholkopf,12,5329751070743710554,,,735-744,IEEE,Implicit estimation of Wiener series,https://ieeexplore.ieee.org/abstract/document/1423040/,,2004,/scholar?cites=5329751070743710554,DZ-fHPgAAAAJ:w1MjKQ0l0TYC
14515,"This article gives a short introduction to the main ideas of statistical learning theory, support vector machines, and kernel feature spaces.",Bernhard Schölkopf,12,1331896119852546928,,,3-17,Elsevier,An introduction to support vector machines,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792608,,2003,/scholar?cites=1331896119852546928,DZ-fHPgAAAAJ:EiA1hA_4fYQC
14516,"It is commonplace to encounter heterogeneous or nonstationary data, of which the underlying generating process changes across domains or over time. Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper, we develop a framework for causal discovery from such data, called Constraint-based causal Discovery from heterogeneous/NOnstationary Data (CD-NOD), to find causal skeleton and directions and estimate the properties of mechanism changes. First, we propose an enhanced constraintbased procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a method to determine causal orientations by making use of independent changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. After learning the causal structure, next, we investigate how to efficiently estimate the “driving force” of the nonstationarity of a causal mechanism. That is, we aim to extract from data a low-dimensional representation of changes. The proposed methods are nonparametric, with no hard restrictions on data distributions and causal mechanisms, and do not rely on window segmentation. Furthermore, we find that data heterogeneity benefits causal structure identification even with particular types of confounders. Finally, we show the connection between heterogeneity/nonstationarity and soft intervention in causal discovery. Experimental results on various synthetic and real-world data sets (task-fMRI and stock market data) are presented to …",Biwei Huang and Kun Zhang and Jiji Zhang and Joseph Ramsey and Ruben Sanchez-Romero and Clark Glymour and Bernhard Schölkopf,11,13186920956482038058,Journal of Machine Learning Research,89,1-53,Microtome Publishing,Causal discovery from heterogeneous/nonstationary data,https://www.jmlr.org/papers/volume21/19-232/19-232.pdf,21,2020,/scholar?cites=13186920956482038058,DZ-fHPgAAAAJ:8dO9Xi6wxqgC
14517,"The goal in extreme multi-label classification is to learn a classifier which can assign a small subset of relevant labels to an instance from an extremely large set of target labels. Datasets in extreme classification exhibit a long tail of labels which have small number of positive training instances. In this work, we pose the learning task in extreme classification with large number of tail-labels as learning in the presence of adversarial perturbations. This view motivates a robust optimization framework and equivalence to a corresponding regularized objective.Under the proposed robustness framework, we demonstrate efficacy of Hamming loss for tail-label detection in extreme classification. The equivalent regularized objective, in combination with proximal gradient based optimization, performs better than state-of-the-art methods on propensity scored versions of precision@ k and nDCG@ k (upto 20% relative improvement over PFastreXML-a leading tree-based approach and 60% relative improvement over SLEEC-a leading label-embedding approach). Furthermore, we also highlight the sub-optimality of a sparse solver in a widely used package for large-scale linear classification, which is interesting in its own right. We also investigate the spectral properties of label graphs for providing novel insights towards understanding the conditions governing the performance of Hamming loss based one-vs-rest scheme vis-à-vis label embedding methods.",Rohit Babbar and Bernhard Schölkopf,11,1327176333898802435,arXiv preprint arXiv:1803.01570,,,,Adversarial extreme multi-label classification,https://arxiv.org/abs/1803.01570,,2018,/scholar?cites=1327176333898802435,DZ-fHPgAAAAJ:KgZgvZP_DzwC
14518,"Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",Mehdi SM Sajjadi and Giambattista Parascandolo and Arash Mehrjou and Bernhard Schölkopf,11,14474674151316898300,arXiv preprint arXiv:1802.04374,,,,Tempered adversarial networks,https://arxiv.org/abs/1802.04374,,2018,/scholar?cites=14474674151316898300,DZ-fHPgAAAAJ:5eHNVgTYnwwC
14519,"The first detection of gravitational waves (GWs) from a binary black hole merger in 2015 was a milestone in modern physics, and just recently awarded with the Nobel Prize. However, despite the unparalleled sensitivity of the LIGO detectors, there still exist challenges in the analysis of the recorded data. We apply CONVWAVE, a dilated, fully convolutional neural net directly on the time series strain data to identify simulated GW signals from black hole mergers in real, non-Gaussian background measurements from the LIGO detectors. CONVWAVE performs well on simulated signals with masses and distances chosen from ranges that contain the estimated parameters of all previously detected real events. It efficiently runs on strain data of arbitrary length from any number of detectors in real time. Through our proposed evaluation approach, it has the potential to develop into a complementary trigger generator in the existing LIGO search pipeline.",Timothy Gebhard and Niki Kilbertus and Giambattista Parascandolo and Ian Harry and Bernhard Schölkopf,11,5027451599202611268,Workshop on Deep Learning for Physical Sciences (DLPS) at the 31st Conference on Neural Information Processing Systems (NIPS),,1-6,,Convwave: Searching for gravitational waves with fully convolutional neural nets,https://pdfs.semanticscholar.org/4033/ea2116bb5b6bc791cd0d55114fcd70f4b358.pdf,,2017,/scholar?cites=5027451599202611268,DZ-fHPgAAAAJ:sW3koLuhEv8C
14520,"Due to its strong requirements in motor abilities, robot table tennis is an important test bed for new robot learning approaches. Learning approaches have to generalize a complex hitting behavior from relatively few demonstrated trajectories, which neither cover all ball trajectories nor all desired hitting directions. Therefore, past approaches that only modeled a deterministic mean behavior without capturing the variability of the movement have been fairly limited. Recent work on capturing trajectory distributions using probabilistic movement representations opens important new possibilities for robot table tennis. In this paper, we present two new methods to adapt probabilistic movement primitives. First we present a method to adapt a probability distribution of hitting movements learned in joint space to have a desired end effector position, velocity and orientation. Subsequently, we present a method to find the initial …",Sebastian Gomez-Gonzalez and Gerhard Neumann and Bernhard Schölkopf and Jan Peters,11,18237696365625943602,,,502-508,IEEE,Using probabilistic movement primitives for striking movements,https://ieeexplore.ieee.org/abstract/document/7803322/,,2016,/scholar?cites=18237696365625943602,DZ-fHPgAAAAJ:KaR_JG5PXAAC
14521,"Structural Causal Models are widely used in causal modelling, but how they relate to other modelling tools is poorly understood. In this paper we provide a novel perspective on the relationship between Ordinary Differential Equations and Structural Causal Models. We show how, under certain conditions, the asymptotic behaviour of an Ordinary Differential Equation under non-constant interventions can be modelled using Dynamic Structural Causal Models. In contrast to earlier work, we study not only the effect of interventions on equilibrium states; rather, we model asymptotic behaviour that is dynamic under interventions that vary in time, and include as a special case the study of static equilibria.",Paul K Rubenstein and Stephan Bongers and Bernhard Schölkopf and Joris M Mooij,11,324121710188197777,arXiv preprint arXiv:1608.08028,,,,From deterministic ODEs to dynamic structural causal models,https://arxiv.org/abs/1608.08028,,2016,/scholar?cites=324121710188197777,DZ-fHPgAAAAJ:zOZn3-bHan0C
14522,"A system and process for creating an interactive digital image, which allows a viewer to interact with a displayed image so as to change it with regard to a desired effect, such as exposure, focus or color, among others. An interactive image includes representative images which depict a scene with some image parameter varying between them. The interactive image also includes an index image, whose pixels each identify the representative image that exhibits the desired effect related to the varied image parameter at a corresponding pixel location. For example, a pixel of the index image might identify the representative image having a correspondingly-located pixel that depicts a portion of the scene at the sharpest focus. One primary form of interaction involves selecting a pixel of a displayed image whereupon the representative image identified in the index image at a corresponding pixel location is displayed in …",,11,6061492451388606359,,,,,Interactive images,https://patents.google.com/patent/US7444016B2/en,,2008,/scholar?cites=6061492451388606359,DZ-fHPgAAAAJ:AvsVOL42qi4C
14523,The concept of Support Vector Regression is extended to a more general class of convex cost functions. It is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method. Both computational feasibility and improvement of estimation is demonstrated in the experiments.,Alex Smola and Bernhard Schölkopf and Klaus-Robert Müller,11,6836561972859116946,,,99-104,"Springer, London",Convex cost functions for support vector regression,https://link.springer.com/chapter/10.1007/978-1-4471-1599-1_10,,1998,/scholar?cites=6836561972859116946,DZ-fHPgAAAAJ:hCrLmN-GePgC
14524,"As machine learning is increasingly used to inform consequential decision-making (eg, pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations--"" how the world would have (had) to be different for a desirable outcome to occur""--aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, one of the main objectives of"" explanations as a means to help a data-subject act rather than merely understand"" has been overlooked. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, moving the focus from explanations to recommendations. Finally, we provide the reader with an extensive discussion on how to realistically achieve recourse beyond structural interventions.",Amir-Hossein Karimi and Bernhard Schölkopf and Isabel Valera,10,11883095561520158903,arXiv preprint arXiv:2002.06278,,,,Algorithmic Recourse: from Counterfactual Explanations to Interventions,https://arxiv.org/abs/2002.06278,,2020,/scholar?cites=11883095561520158903,DZ-fHPgAAAAJ:mlYb_ZI0coIC
14525,"Variational Auto-Encoders (VAEs) are capable of learning latent representations for high dimensional data. However, due to the iid assumption, VAEs only optimize the singleton variational distributions and fail to account for the correlations between data points, which might be crucial for learning latent representations from dataset where a priori we know correlations exist. We propose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent representations with VAEs. CVAEs apply a prior based on the correlation structure. To address the intractability introduced by the correlated prior, we develop an approximation by average of a set of tractable lower bounds over all maximal acyclic subgraphs of the undirected correlation graph. Experimental results on matching and link prediction on public benchmark rating datasets and spectral clustering on a synthetic dataset show the effectiveness of the proposed method over baseline algorithms.",Da Tang and Dawen Liang and Tony Jebara and Nicholas Ruozzi,10,14520356175099829641,arXiv preprint arXiv:1905.05335,,,,Correlated variational auto-encoders,https://arxiv.org/abs/1905.05335,,2019,/scholar?cites=14520356175099829641,DZ-fHPgAAAAJ:OLI3yu-XQ3gC
14526,"We consider linear models where  potential causes  are correlated with one target quantity  and propose a method to infer whether the association is causal or whether it is an artifact caused by overfitting or hidden common causes. We employ the idea that in the former case the vector of regression coefficients has' generic'orientation relative to the covariance matrix  of . Using an ICA based model for confounding, we show that both confounding and overfitting yield regression vectors that concentrate mainly in the space of low eigenvalues of .",Dominik Janzing and Bernhard Schölkopf,10,6321142934893531881,arXiv preprint arXiv:1803.00810,,,,Detecting non-causal artifacts in multivariate linear regression models,https://arxiv.org/abs/1803.00810,,2018,/scholar?cites=6321142934893531881,DZ-fHPgAAAAJ:nTpwJ0UNYGwC
14527,"The alpha peak frequency (APF) of the human electroencephalogram (EEG) is a reliable neurophysiological marker for cognitive abilities. In these case series, we document a shift of the APF towards the lower end of the EEG spectrum in two completely locked-in ALS patients. In not completely locked-in ALS patients, the alpha rhythm lies within the common frequency range. We discuss potential implications of this shift for the largely unknown cognitive state of completely locked-in ALS patients.",Matthias R Hohmann and Tatiana Fomina and Vinay Jayaram and Theresa Emde and Jennifer Just and Matthis Synofzik and Bernhard Schölkopf and Ludger Schöls and Moritz Grosse-Wentrup,10,3659599321424169426,Clinical Neurophysiology,2,406-408,Elsevier,Case series: Slowing alpha rhythm in late-stage ALS patients,https://www.sciencedirect.com/science/article/pii/S1388245717311604,129,2018,/scholar?cites=3659599321424169426,DZ-fHPgAAAAJ:1SHMjVdekzgC
14528,"We study the identifiability and estimation of functional causal models under selection bias, with a focus on the situation where the selection depends solely on the effect variable, which is known as outcome-dependent selection. We address two questions of identifiability: the identifiability of the causal direction between two variables in the presence of selection bias, and, given the causal direction, the identifiability of the model with outcomedependent selection. Regarding the first, we show that in the framework of post-nonlinear causal models, once outcome-dependent selection is properly modeled, the causal direction between two variables is generically identifiable; regarding the second, we identify some mild conditions under which an additive noise causal model with outcome-dependent selection is to a large extent identifiable. We also propose two methods for estimating an additive noise model from data that are generated with outcome-dependent selection.",Kun Zhang and Jiji Zhang and Biwei Huang and Bernhard Schölkopf and Clark Glymour,10,11378288486975691245,,,,,On the Identifiability and Estimation of Functional Causal Models in the Presence of Outcome-Dependent Selection.,http://auai.org/uai2016/proceedings/papers/305.pdf,,2016,/scholar?cites=11378288486975691245,DZ-fHPgAAAAJ:R6WN2b6jgFYC
14529,"The Default Mode Network (DMN) is a brain resting-state network that is closely linked to consciousness and neuropsychiatric disorders. The DMN is routinely identified with functional magnetic resonance imaging (fMRI) or positron emission tomography (PET). However, both of these methods impose restrictions on the groups of patients that can be examined. We show that the DMN can also be identified by electroencephalography (EEG). Instructing subjects to alternate between self-referential memory recall and focusing on their breathing induces a spatial pattern of spectral band power modulation in the θ- and α-band (4-16 Hz) that is consistent with the DMN pattern observed with PET and fMRI. Since EEG is a portable, cheap, and safe technology, our work enables the characterization of DMN alterations in patient groups that are difficult to study with fMRI or PET.",Tatiana Fomina and Matthias Hohmann and Bernhard Schölkopf and Moritz Grosse-Wentrup,10,7553336341313678835,,,7566-7569,IEEE,Identification of the default mode network with electroencephalography,https://ieeexplore.ieee.org/abstract/document/7320143/,,2015,/scholar?cites=7553336341313678835,DZ-fHPgAAAAJ:wi3gcRp_NToC
14530,"Assessing the causal effect of a treatment variable X on an outcome variable Y is usually difficult due to the existence of unobserved common causes. Without further assumptions, observed dependences do not even prove the existence of a causal effect from X to Y. It is intuitively clear that strong statistical dependences between X and Y do provide evidence for X influencing Y if the influence of common causes is known to be weak. We propose a framework that formalizes effect versus confounding in various ways and derive upper/lower bounds on the effect in terms of a priori given bounds on confounding. The formalization includes information theoretic quantities like information flow and causal strength, as well as other common notions like effect of treatment on the treated (ETT). We discuss several scenarios where upper bounds on the strength of confounding can be derived. This justifies to some extent human intuition which assumes the presence of causal effect when strong (eg close to deterministic) statistical relations are observed.",Philipp Geiger and Dominik Janzing and Bernhard Schölkopf,10,13869535671640143426,,,240-249,,Estimating Causal Effects by Bounding Confounding.,http://www.auai.org/uai2014/proceedings/individuals/303.pdf,,2014,/scholar?cites=13869535671640143426,DZ-fHPgAAAAJ:svGagg1hbZMC
14531,"We develop a novel method for detection of signals and reconstruction of images in the presence of random noise. The method uses results from percolation theory. We specifically address the problem of detection of multiple objects of unknown shapes in the case of nonparametric noise. The noise density is unknown and can be heavy-tailed. The objects of interest have unknown varying intensities. No boundary shape constraints are imposed on the objects, only a set of weak bulk conditions is required. We view the object detection problem as a multiple hypothesis testing for discrete statistical inverse problems. We present an algorithm that allows to detect greyscale objects of various shapes in noisy images. We prove results on consistency and algorithmic complexity of our procedures. Applications to cryo-electron microscopy are presented.",Mikhail Langovoy and Michael Habeck and Bernhard Schölkopf,10,15573838371290798377,arXiv preprint arXiv:1310.8574,,,,"Spatial statistics, image analysis and percolation theory",https://arxiv.org/abs/1310.8574,,2013,/scholar?cites=15573838371290798377,DZ-fHPgAAAAJ:qe6vwMD2xtsC
14532,"Automatic image colorization is the task of adding colors to a grayscale image without any user intervention. This problem is ill-posed in the sense that there is not a unique colorization of a grayscale image without any prior knowledge. Indeed, many objects can have different colors. This is not only true for artificial objects, such as plastic objects",Guillaume Charpiat and Ilja Bezrukov and Yasemin Altun and Matthias Hofmann and BERNHARD SCH,10,13498601679267843482,,,1-27,CRC Press,Machine learning methods for automatic image colorization,http://books.google.com/books?hl=en&lr=&id=rGWxB7rkJl8C&oi=fnd&pg=PA395&dq=info:mk1AabWqVLsJ:scholar.google.com&ots=eK_9nIlMxR&sig=qJJ0AsW-EmITDtwDB3j93bYxOaI,,2011,/scholar?cites=13498601679267843482,DZ-fHPgAAAAJ:AI7cNJ8lPWoC
14533,"Implicit Wiener series are a powerful tool to build Volterra representations of time series with any degree of non-linearity. A natural question is then whether higher order representations yield more useful models. In this work we shall study this question for ECoG data channel relationships in epileptic seizure recordings, considering whether quadratic representations yield more accurate classifiers than linear ones. To do so we first show how to derive statistical information on the Volterra coefficient distribution and how to construct seizure classification patterns over that information. As our results illustrate, a quadratic model seems to provide no advantages over a linear one. Nevertheless, we shall also show that the interpretability of the implicit Wiener series provides insights into the inter-channel relationships of the recordings.",Alvaro Barbero and Matthias Franz and Wim Van Drongelen and José R Dorronsoro and B Scholkopf and Moritz Grosse-Wentrup,10,12588256925087594465,,,5304-5307,IEEE,Implicit wiener series analysis of epileptic seizure recordings,https://ieeexplore.ieee.org/abstract/document/5333080/,,2009,/scholar?cites=12588256925087594465,DZ-fHPgAAAAJ:BrOSOlqYqPUC
14534,,Sebastian Mika and Gunnar Rätsch and Jason Weston and Bernhard Schölkopf and Alex J Smola and Klaus-Robert Müller,10,5749332078794573810,IEEE Transactions on Pattern Analysis and Machine Intelligence,5,623-628,,Learning discriminative and invariant nonlinear features,http://scholar.google.com/scholar?cluster=5749332078794573810&hl=en&oi=scholarr,25,2003,/scholar?cites=5749332078794573810,DZ-fHPgAAAAJ:ol8SaEdbPKgC
14535,"Probabilistic representations of movement primitives open important new possibilities for machine learning in robotics. These representations are able to capture the variability of the demonstrations from a teacher as a probability distribution over trajectories, providing a sensible region of exploration and the ability to adapt to changes in the robot environment. However, to be able to capture variability and correlations between different joints, a probabilistic movement primitive requires the estimation of a larger number of parameters compared to their deterministic counterparts, which focus on modeling only the mean behavior. In this article, we make use of prior distributions over the parameters of a probabilistic movement primitive to make robust estimates of the parameters with few training instances. In addition, we introduce general purpose operators to adapt movement primitives in joint and task space. The …",Sebastian Gomez-Gonzalez and Gerhard Neumann and Bernhard Schölkopf and Jan Peters,9,15367734054490187392,IEEE Transactions on Robotics,2,366-379,IEEE,Adaptation and robust learning of probabilistic movement primitives,https://ieeexplore.ieee.org/abstract/document/9020014/,36,2020,/scholar?cites=15367734054490187392,DZ-fHPgAAAAJ:ylArFPSbTQAC
14536,"Sequential data often originates from diverse domains across which statistical regularities and domain specifics exist. To specifically learn cross-domain sequence representations, we introduce disentangled state space models (DSSM)--a class of SSM in which domain-invariant state dynamics is explicitly disentangled from domain-specific information governing that dynamics. We analyze how such separation can improve knowledge transfer to new domains, and enable robust prediction, sequence manipulation and domain characterization. We furthermore propose an unsupervised VAE-based training procedure to implement DSSM in form of Bayesian filters. In our experiments, we applied VAE-DSSM framework to achieve competitive performance in online ODE system identification and regression across experimental settings, and controlled generation and prediction of bouncing ball video sequences across varying gravitational influences.",Đorđe Miladinović and Muhammad Waleed Gondal and Bernhard Schölkopf and Joachim M Buhmann and Stefan Bauer,9,17131022863709378108,arXiv preprint arXiv:1906.03255,,,,Disentangled state space representations,https://arxiv.org/abs/1906.03255,,2019,/scholar?cites=17131022863709378108,DZ-fHPgAAAAJ:uSmCpOFNWIoC
14537,"We are witnessing an increasing use of data-driven predictive models to inform decisions. As decisions have implications for individuals and society, there is increasing pressure on decision makers to be transparent about their decision policies, models, and the features they use. At the same time, individuals may use knowledge, gained by transparency, to invest effort strategically in order to maximize their chances of receiving a beneficial decision. In this paper, our goal is to find decision policies that are optimal in terms of utility in such a strategic setting. To this end, we first use the theory of optimal transport to characterize how strategic investment of effort by individuals leads to a change in the feature distribution at a population level. Then, we show that, in contrast with the non-strategic setting, optimal decision policies are stochastic, and we cannot expect to find them in polynomial time. Finally, we derive an …",Moein Khajehnejad and Behzad Tabibian and Bernhard Schölkopf and Adish Singla and Manuel Gomez-Rodriguez,9,3470670758393697271,arXiv preprint arXiv:1905.09239,,,,Optimal decision making under strategic behavior,https://128.84.4.13/abs/1905.09239v1,,2019,/scholar?cites=3470670758393697271,DZ-fHPgAAAAJ:G546HapNa1AC
14538,"Self-referential processing is a key cognitive process, associated with the serotonergic system and the default mode network (DMN). Decreased levels of serotonin and reduced activations of the DMN observed in amyotrophic lateral sclerosis (ALS) suggest that self-referential processing might be altered in patients with ALS. Here, we investigate the effects of ALS on the electroencephalography correlates of self-referential thinking. We find that electroencephalography (EEG) correlates of self-referential thinking are present in healthy individuals, but not in those with ALS. In particular, thinking about themselves or others significantly modulates the bandpower in the medial prefrontal cortex in healthy individuals, but not in ALS patients. This finding supports the view of ALS as a complex multisystem disorder which, as shown here, includes dysfunctional processing of the medial prefrontal cortex. It points towards possible alterations of self-consciousness in ALS patients, which might have important consequences for patients’ self-conceptions, personal relations, and decision-making.",Tatiana Fomina and Sebastian Weichwald and Matthis Synofzik and Jenifer Just and Ludger Schöls and Bernhard Schölkopf and Moritz Grosse-Wentrup,9,18110956432739095402,PloS one,6,e0180136,Public Library of Science,Absence of EEG correlates of self-referential processing depth in ALS,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180136,12,2017,/scholar?cites=18110956432739095402,DZ-fHPgAAAAJ:tTtrriHYXHoC
14539,"We introduce a novel framework for adversarial training where the target distribution is annealed between the uniform distribution and the data distribution. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable irrespective of the divergence measures in the objective function and proposed an algorithm, dubbed ß-GAN, in corollary. In this framework, the fact that the initial support of the generative network is the whole ambient space combined with annealing are key to balancing the minimax game. In our experiments on synthetic data, MNIST, and CelebA, ß-GAN with a fixed annealing schedule was stable and did not suffer from mode collapse.",Arash Mehrjou and Bernhard Schölkopf and Saeed Saremi,9,5115247932126406942,arXiv preprint arXiv:1705.07505,,,,Annealed generative adversarial networks,https://arxiv.org/abs/1705.07505,,2017,/scholar?cites=5115247932126406942,DZ-fHPgAAAAJ:zVSo-THGCDcC
14540,"Structural causal models (SCMs), also known as non-parametric structural equation models (NP-SEMs), are widely used for causal modeling purposes. In this paper, we give a rigorous treatment of structural causal models, dealing with measure-theoretic complications that arise in the presence of cyclic relations. The central question studied in this paper is: given a (possibly cyclic) SCM defined on a large system (consisting of observable endogenous and latent exogenous variables), can we “project it down” to an SCM that describes a subsystem (consisting of a subset of the observed endogenous variables and possibly different latent exogenous variables) in order to obtain a more parsimonious but equivalent representation of the subsystem? We define a marginalization operation that effectively removes a subset of the endogenous variables from the model, and a class of mappings, exogenous reparameterizations, that can be used to reduce the space of exogenous variables. We show that both operations preserve the causal semantics of the model and that under mild conditions they can lead to a significant reduction of the model complexity, at least in terms of the number of variables in the model. We argue that for the task of estimating an SCM from data, the existence of “smooth” reductions would be desirable. We provide several conditions under which the existence of such reductions can be shown, but also provide a counterexample that shows that such reductions do not exist in general. The latter result implies that existing approaches to estimate linear or Markovian SCMs from data cannot be extended to general SCMs.",Stephan Bongers and Jonas Peters and Bernhard Schölkopf and Joris M Mooij,9,4387860503998649694,arXiv preprint arXiv,,,,"Structural causal models: Cycles, marginalizations, exogenous reparametrizations and reductions",https://staff.fnwi.uva.nl/j.m.mooij/articles/1611.06221v1.pdf,1611,2016,/scholar?cites=4387860503998649694,DZ-fHPgAAAAJ:iI4frfBhC7MC
14541,,Mateo Rojas-Carulla and Bernhard Schölkopf and Richard Turner and Jonas Peters,9,5063716263014630840,arXiv preprint arXiv:1507.05333,,,,Causal transfer in machine learning,http://scholar.google.com/scholar?cluster=5063716263014630840&hl=en&oi=scholarr,,2015,/scholar?cites=5063716263014630840,DZ-fHPgAAAAJ:tcItjQryOTkC
14542,"We introduce machine learning techniques, more specifically kernel methods, and show how they can be used for medical imaging. After a tutorial presentation of machine learning concepts and tools, including Support Vector Machine (SVM), kernel ridge regression and kernel PCA, we present an application of these tools to the prediction of Computed Tomography (CT) images based on Magnetic Resonance (MR) images.",Guillaume Charpiat and Matthias Hofmann and Bernhard Schölkopf,9,5522114874739775607,,,63-81,"Springer, Boston, MA",Kernel methods in medical imaging,https://link.springer.com/chapter/10.1007/978-0-387-09749-7_4,,2015,/scholar?cites=5522114874739775607,DZ-fHPgAAAAJ:_B80troHkn4C
14543,"We introduce a learning technique for regression between high-dimensional spaces. Standard methods typically reduce this task to many one-dimensional problems, with each output dimension considered independently. By contrast, in our approach the feature construction and the regression estimation are performed jointly, directly minimizing a loss function that we specify, subject to a rank constraint. A major advantage of this approach is that the loss is no longer chosen according to the algorithmic requirements, but can be tailored to the characteristics of the task at hand; the features will then be optimal with respect to this objective, and dependence between the outputs can be exploited.",Gokhan H Bakir and Arthur Gretton and Matthias Franz and Bernhard Scholkopf,9,4597483648834348951,Lecture Notes in Computer Science,,262-269,"Berlin: Springer-Verlag, 1973-",Multivariate Regression via Stiefel Manifold Constraints,https://link.springer.com/chapter/10.1007/978-3-540-28649-3_32,,2004,/scholar?cites=4597483648834348951,DZ-fHPgAAAAJ:HJSXoJQnj-YC
14544,"Besides the familiar moon illusion [eg Hershenson, 1989 The Moon Illusion (Hillsdale, NJ: Lawrence Erlbaum Associates)], wherein the moon appears bigger when it is close to the horizon, there is a less known illusion which causes the moon's illuminated side to appear turned away from the direction of the sun. An experiment documenting the effect is described, and a possible explanation is put forward.",Bernhard Schölkopf,9,2747999048802763145,Perception,10,1229-1232,SAGE Publications,The moon tilt illusion,https://journals.sagepub.com/doi/abs/10.1068/p271229,27,1998,/scholar?cites=2747999048802763145,DZ-fHPgAAAAJ:X_muxcni1ycC
14545,"We derive new bounds on covering numbers for hypothesis classes generated by convex combinations of basis functions. These are useful in bounding the generalization performance of algorithms such as RBF-networks, boosting and a new class of linear programming machines similar to SV machines. We show that p-convex combinations with p> 1 lead to diverging bounds, whereas for p= 1 good bounds in terms of entropy numbers can be obtained. In the case of kernel expansions, significantly better bounds can be obtained depending on the eigenvalues of the corresponding integral operators.",Alex J Smola and Robert C Williamson and Bernhard Schölkopf,9,17379663492975749815,,,,,Generalization bounds for convex combinations of kernel functions,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1911,,1998,/scholar?cites=17379663492975749815,DZ-fHPgAAAAJ:ClCfbGk0d_YC
14546,,B Schölkopf and D Burges and V Vapnik,9,9832262506652318355,,,252-257,"Menlo Park, CA: AAAI Press",Extracting support data for a given task Proceedings of first international conference on knowledge discovery and data mining,http://scholar.google.com/scholar?cluster=9832262506652318355&hl=en&oi=scholarr,,1995,/scholar?cites=9832262506652318355,DZ-fHPgAAAAJ:kmyamZXjmjoC
14547,"Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this paper, we propose a novel, probabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical approaches. This leads to significant improvements in parameter accuracy and robustness given random initial guesses. On four established benchmark systems, we compare the performance of our algorithms to state-of-the-art solutions based on extended Kalman filtering and Gaussian processes.",Gabriele Abbati and Philippe Wenk and Michael A Osborne and Andreas Krause and Bernhard Schölkopf and Stefan Bauer,8,17009642068403205784,arXiv preprint arXiv:1902.08480,,,,AReS and MaRS-Adversarial and MMD-Minimizing Regression for SDEs,https://arxiv.org/abs/1902.08480,,2019,/scholar?cites=17009642068403205784,DZ-fHPgAAAAJ:qg3aUzpqnwkC
14548,"We introduce GeNet, a method for shotgun metagenomic classification from raw DNA sequences that exploits the known hierarchical structure between labels for training. We provide a comparison with state-of-the-art methods Kraken and Centrifuge on datasets obtained from several sequencing technologies, in which dataset shift occurs. We show that GeNet obtains competitive precision and good recall, with orders of magnitude less memory requirements. Moreover, we show that a linear model trained on top of representations learned by GeNet achieves recall comparable to state-of-the-art methods on the aforementioned datasets, and achieves over 90% accuracy in a challenging pathogen detection problem. This provides evidence of the usefulness of the representations learned by GeNet for downstream biological tasks.",Mateo Rojas-Carulla and Ilya Tolstikhin and Guillermo Luque and Nicholas Youngblut and Ruth Ley and Bernhard Schölkopf,8,3944137864966058382,arXiv preprint arXiv:1901.11015,,,,Genet: Deep representations for metagenomics,https://arxiv.org/abs/1901.11015,,2019,/scholar?cites=3944137864966058382,DZ-fHPgAAAAJ:HxXWaseYyVkC
14549,"A common assumption in causal modeling posits that the data is generated by a set of independent mechanisms, and algorithms should aim to recover this structure. Standard unsupervised learning, however, is often concerned with training a single model to capture the overall distribution or aspects thereof. Inspired by clustering approaches, we consider mixtures of implicit generative models that``disentangle''the independent generative mechanisms underlying the data. Relying on an additional set of discriminators, we propose a competitive training procedure in which the models only need to capture the portion of the data distribution from which they can produce realistic samples. As a by-product, each model is simpler and faster to train. We empirically show that our approach splits the training distribution in a sensible way and increases the quality of the generated samples.",Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf,8,11241300089336272432,arXiv preprint arXiv:1804.11130,,,,Competitive training of mixtures of independent deep generative models,https://arxiv.org/abs/1804.11130,,2018,/scholar?cites=11241300089336272432,DZ-fHPgAAAAJ:5ct2hXp9qB4C
14550,"Structural causal models (SCMs), also known as (non-parametric) structural equation models (SEMs), are widely used for causal modeling purposes. A large body of theoretical results is available for the special case in which cycles are absent (ie, acyclic SCMs, also known as recursive SEMs). However, in many application domains cycles are abundantly present, for example in the form of feedback loops. In this paper, we provide a general and rigorous theory of cyclic SCMs. The paper consists of two parts: the first part gives a rigorous treatment of structural causal models, dealing with measure-theoretic and other complications that arise in the presence of cycles. In contrast with the acyclic case, in cyclic SCMs solutions may no longer exist, or if they exist, they may no longer be unique, or even measurable in general. We give several sufficient and necessary conditions for the existence of (unique) measurable solutions. We show how causal reasoning proceeds in these models and how this differs from the acyclic case. Moreover, we give an overview of the Markov properties that hold for cyclic SCMs. In the second part, we address the question of how one can marginalize an SCM (possibly with cycles) to a subset of the endogenous variables. We show that under a certain condition, one can effectively remove a subset of the endogenous variables from the model, leading to a more parsimonious marginal SCM that preserves the causal and counterfactual semantics of the original SCM on the remaining variables. Moreover, we show how the marginalization relates to the latent projection and to latent confounders, ie latent common causes.",Stephan Bongers and Jonas Peters and Bernhard Schölkopf and Joris M Mooij,8,7111237426085011391,arXiv preprint arXiv:1611.06221,,,,Theoretical aspects of cyclic structural causal models,https://arxiv.org/abs/1611.06221,,2016,/scholar?cites=7111237426085011391,DZ-fHPgAAAAJ:RKELEoasyEoC
14551,"For large-scale multi-class classification problems, consisting of tens of thousand target categories, recent works have emphasized the need to store billions of parameters. For instance, the classical l2-norm regularization employed by a state-of-the-art method results in the model size of 17GB for a training set whose size is only 129MB. To the contrary, by using a mixed-norm regularization approach, we show that around 99.5% of the stored parameters is dispensable noise. Using this strategy, we can extract the information relevant for classification, which is constituted in remaining 0.5% of the parameters, and hence demonstrate drastic reduction in model sizes. Furthermore, the proposed method leads to improvement in generalization performance compared to state-of-the-art methods, especially for under-represented categories. Lastly, our method enjoys easy parallelization, and scales well to tens of thousand …",Rohit Babbar and Krikamol Maundet and Bernhard Schölkopf,8,18041043841761565124,,,234-242,Society for Industrial and Applied Mathematics,Tersesvm: A scalable approach for learning compact models in large-scale classification,https://epubs.siam.org/doi/abs/10.1137/1.9781611974348.27,,2016,/scholar?cites=18041043841761565124,DZ-fHPgAAAAJ:sULuYBl4uKQC
14552,"Pattern recognition in neuroimaging distinguishes between two types of models: encoding- and decoding models. This distinction is based on the insight that brain state features, that are found to be relevant in an experimental paradigm, carry a different meaning in encoding-than in decoding models. In this paper, we argue that this distinction is not sufficient: Relevant features in encoding- and decoding models carry a different meaning depending on whether they represent causal-or anti-causal relations. We provide a theoretical justification for this argument and conclude that causal inference is essential for interpretation in neuroimaging.",Sebastian Weichwald and Bernhard Schölkopf and Tonio Ball and Moritz Grosse-Wentrup,8,10954113406547859658,,,1-4,IEEE,Causal and anti-causal learning in pattern recognition for neuroimaging,https://ieeexplore.ieee.org/abstract/document/6858551/,,2014,/scholar?cites=10954113406547859658,DZ-fHPgAAAAJ:I2jIoRS3jIgC
14553,"Cryo-electron microscopy (cryo-EM) is an emerging experimental method to characterize the structure of large biomolecular assemblies. Single particle cryo-EM records 2D images (so-called micrographs) of projections of the three-dimensional particle, which need to be processed to obtain the three-dimensional reconstruction. A crucial step in the reconstruction process is particle picking which involves detection of particles in noisy 2D micrographs with low signal-to-noise ratios of typically 1: 10 or even lower. Typically, each picture contains a large number of particles, and particles have unknown irregular and nonconvex shapes.",Mikhail Langovoy and Michael Habeck and Bernhard Schölkopf,8,11288450978722148325,arXiv preprint arXiv:1311.7650,,,,Adaptive nonparametric detection in cryo-electron microscopy,https://arxiv.org/abs/1311.7650,,2013,/scholar?cites=11288450978722148325,DZ-fHPgAAAAJ:WF5omc3nYNoC
14554,"It is disclosed a system and method (12) for determining a property map (82) of an object, particularly a human being, based on at least a first image (84), particularly an magnetic resonance (MR) image, of the object. In the method (12), a structure of reference pairs is defined in a first step (96), wherein each reference pair (16-26) comprises at least two entries (62). The first entry represents a property value, particularly an attenuation value. The second entry (62) preferably represents a group of image points (67) belonging together, which is extracted particularly from MR images (28) and comprises an interesting image point corresponding to the property value. In another step (98) of the method (12) a plurality of training pairs (16-26) is provided. A structure of the training pairs (16-26) corresponds to the structure of reference pairs, and the entries of respective training pairs (16-26) are known. In another step (100 …",,8,3881176044483063646,,,,,"Method for determining a property map of an object, particularly of a living being, based on at least a first image, particularly a magnetic resonance image",https://patents.google.com/patent/US8290568B2/en,,2012,/scholar?cites=3881176044483063646,DZ-fHPgAAAAJ:zGdJYJv2LkUC
14555,"Cryo-electron microscopy (cryo-EM) plays an increasingly prominent role in structure elucidation of macromolecular assemblies. Advances in experimental instrumentation and computational power have spawned numerous cryo-EM studies of large biomolecular complexes resulting in the reconstruction of three-dimensional density maps at intermediate and low resolution. In this resolution range, identification and interpretation of structural elements and modeling of biomolecular structure with atomic detail becomes problematic. In this article, we present a novel algorithm that enhances the resolution of intermediate- and low-resolution density maps. Our underlying assumption is to model the low-resolution density map as a blurred and possibly noise-corrupted version of an unknown high-resolution map that we seek to recover by deconvolution. By exploiting the nonnegativity of both the high-resolution map and …",Michael Hirsch and Bernhard Schölkopf and Michael Habeck,8,1576329200835716726,Journal of Computational Biology,3,335-346,"Mary Ann Liebert, Inc.",A blind deconvolution approach for improving the resolution of cryo-EM density maps,https://www.liebertpub.com/doi/abs/10.1089/cmb.2010.0264,18,2011,/scholar?cites=1576329200835716726,DZ-fHPgAAAAJ:kzcrU_BdoSEC
14556,"The analysis of n-ary relations receives attention in many different fields, for instance biology, web mining, and social studies. In the basic setting, there are n sets of instances, and each observation associates n instances, one from each set. A common approach to explore these n-way data is the search for n-set patterns. An n-set pattern consists of specific subsets of the n instance sets such that all possible n-ary associations between the corresponding instances are observed. This provides a higher-level view of the data, revealing associative relationships between groups of instances. Here, we generalize this approach in two respects. First, we tolerate missing observations to a certain degree, that means we are also interested in n-sets where most (although not all) of the possible combinations have been recorded in the data. Second, we take association weights into account. More precisely, we propose a …",Elisabeth Georgii and Koji Tsuda and Bernhard Schölkopf,8,15738746827053059017,,,1-10,,Multi-way set enumeration in real-valued tensors,https://dl.acm.org/doi/abs/10.1145/1581114.1581118,,2009,/scholar?cites=15738746827053059017,DZ-fHPgAAAAJ:UxriW0iASnsC
14557,"We present a generalization of thin‐plate splines for interpolation and approximation of manifold‐valued data, and demonstrate its usefulness in computer graphics with several applications from different fields. The cornerstone of our theoretical framework is an energy functional for mappings between two Riemannian manifolds which is independent of parametrization and respects the geometry of both manifolds. If the manifolds are Euclidean, the energy functional reduces to the classical thin‐plate spline energy. We show how the resulting optimization problems can be solved efficiently in many cases. Our example applications range from orientation interpolation and motion planning in animation over geometric modelling tasks to color interpolation.",Florian Steinke and Matthias Hein and Jan Peters and Bernhard Schölkopf,8,6278190320104248669,Computer Graphics Forum,2,437-448,Blackwell Publishing Ltd,Manifold‐valued Thin‐Plate Splines with Applications in Computer Graphics,https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2008.01141.x,27,2008,/scholar?cites=6278190320104248669,DZ-fHPgAAAAJ:dBIO0h50nwkC
14558,We propose a method to evaluate the complexity of probability measures from data that is based on a reproducing kernel Hilbert space seminorm of the logarithm of conditional probability densities. The motivation is to provide a tool for a causal inference method which assumes that conditional probabilities for effects given their causes are typically simpler and smoother than vice-versa. We present experiments with toy data where the quantitative results are consistent with our intuitive understanding of complexity and smoothness. Also in some examples with real-world data the probability measure corresponding to the true causal direction turned out to be less complex than those of the reversed order.,Xiaohai Sun and Dominik Janzing and Bernhard Schölkopf,8,13955737763925028206,,,441-446,D-Side Publications,Distinguishing between cause and effect via kernel-based complexity measures for conditional distributions,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790532,,2007,/scholar?cites=13955737763925028206,DZ-fHPgAAAAJ:ZHo1McVdvXMC
14559,"Page 1. kyb-logo mc-logo Motivation Automatic Feature Selection Summary Learning optimal EEG
features across time, frequency and space. Jason Farquhar, Jeremy Hill, Bernhard Schölkopf Max
Planck Institute for Biological Cybernetics, Tübingen Germany NIPS06 Workshop on Trends in
BCI Jason Farquhar, Jeremy Hill, Bernhard Schölkopf MPI Tübingen Learning optimal EEG features
across time, frequency and space. Page 2. kyb-logo mc-logo Motivation Automatic Feature Selection
Summary Outline Motivation Source types in EEG based BCI Automatic Feature Selection Learning
Spatial Features Feature selection as Model Selection Spectral/Temporal Filtering Jason Farquhar,
Jeremy Hill, Bernhard Schölkopf MPI Tübingen Learning optimal EEG features across time,
frequency and space. Page 3. kyb-logo mc-logo Motivation Automatic Feature Selection Summary
The current approach to learning in BCIs … 
",Jason Farquhar and Jeremy Hill and Bernhard Schölkopf,8,7655240138687361249,,,,,"Learning optimal EEG features across time, frequency and space",https://pure.mpg.de/rest/items/item_1790695/component/file_3167646/content,,2006,/scholar?cites=7655240138687361249,DZ-fHPgAAAAJ:FAceZFleit8C
14560,"In this paper we report the results obtained using a kernel-based approach to predict the temporal development of four response signals in the process control of a glass melting tank with 16 input parameters. The data set is a revised version from the modelling challenge in EUNITE-2003. The central difficulties are: large time-delays between changes in the inputs and the outputs, large number of data, and a general lack of knowledge about the relevant variables that intervene in the process. The methodology proposed here comprises Support Vector Machines (SVM) and Regularization Networks (RN). We use the idea of sparse approximation both as a means of regularization and as a means of reducing the computational complexity. Furthermore, we will use an incremental approach to add new training examples to the kernel-based method and efficiently update the current solution. This allows us to use …",Tobias Jung and Luis Herrera and Bernhard Schoelkopf,8,16304987512126866415,,,960-967,"Springer, Berlin, Heidelberg",Long term prediction of product quality in a glass manufacturing process using a kernel based approach,https://link.springer.com/chapter/10.1007/11494669_118,,2005,/scholar?cites=16304987512126866415,DZ-fHPgAAAAJ:SP6oXDckpogC
14561,"The use of non-orthonormal basis functions in ridge regression leads to an often undesired non-isotropic prior in function space. In this study, we investigate an alternative regularization technique that results in an implicit whitening of the basis functions by penalizing directions in function space with a large prior variance. The regularization term is computed from unlabelled input data that characterizes the input distribution. Tests on two datasets using polynomial basis functions showed an improved average performance compared to standard ridge regression.",Matthias O Franz and Younghee Kwon and Carl Edward Rasmussen and Bernhard Schölkopf,8,16415276601820469980,,,18-26,"Springer, Berlin, Heidelberg",Semi-supervised kernel regression using whitened function classes,https://link.springer.com/chapter/10.1007/978-3-540-28649-3_3,,2004,/scholar?cites=16415276601820469980,DZ-fHPgAAAAJ:YohjEiUPhakC
14562,,S Romdhani and P Torr and B Schölkopf and A Blake,8,5122153558169470036,Proc. Int’l Conf. on Computer Vision,,,,"Fast face detection, using a sequential reduced support vector evaluation",http://scholar.google.com/scholar?cluster=5122153558169470036&hl=en&oi=scholarr,,2001,/scholar?cites=5122153558169470036,DZ-fHPgAAAAJ:FhAcYObmztMC
14563,"Autor: Bartlett, PL et al.; Genre: Bericht; Im Druck veröffentlicht:
2001; Titel: Some kernels for structured data.
",PL Bartlett and B Schölkopf,8,1929835446291053310,,,,Biowulf Technologies,Some kernels for structured data,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1793351,,2001,/scholar?cites=1929835446291053310,DZ-fHPgAAAAJ:eLRq4zTgah0C
14564,"CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 … 
",B Schölkopf,8,4200322730949001504,Advances in Kernel Methods: Support Vector Learning,,,MIT Press,Kernel principal component analysis,https://ci.nii.ac.jp/naid/10022345923/,,1998,/scholar?cites=4200322730949001504,DZ-fHPgAAAAJ:4ySvW73dVKYC
14565,"We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.",Luigi Gresele and Paul K Rubenstein and Arash Mehrjou and Francesco Locatello and Bernhard Schölkopf,7,10611795769587929892,,,217-227,PMLR,The incomplete rosetta stone problem: Identifiability results for multi-view nonlinear ica,http://proceedings.mlr.press/v115/gresele20a.html,,2020,/scholar?cites=10611795769587929892,DZ-fHPgAAAAJ:Zx2yaX5tUAoC
14566,"Controlling musculoskeletal systems, especially robots actuated by pneumatic artificial muscles, is a challenging task due to nonlinearities, hysteresis effects, massive actuator delay, and unobservable dependencies such as temperature. Despite such difficulties, muscular systems offer many beneficial properties to achieve human-comparable performance in uncertain and fast-changing tasks. For example, muscles are backdrivable and provide variable stiffness while offering high forces to reach high accelerations. In addition, the embodied intelligence deriving from the compliance might reduce the control demands for specific tasks. In this letter, we address the problem of how to accurately control musculoskeletal robots. To address this issue, we propose to learn probabilistic forward dynamics models using Gaussian processes and, subsequently, to employ these models for control. However, Gaussian …",Dieter Büchler and Roberto Calandra and Bernhard Schölkopf and Jan Peters,7,18160256761890071609,IEEE Robotics and Automation Letters,4,3161-3168,IEEE,Control of musculoskeletal systems using learned dynamics models,https://ieeexplore.ieee.org/abstract/document/8391763/,3,2018,/scholar?cites=18160256761890071609,DZ-fHPgAAAAJ:d7h7K-jXHrsC
14567,,Rohit Babbar and Martin Heni and Andreas Peter and Martin Hrabě de Angelis and Hans-Ulrich Häring and Andreas Fritsche and Hubert Preissl and Bernhard Schölkopf and Róbert Wagner,7,5974093260033232510,Frontiers in endocrinology,,82,Frontiers,Prediction of glucose tolerance without an oral glucose tolerance test,https://www.frontiersin.org/articles/10.3389/fendo.2018.00082/full?report=reader,9,2018,/scholar?cites=5974093260033232510,DZ-fHPgAAAAJ:aLuW2LJrqxQC
14568,"Clustering is a cornerstone of unsupervised learning which can be thought as disentangling multiple generative mechanisms underlying the data. In this paper we introduce an algorithmic framework to train mixtures of implicit generative models which we particularize for variational autoencoders. Relying on an additional set of discriminators, we propose a competitive procedure in which the models only need to approximate the portion of the data distribution from which they can produce realistic samples. As a byproduct, each model is simpler to train, and a clustering interpretation arises naturally from the partitioning of the training points among the models. We empirically show that our approach splits the training distribution in a reasonable way and increases the quality of the generated samples.",Francesco Locatello and Damien Vincent and Ilya Tolstikhin and Gunnar Ratsch and Sylvain Gelly and Bernhard Scholkopf,7,2981142145370634434,,,,,Clustering meets implicit generative models,https://openreview.net/forum?id=rk4QYDkwz,,2018,/scholar?cites=2981142145370634434,DZ-fHPgAAAAJ:YaGrn9P4G-YC
14569,"State-of-the-art video restoration methods integrate optical flow estimation networks to utilize temporal information. However, these networks typically consider only a pair of consecutive frames and hence are not capable of capturing long-range temporal dependencies and fall short of establishing correspondences across several timesteps. To alleviate these problems, we propose a novel Spatio-temporal Transformer Network (STTN) which handles multiple frames at once and thereby manages to mitigate the common nuisance of occlusions in optical flow estimation. Our proposed STTN comprises a module that estimates optical flow in both space and time and a resampling layer that selectively warps target frames using the estimated flow. In our experiments, we demonstrate the efficiency of the proposed network and show state-of-the-art restoration results in video super-resolution and video deblurring.",Tae Hyun Kim and Mehdi SM Sajjadi and Michael Hirsch and Bernhard Scholkopf,7,7304810781046710446,,,106-122,,Spatio-temporal transformer network for video restoration,http://openaccess.thecvf.com/content_ECCV_2018/html/Tae_Hyun_Kim_Spatio-temporal_Transformer_Network_ECCV_2018_paper.html,,2018,/scholar?cites=7304810781046710446,DZ-fHPgAAAAJ:LdmrAZdbiBsC
14570,"Objective. Task-induced amplitude modulation of neural oscillations is routinely used in brain-computer interfaces (BCIs) for decoding subjects' intents, and underlies some of the most robust and common methods in the field, such as common spatial patterns and Riemannian geometry. While there has been some interest in phase-related features for classification, both techniques usually presuppose that the frequencies of neural oscillations remain stable across various tasks. We investigate here whether features based on task-induced modulation of the frequency of neural oscillations enable decoding of subjects' intents with an accuracy comparable to task-induced amplitude modulation. Approach. We compare cross-validated classification accuracies using the amplitude and frequency modulated features, as well as a joint feature space, across subjects in various paradigms and pre-processing conditions. We …",Vinay Jayaram and Matthias Hohmann and Jennifer Just and Bernhard Schölkopf and Moritz Grosse-Wentrup,7,9798307669492435547,Journal of Neural Engineering,5,056015,IOP Publishing,Task-induced frequency modulation features for brain-computer interfacing,https://iopscience.iop.org/article/10.1088/1741-2552/aa7778/meta,14,2017,/scholar?cites=9798307669492435547,DZ-fHPgAAAAJ:2zIWv0KGyqYC
14571,"We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function f, consistent estimators of the mean embedding of a random variable X lead to consistent estimators of the mean embedding of f (X). For Matern kernels and sufficiently smooth functions we also provide rates of convergence. Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on iid samples as well as"" reduced set"" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.",Carl-Johann Simon-Gabriel and Adam Scibior and Ilya O Tolstikhin and Bernhard Schölkopf,7,15605698518429640623,,,1732-1740,,Consistent kernel mean estimation for functions of random variables,http://papers.nips.cc/paper/6545-consistent-kernel-mean-estimation-for-functions-of-random-variables,,2016,/scholar?cites=15605698518429640623,DZ-fHPgAAAAJ:APzTi3oVP6kC
14572,"It is commonplace to encounter nonstationary data, of which the underlying generating process may change over time or across domains. The nonstationarity presents both challenges and opportunities for causal discovery. In this paper we propose a principled framework to handle nonstationarity, and develop some methods to address three important questions. First, we propose an enhanced constraint-based method to detect variables whose local mechanisms are nonstationary and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine some causal directions by taking advantage of information carried by changing distributions. Third, we develop a method for visualizing the nonstationarity of causal modules. Experimental results on various synthetic and real-world data sets are presented to demonstrate the efficacy of our methods.",Kun Zhang and Biwei Huang and Jiji Zhang and Bernhard Schölkopf and Clark Glymour,7,14024465000218805817,arXiv preprint arXiv:1509.08056,,,,Discovery and visualization of nonstationary causal models,https://arxiv.org/abs/1509.08056,,2015,/scholar?cites=14024465000218805817,DZ-fHPgAAAAJ:ilzMrZUAdaAC
14573,"Despite decades of research on EEG-based brain-computer interfaces (BCIs) in patients with amyotrophic lateral sclerosis (ALS), there is still little known about how the disease affects the electromagnetic field of the brain. This may be one reason for the present failure of EEG-based BCI paradigms for completely locked-in ALS patients. In order to help understand this failure, we have recorded resting state data from six ALS patients and thirty-two healthy controls to investigate for group differences. While similar studies have been attempted in the past, none have used high-density EEG or tried to distinguish between physiological and non-physiological sources of the EEG. We find an ALS-specific global increase in gamma power (30-90 Hz) that is not specific to the motor cortex, suggesting that the mechanism behind ALS affects non-motor cortical regions even in the absence of comorbid cognitive deficits.",Vinay Jayaram and Natalie Widmann and Christian Förster and Tatiana Fomina and Matthias Hohmann and Jennifer Müller vom Hagen and Matthis Synofzik and Bernhard Schölkopf and Ludger Schöls and Moritz Grosse-Wentrup,7,2786851348153935595,,,6979-6982,IEEE,Brain-computer interfacing in amyotrophic lateral sclerosis: Implications of a resting-state EEG analysis,https://ieeexplore.ieee.org/abstract/document/7319998/,,2015,/scholar?cites=2786851348153935595,DZ-fHPgAAAAJ:8_Ku1U0RzzYC
14574,"We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as halfsibling regression, is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification and illustrate the potential of the method in a challenging astronomy application.",Bernhard Schölkopf and David Hogg and Dun Wang and Dan Foreman-Mackey and Dominik Janzing and Carl-Johann Simon-Gabriel and Jonas Peters,7,11768165421845046384,,,2218-2226,,Removing systematic errors for exoplanet search via latent causes,http://www.jmlr.org/proceedings/papers/v37/scholkopf15.pdf,,2015,/scholar?cites=11768165421845046384,DZ-fHPgAAAAJ:veuxFd29oYUC
14575,"We present a new method for separating motion blurred foreground objects from their background given a single image. Previous techniques focused on estimating alpha mattes for separating sharp, non-moving foreground objects from fairly homogeneous background. In those cases the only pixels which are ambiguous are those which exhibit fractional pixel occupancy. In this paper, we address the problem of alpha matte and foreground estimation of motion blurred objects. We show, that explicit modeling of the object motion facilitates the estimation and improves the quality of the estimated alpha mattes. In addition, we improve foreground extraction of motion blurred objects with a new regularization term. This task is particularly difficult in smeared out regions, where the background shimmers through. Both synthetic and real-world examples illustrate the merit of our approach.",Rolf Köhler and Michael Hirsch and Bernhard Schölkopf and Stefan Harmeling,7,1548722475211092502,,,3446-3450,IEEE,Improving alpha matting and motion blurred foreground estimation,https://ieeexplore.ieee.org/abstract/document/6738711/,,2013,/scholar?cites=1548722475211092502,DZ-fHPgAAAAJ:Q3-QASNKTMEC
14576,"Learning machines, such as support vector machines, are used to analyze datasets to recognize patterns within the dataset using kernels that are selected according to the nature of the data to be analyzed. Where the datasets include an invariance transformation or noise, tangent vectors are defined to identify relationships between the invariance or noise and the training data points. A covariance matrix is formed using the tangent vectors, then used in generation of the kernel, which may be based on a kernel PCA map.",,7,15867478944416534054,,,,,Kernels for identifying patterns in datasets containing noise or transformation invariances,https://patents.google.com/patent/US8209269B2/en,,2012,/scholar?cites=15867478944416534054,DZ-fHPgAAAAJ:b0M2c_1WBrUC
14577,"Systems and methods for object or pattern detection that use a nonlinear support vector (SV) machine are described. In the illustrated and described embodiment, objects or patterns comprising faces are detected. The decision surface is approximated in terms of a reduced set of expansion vectors. In order to determine the presence of a face, the kernelized inner product of the expansion vectors with the input pattern are sequentially evaluated and summed, such that if at any point the pattern can be rejected as not comprising a face, no more expansion vectors are used. The sequential application of the expansion vectors produces a substantial saving in computational time.",,7,16922262954773694395,,,,,Pattern detection methods and systems and face detection methods and systems,https://patents.google.com/patent/US7099504B2/en,,2006,/scholar?cites=16922262954773694395,DZ-fHPgAAAAJ:GN6RKO0oIOIC
14578,"This chapter contains sections titled: Do Unlabeled Data Improve or Degrade Classification Performance?, Understanding Unlabeled Data: Asymptotic Bias, The Asymptotic Analysis of Generative Semi-Supervised Learning, The Value of Labeled and Unlabeled Data, Finite Sample Effects, Model Search and Robustness, Conclusion",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,7,14335044759870780191,,,57-72,MIT Press,Risks of semi-supervised learning: how unlabeled data can degrade performance of generative classifiers,https://ieeexplore.ieee.org/abstract/document/6280905/,,2006,/scholar?cites=14335044759870780191,DZ-fHPgAAAAJ:Dbk3E0wgKaIC
14579,"We compare Sir Karl Popper’s ideas concerning the falsifiability of a theory with similar notions from VC-theory. Having located some divergences, we discuss how best to view Popper’s work from the perspective of statistical learning theory.",David Corfield and Bernhard Schölkopf and Vladimir Vapnik,7,15138398874398391223,,,,Max Planck Institute for Biological Cybernetics,"Popper, Falsification and the VC-dimension",https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1791225,,2005,/scholar?cites=15138398874398391223,DZ-fHPgAAAAJ:WJVC3Jt7v1AC
14580,"We describe methods for taking into account unlabeled data in the training of a kernel-based classifier, such as a Support Vector Machines (SVM). We propose two approaches utilizing unlabeled points in the vicinity of labeled ones. Both of the approaches effectively modify the metric of the pattern space, either by using non-spherical Gaussian density estimates which are determined using EM, or by modifying the kernel function using displacement vectors computed from pairs of unlabeled and labeled points. The latter is linked to techniques for training invariant SVMs. We present experimental results indicating that the proposed technique can lead to substantial improvements of classification accuracy.",Olivier Chapelle and Bernhard Schölkopf and Jason Weston,7,6919741726954297550,,,1-7,,Semi-supervised learning through principal directions estimation,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792600,,2003,/scholar?cites=6919741726954297550,DZ-fHPgAAAAJ:HtEfBTGE9r8C
14581,,Bernhard Schölkopf and Manfred K Warmuth,7,14722930829831623982,,,,Springer,"Computational Learning Theory and Kernel Machines: 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, August 24 …",http://scholar.google.com/scholar?cluster=14233098832912348727&hl=en&oi=scholarr,,2003,/scholar?cites=14722930829831623982,DZ-fHPgAAAAJ:eGYfIraVYiQC
14582,,B Schoelkopf and J Weston and E Eskin and C Leslie and WS Noble,7,14040126225062698866,Lecture Notes in Computer Science,,,,Dealing with Large Diagonals in Kernel Matrices. Principles of Data Mining and Knowledge Discovery,http://scholar.google.com/scholar?cluster=14040126225062698866&hl=en&oi=scholarr,,2002,/scholar?cites=14040126225062698866,DZ-fHPgAAAAJ:UyABD_aAzOsC
14583,,B Schölkopf and R Herbrich and AJ Smola and A Generalized Representer Theorem,7,6399781576455008889,Lecture Notes in Computer Science,,416-426,,Computational Learning Theory,http://scholar.google.com/scholar?cluster=6399781576455008889&hl=en&oi=scholarr,2111,2001,/scholar?cites=6399781576455008889,DZ-fHPgAAAAJ:iw8yXCQF3REC
14584,京 ICP 证: 010071 京公网安备 11010802020237 号 万方数据知识资源云服务系统 [简称: 知识云服务平台] V1. 0 证书号: 软著登字第 1016589 号,Schoelkopf Bernhard and M Sebastian and J Chris and PK Burges and KR Müller and G Rätsch,7,16810492478887139568,IEEE T Neural Networ,,1000-1017,,Input space versus feature space in kernel-based methods,http://scholar.google.com/scholar?cluster=16810492478887139568&hl=en&oi=scholarr,10,1999,/scholar?cites=16810492478887139568,DZ-fHPgAAAAJ:sXyLePf05P8C
14585,"Consequential decisions are increasingly informed by sophisticated data-driven predictive models. However, consistently learning accurate predictive models requires access to ground truth labels. Unfortunately, in practice, labels may only exist conditional on certain decisions—if a loan is denied, there is not even an option for the individual to pay back the loan. In this paper, we show that, in this selective labels setting, learning to predict is suboptimal in terms of both fairness and utility. To avoid this undesirable behavior, we propose to directly learn stochastic decision policies that maximize utility under fairness constraints. In the context of fair machine learning, our results suggest the need for a paradigm shift from"" learning to predict"" to"" learning to decide"". Experiments on synthetic and real-world data illustrate the favorable properties of learning to decide, in terms of both utility and fairness.",Niki Kilbertus and Manuel Gomez Rodriguez and Bernhard Schölkopf and Krikamol Muandet and Isabel Valera,6,5871484873980383162,,,277-287,PMLR,Fair decisions despite imperfect predictions,http://proceedings.mlr.press/v108/kilbertus20a.html,,2020,/scholar?cites=5871484873980383162,DZ-fHPgAAAAJ:LdboP74Pu9YC
14586,"The kernel mean embedding of probability distributions is commonly used in machine learning as an injective mapping from distributions to functions in an infinite-dimensional Hilbert space. It allows us, for example, to define a distance measure between probability distributions, called the maximum mean discrepancy. In this work, we propose to represent probability distributions in a pure quantum state of a system that is described by an infinite-dimensional Hilbert space and prove that the representation is unique if the corresponding kernel function is c 0 universal. This enables us to work with an explicit representation of the mean embedding, whereas classically one can only work implicitly with an infinite-dimensional Hilbert space through the use of the kernel trick. We show how this explicit representation can speed up methods that rely on inner products of mean embeddings and discuss the theoretical and …",Jonas M Kübler and Krikamol Muandet and Bernhard Schölkopf,6,12629973218553292149,Physical Review Research,3,033159,American Physical Society,Quantum mean embedding of probability distributions,https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.1.033159,1,2019,/scholar?cites=12629973218553292149,DZ-fHPgAAAAJ:iRxVrxesHjoC
14587,"Ranking models are typically designed to provide rankings that optimize some measure of immediate utility to the users. As a result, they have been unable to anticipate an increasing number of undesirable long-term consequences of their proposed rankings, from fueling the spread of misinformation and increasing polarization to degrading social discourse. Can we design ranking models that understand the consequences of their proposed rankings and, more importantly, are able to avoid the undesirable ones? In this paper, we first introduce a joint representation of rankings and user dynamics using Markov decision processes. Then, we show that this representation greatly simplifies the construction of consequential ranking models that trade off the immediate utility and the long-term welfare. In particular, we can obtain optimal consequential rankings just by applying weighted sampling on the rankings provided by models that maximize measures of immediate utility. However, in practice, such a strategy may be inefficient and impractical, specially in high dimensional scenarios. To overcome this, we introduce an efficient gradient-based algorithm to learn parameterized consequential ranking models that effectively approximate optimal ones. We showcase our methodology using synthetic and real data gathered from Reddit and show that ranking models derived using our methodology provide ranks that may mitigate the spread of misinformation and improve the civility of online discussions.",Behzad Tabibian and Vicenç Gómez and Abir De and Bernhard Schölkopf and Manuel Gomez Rodriguez,6,2589829811219747144,arXiv preprint arXiv:1905.05305,,,,Consequential ranking algorithms and long-term welfare,https://arxiv.org/abs/1905.05305,,2019,/scholar?cites=2589829811219747144,DZ-fHPgAAAAJ:KsHrIg64uVUC
14588,"Spaced repetition is a technique for efficient memorization which uses repeated, spaced review of content to improve long-term retention. Can we find the optimal reviewing schedule to maximize the benefits of spaced repetition? In this paper, we introduce a novel, flexible representation of spaced repetition using the framework of marked temporal point processes and then address the above question as an optimal control problem for stochastic differential equations with jumps. For two well-known human memory models, we show that the optimal reviewing schedule is given by the recall probability of the content to be learned. As a result, we can then develop a simple, scalable online algorithm, Memorize, to sample the optimal reviewing times. Experiments on both synthetic and real data gathered from Duolingo, a popular language-learning online platform, show that our algorithm may be able to help learners memorize more effectively than alternatives.",Behzad Tabibian and Utkarsh Upadhyay and Abir De and Ali Zarezade and Bernhard Schoelkopf and Manuel Gomez-Rodriguez,6,727896369330607422,arXiv preprint arXiv:1712.01856,,,,Optimizing human learning,https://arxiv.org/abs/1712.01856,,2017,/scholar?cites=727896369330607422,DZ-fHPgAAAAJ:jY3sEdjNJ28C
14589,"We consider the problem of learning the functions computing children from parents in a Structural Causal Model once the underlying causal graph has been identified. This is in some sense the second step after causal discovery. Taking a probabilistic approach to estimating these functions, we derive a natural myopic active learning scheme that identifies the intervention which is optimally informative about all of the unknown functions jointly, given previously observed data. We test the derived algorithms on simple examples, to demonstrate that they produce a structured exploration policy that significantly improves on unstructured base-lines.",Paul K Rubenstein and Ilya Tolstikhin and Philipp Hennig and Bernhard Schölkopf,6,13719401953042420830,arXiv preprint arXiv:1706.10234,,,,Probabilistic active learning of functions in structural causal models,https://arxiv.org/abs/1706.10234,,2017,/scholar?cites=13719401953042420830,DZ-fHPgAAAAJ:WZ_cLlf9y-MC
14590,"Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN (Mixture Effect Recovery in Linear Networks) algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm (and ready to use code) that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the …",Sebastian Weichwald and Arthur Gretton and Bernhard Scholkopf and Moritz Grosse-Wentrup,6,8903091969848209551,,,1-4,IEEE,Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data,https://ieeexplore.ieee.org/abstract/document/7552331/,,2016,/scholar?cites=8903091969848209551,DZ-fHPgAAAAJ:gWwPWYJ7j-oC
14591,"While invasively recorded brain activity is known to provide detailed information on motor commands, it is an open question at what level of detail information about positions of body parts can be decoded from non-invasively acquired signals. In this work it is shown that index finger positions can be differentiated from non-invasive electroencephalographic (EEG) recordings in healthy human subjects. Using a leave-one-subject-out cross-validation procedure, a random forest distinguished different index finger positions on a numerical keyboard above chance-level accuracy. Among the different spectral features investigated, high β-power (20-30 Hz) over contralateral sensorimotor cortex carried most information about finger position. Thus, these findings indicate that finger position is in principle decodable from non-invasive features of brain activity that generalize across individuals.",Sebastian Weichwald and Timm Meyer and Bernhard Schölkopf and Tonio Ball and Moritz Grosse-Wentrup,6,11808353521453770669,,,1-6,IEEE,Decoding index finger position from EEG using random forests,https://ieeexplore.ieee.org/abstract/document/6844513/,,2014,/scholar?cites=11808353521453770669,DZ-fHPgAAAAJ:nroGzMJTTpEC
14592,"Compared to constraint-based causal discovery, causal discovery based on functional causal models is able to identify the whole causal model under appropriate assumptions. Functional causal models represent the effect as a function of the direct causes together with an independent noise term. Examples include the linear non-Gaussian a cyclic model (LiNGAM), nonlinear additive noise model, and post-nonlinear (PNL) model. Currently there are two ways to estimate the parameters in the models, one is by dependence minimization, and the other is maximum likelihood. In this paper, we show that for any a cyclic functional causal model, minimizing the mutual information between the hypothetical cause and the noise term is equivalent to maximizing the data likelihood with a flexible model for the distribution of the noise term. We then focus on estimation of the PNL causal model, and propose to estimate it with …",Kun Zhang and Zhikun Wang and Bernhard Schölkopf,6,14072168674587299377,,,139-146,IEEE,On estimation of functional causal models: Post-nonlinear causal model as an example,https://ieeexplore.ieee.org/abstract/document/6753913/,,2013,/scholar?cites=14072168674587299377,DZ-fHPgAAAAJ:Ul_CLA4dPeMC
14593,"We provide a simple method, based on volume conduction models, to quantify the neurophysiological plausibility of independent components (ICs) reconstructed from EEG/MEG data. We evaluate the method on EEG data recorded from 19 subjects and compare the results with two established procedures for judging the quality of ICs. We argue that our procedure provides a sound empirical basis for the inclusion or exclusion of ICs in the analysis of experimental data.",Moritz Grosse-Wentrup and Stefan Harmeling and Thorsten Zander and Jeremy Hill and Bernhard Schölkopf,6,124803931598443677,,,102-105,IEEE,How to test the quality of reconstructed sources in independent component analysis (ICA) of EEG/MEG data,https://ieeexplore.ieee.org/abstract/document/6603567/,,2013,/scholar?cites=124803931598443677,DZ-fHPgAAAAJ:hHIA4WEVY-EC
14594,We investigated the impact of gamma-activity in a task-positive network on performance of subjects in a general measure for attention (D2-test). Subjects modulated their gamma activity previous to each run of the test with a simple neurofeedback mechanism. Results indicate that visual attention can be increased significantly by tuning activity in the investigated network into a specific state.,T Zander and B Battes and B Schoelkopf and M Grosse-Wentrup,6,8276451185080135256,"Proceedings of the Fifth International Brain-Computer Interface Meeting: Defining the Future, page Article ID",,,,Towards neurofeedback for improving visual attention,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.444.3269&rep=rep1&type=pdf,86,2013,/scholar?cites=8276451185080135256,DZ-fHPgAAAAJ:Ej9njvOgR2oC
14595,"We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.",Dominik Janzing and Jonas Peters and Eleni Sgouritsa and Kun Zhang and Joris M Mooij and Bernhard Schölkopf,6,14948580422377958899,,,1255-1262,,On causal and anticausal learning,http://scholar.google.com/scholar?cluster=14948580422377958899&hl=en&oi=scholarr,,2012,/scholar?cites=14948580422377958899,DZ-fHPgAAAAJ:EoFFR3w8WeYC
14596,"Empirical Inference is the process of drawing conclusions from observational data. For instance, the data can be measurements from an experiment, which are used by a researcher to infer a scientific law. Another kind of empirical inference is performed by living beings, continuously recording data from their environment and carrying out appropriate actions. Do these problems have anything in common, and are there underlying principles governing the extraction of regularities from data? What characterizes hard inference problems, and how can we solve them? Such questions are studied by a community of scientists from various fields, engaged in machine learning research.This short paper, which is based on the authorüs lecture to the scientific council of the Max Planck Society in February 2010, will attempt to describe some of the main ideas and problems of machine learning. It will provide illustrative …",Bernhard Schölkopf,6,10074840029589176855,International Journal of Materials Research,7,809-814,Carl Hanser Verlag,Empirical inference,https://www.hanser-elibrary.com/doi/pdf/10.3139/146.110530,102,2011,/scholar?cites=10074840029589176855,DZ-fHPgAAAAJ:SAZ1SQo2q1kC
14597,"Learning machines, such as support vector machines, are used to analyze datasets to recognize patterns within the dataset using kernels that are selected according to the nature of the data to be analyzed. Where the datasets possesses structural characteristics, locational kernels can be utilized to provide measures of similarity among data points within the dataset. The locational kernels are then combined to generate a decision function, or kernel, that can be used to analyze the dataset. Where an invariance transformation or noise is present, tangent vectors are defined to identify relationships between the invariance or noise and the data points. A covariance matrix is formed using the tangent vectors, then used in generation of the kernel.",,6,12206479647826534986,,,,,Kernels and methods for selecting kernels for use in learning machines,https://patents.google.com/patent/US7788193B2/en,,2010,/scholar?cites=12206479647826534986,DZ-fHPgAAAAJ:IWHjjKOFINEC
14598,"In this chapter, we are concerned with the problem of reconstructing patterns from their representation in feature space, known as the pre-image problem. We review existing algorithms and propose a learning-based approach. All algorithms are discussed regarding their usability and complexity, and evaluated on an image denoising application.",Gökhan Bakir and Bernhard Schölkopf and Jason Weston,6,8464271177947435690,,,284-302,IGI Global,On the pre-image problem in kernel methods,https://www.igi-global.com/chapter/pre-image-problem-kernel-methods/24828,,2007,/scholar?cites=8464271177947435690,DZ-fHPgAAAAJ:1sJd4Hv_s6UC
14599,"For IEEE to continue sending you helpful information on our products and services, please consent 
to our updated Privacy Policy … I have read and accepted the IEEE Privacy Policy … A 
not-for-profit organization, IEEE is the world's largest technical professional organization dedicated 
to advancing technology for the benefit of humanity. © Copyright 2019 IEEE - All rights 
reserved. Use of this web site signifies your agreement to the terms and conditions.  ",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,6,16493274553527043153,,,331-331,MIT Press,Semi-Supervised Learning in Practice,https://ieeexplore.ieee.org/abstract/document/6280910/,,2006,/scholar?cites=16493274553527043153,DZ-fHPgAAAAJ:62jtWhPur1oC
14600,,Sebastian Thrun and Lawrence K Saul and Bernhard Schölkopf,6,1633010456852150778,,,,MIT Press,"Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British Columbia, Canada]",http://scholar.google.com/scholar?cluster=1633010456852150778&hl=en&oi=scholarr,,2004,/scholar?cites=1633010456852150778,DZ-fHPgAAAAJ:5aZEtZeAH24C
14601,,B Schlkopf and AJ Smola,6,13932099543599685973,,,,"Cambridge, M assachusetts: MIT Press","Learning w ith Kernels Support Vec tor Machines: Regularization, Optimization and Beyond",http://scholar.google.com/scholar?cluster=13932099543599685973&hl=en&oi=scholarr,,2002,/scholar?cites=13932099543599685973,DZ-fHPgAAAAJ:aGf9zYFaS9MC
14602,"This chapter contains sections titled: Introduction, Tools from Functional Analysis, Convex Combinations of Parametric Families, Convex Combinations of Kernels, Multilayer Networks, Discussion, Appendix: A Remark on Traditional Weight Decay, Appendix: Proofs",Alexander J Smola and Peter Bartlett and Bernhard Schölkopf and Dale Schuurmans,6,1571611385231944666,,,369-387,MIT Press,Entropy numbers for convex combinations and MLPs,https://ieeexplore.ieee.org/abstract/document/6274985/,,2000,/scholar?cites=1571611385231944666,DZ-fHPgAAAAJ:wYSbTkB9UeAC
14603,,B Scholkopf and AJ Smola and K-R Muller and M Scholz and G Ratsch,6,12161145135262507271,Advances in Neural Information Processing Systems,,536-542,MIT; 1998,"Kernel PCA and De-Noising in Feature Spaces, Sebastian Mika",http://scholar.google.com/scholar?cluster=12161145135262507271&hl=en&oi=scholarr,,1999,/scholar?cites=12161145135262507271,DZ-fHPgAAAAJ:0sTkTiv_uMkC
14604,,Yair Weiss and Bernhard Schölkopf and John C Platt,6,6692273213544547384,,,,MIT Press,"Advances in Neural Information Processing Systems, volume 18, Cambridge, MA, 2006",http://scholar.google.com/scholar?cluster=6692273213544547384&hl=en&oi=scholarr,6,,/scholar?cites=6692273213544547384,DZ-fHPgAAAAJ:H_lu47GSfL8C
14605,"A multi‐coil shim setup is designed and optimized for human brain shimming. Here, the size and position of a set of square coils are optimized to improve the shim performance without increasing the number of local coils. Utilizing such a setup is especially beneficial at ultrahigh fields where B0 inhomogeneity in the human brain is more severe.The optimization started with a symmetric arrangement of 32 independent coils. Three parameters per coil were optimized in parallel, including angular and axial positions on a cylinder surface and size of the coil, which were constrained by cylinder size, construction consideration, and amplifiers specifications. B0 maps were acquired at 9.4T in 8 healthy volunteers for use as training data. The global and dynamic shimming performance of the optimized multi‐coil were compared in simulations and measurements to a symmetric design and to the scanner's …",Ali Aghaeifar and Jiazheng Zhou and Rahel Heule and Behzad Tabibian and Bernhard Schölkopf and Feng Jia and Maxim Zaitsev and Klaus Scheffler,5,15033291419631702911,Magnetic resonance in medicine,2,749-764,,A 32‐channel multi‐coil setup optimized for human brain shimming at 9.4 T,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27929,83,2020,/scholar?cites=15033291419631702911,DZ-fHPgAAAAJ:YzE6FWut7DQC
14606,"Parameter inference in ordinary differential equations is an important problem in many applied sciences and in engineering, especially in a data-scarce setting. In this work, we introduce a novel generative modeling approach based on constrained Gaussian processes and leverage it to build a computationally and data efficient algorithm for state and parameter inference. In an extensive set of experiments, our approach outperforms the current state of the art for parameter inference both in terms of accuracy and computational cost. It also shows promising results for the much more challenging problem of model selection.",Philippe Wenk and Gabriele Abbati and Michael A Osborne and Bernhard Schölkopf and Andreas Krause and Stefan Bauer,5,13205585180807713119,,,6364-6371,,ODIN: ODE-Informed Regression for Parameter and State Inference in Time-Continuous Dynamical Systems.,http://scholar.google.com/scholar?cluster=13205585180807713119&hl=en&oi=scholarr,,2020,/scholar?cites=13205585180807713119,DZ-fHPgAAAAJ:dJZfPYwQHqoC
14607,"We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets. We provide a scikit-learn compatible pip-installable Python package coroICA as well as R and Matlab implementations accompanied by a documentation at https://sweichwald. de/coroICA/.",Niklas Pfister and Sebastian Weichwald and Peter Bühlmann and Bernhard Schölkopf,5,6827562404844245254,Journal of Machine Learning Research,147,1-50,MIT Press,Robustifying independent component analysis by adjusting for group-wise stationary noise,http://www.jmlr.org/papers/volume20/18-399/18-399.pdf,20,2019,/scholar?cites=6827562404844245254,DZ-fHPgAAAAJ:8uGDF7sSpbkC
14608,"Consequential decisions are increasingly informed by sophisticated data-driven predictive models. However, consistently learning accurate predictive models requires access to ground truth labels. Unfortunately, in practice, labels may only exist conditional on certain decisions---if a loan is denied, there is not even an option for the individual to pay back the loan. In this paper, we show that, in this selective labels setting, learning to predict is suboptimal in terms of both fairness and utility. To avoid this undesirable behavior, we propose to directly learn stochastic decision policies that maximize utility under fairness constraints. In the context of fair machine learning, our results suggest the need for a paradigm shift from"" learning to predict"" to"" learning to decide"". Experiments on synthetic and real-world data illustrate the favorable properties of learning to decide, in terms of both utility and fairness.",Niki Kilbertus and Manuel Gomez-Rodriguez and Bernhard Schölkopf and Krikamol Muandet and Isabel Valera,5,5304610639392163541,,,,,Improving consequential decision making under imperfect predictions,https://openreview.net/forum?id=AhVMVR8GVUw,,2019,/scholar?cites=5304610639392163541,DZ-fHPgAAAAJ:mwTbor0XN8IC
14609,"We propose to fuse two currently separate research lines on novel therapies for stroke rehabilitation: brain-computer interface (BCI) training and transcranial electrical stimulation (TES). Specifically, we show that BCI technology can be used to learn personalized decoding models that relate the global configuration of brain rhythms in individual subjects (as measured by EEG) to their motor performance during 3D reaching movements. We demonstrate that our models capture substantial across-subject heterogeneity, and argue that this heterogeneity is a likely cause of limited effect sizes observed in TES for enhancing motor performance. We conclude by discussing how our personalized models can be used to derive optimal TES parameters, e.g., stimulation site and frequency, for individual patients.",Anastasia-Atalanti Mastakouri and Sebastian Weichwald and Ozan Özdenizci and Timm Meyer and Bernhard Schölkopf and Moritz Grosse-Wentrup,5,4952253407460283564,,,3024-3029,IEEE,Personalized brain-computer interface models for motor rehabilitation,https://ieeexplore.ieee.org/abstract/document/8123089/,,2017,/scholar?cites=4952253407460283564,DZ-fHPgAAAAJ:2LJqrQHhYIgC
14610,"Structural causal models (SCMs), also known as (non-parametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a well-studied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.1. Introduction. Structural causal models (SCMs), also known as (non-parametric) structural equation models (SEMs), are widely used for causal modeling purposes (Bollen, 1989; Spirtes, Glymour and Scheines, 2000; Pearl, 2009; Peters, Janzing …",Stephan Bongers and Patrick Forré and Jonas Peters and Bernhard Schölkopf and Joris M Mooij,5,11884599396741026376,arXiv preprint arXiv:1611.06221,,,,Foundations of structural causal models with cycles and latent variables,https://staff.science.uva.nl/j.m.mooij/articles/1611.06221v3.pdf,,2016,/scholar?cites=11884599396741026376,DZ-fHPgAAAAJ:HGKA08MZldMC
14611,"Causal inference uses observations to infer the causal structure of the data generating system. We study a class of functional models that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. There are two main contributions:(1) Theoretical: By restricting the model class (eg to additive noise) we can provide a more general identifiability result than existing ones. This result incorporates lagged and instantaneous effects that can be nonlinear and do not need to be faithful, and non-instantaneous feedbacks between the time series.(2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. When the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks is possible, but not discussed. It outperforms existing methods on artificial and real data. Code can be provided upon request.",Jonas Peters and Dominik Janzing and Bernhard Schölkopf,5,4833455479332554258,arXiv preprint arXiv:1207.5136,,,,Causal inference on time series using structural equation models,https://arxiv.org/abs/1207.5136,,2012,/scholar?cites=4833455479332554258,DZ-fHPgAAAAJ:LglpR7gjIOMC
14612,"Photographs taken with long exposure or high ISO setting may contain substantial amounts of noise, drastically reducing the Signal-To-Noise Ratio (SNR). This paper presents a novel optimization approach for denoising. It is based on a library of dark frames previously taken under varying conditions of temperature, ISO setting and exposure time, and a quality measure or prior for the class of images to denoise. The method automatically computes a synthetic dark frame that, when subtracted from an image, optimizes the quality measure. For specific choices of the quality measure, the denoising problem reduces to a quadratic programming (QP) problem that can be solved efficiently. We show experimentally that it is sufficient to consider a limited subsample of pixels when evaluating the quality measure in the optimization, in which case the complexity of the procedure does not depend on the size of the images but …",Manuel Gomez-Rodriguez and Jens Kober and Bernhard Schölkopf,5,4088346148812870572,,,1-9,IEEE,Denoising photographs using dark frames optimized by quadratic programming,https://ieeexplore.ieee.org/abstract/document/5559013/,,2009,/scholar?cites=4088346148812870572,DZ-fHPgAAAAJ:QUX0mv85b1cC
14613,"In this lecture: SSL = semi-supervised classification … 2 Why and How Does SSL Work? Generative 
Models The Semi-Supervised SVM (S3VM) Graph-Based Methods Further Approaches (incl. 
Co-Training, Transduction) … Why would unlabeled data be useful at all … Uniformly distributed 
data do not help. Must use properties of Pr (x) … 1. The data form clusters. 2. Points in the same 
cluster are likely to be of the same class … Don't confuse with the standard Supervised Learning 
Assumption: similar (ie nearby) points tend to have similar labels … Example: 2D view on handwritten 
digits 2, 4, 8 … [non-linear 2D-embedding with “Stochastic Neighbor Embedding”] … The cluster 
assumption seems to hold for many real data sets. Many SSL algorithms (implicitly) make use 
of it … 2 Why and How Does SSL Work? Generative Models The Semi-Supervised SVM 
(S3VM) Graph-Based Methods Further Approaches (incl. Co-Training, Transduction)  ",Alexander Zien and B Schölkopf and O Chapelle,5,17432518588543301169,,,,,Semi-supervised learning,http://23.21.248.66:10080/fml-migrated/raetsch/lectures/ssl-tutorial.pdf,,2006,/scholar?cites=17432518588543301169,DZ-fHPgAAAAJ:LWigWfLDON0C
14614,"A promising new combination in multimodality imaging is MR-PET, where the high soft tissue contrast of Magnetic Resonance Imaging (MRI) and the functional information of Positron Emission Tomography (PET) are combined. Although many technical problems have recently been solved, it is still an open problem to determine the attenuation map from the available MR scan, as the MR intensities are not directly related to the attenuation values. One standard approach is an atlas registration where the atlas MR image is aligned with the patient MR thus also yielding an attenuation image for the patient. We also propose another approach, which to our knowledge has not been tried before: Using Support Vector Machines we predict the attenuation value directly from the local image information. We train this well-established machine learning algorithm using small image patches. Although both approaches sometimes yielded acceptable results, they also showed their specific shortcomings: The registration often fails with large deformations whereas the prediction approach is problematic when the local image structure is not characteristic enough. However, the failures often do not coincide and integration of both information sources is promising. We therefore developed a combination method extending Support Vector Machines to use not only local image structure but also atlas registered coordinates. We demonstrate the strength of this combination approach on a number of examples.",M Hofmann and F Steinke and MS Judenhofer and CD Claussen and B Schoelkopf and BJ Pichler,5,16157672593508327975,,,,,A machine learning approach for determining the PET attenuation map from magnetic resonance images,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790727,,2006,/scholar?cites=16157672593508327975,DZ-fHPgAAAAJ:tOudhMTPpwUC
14615,"Our contribution will be short, but we will try to compensate by being particularly opinionated. The field of support vector machines (SVMs) and related kernel methods has produced an impressive range of theoretical results, algorithms and success stories in real-world applications. While it originated in machine learning, it is also concerned with core problems of statistics and it is thus timely to publish a comprehen sive article that discusses these methods from a statis tician's point of view. We shall use this opportunity to make a few general comments, largely about the field rather than about the present paper. Many papers about SVMs start off saying something like"" SVMs are great because they are based on sta tistical learning theory""(this probably includes some of our own writings). Moguerza and Mu? oz are more careful and only say that SVMs appeared in the context of statistical learning theory. What actually …",Olivier Bousquet and Bernhard Schölkopf,5,10476964224962538073,Statistical Science,3,337-340,Institute of Mathematical Statistics,Comment:[Support Vector Machines with Applications],https://www.jstor.org/stable/27645766,21,2006,/scholar?cites=10476964224962538073,DZ-fHPgAAAAJ:x_nEMsjMe4QC
14616,"We consider the general problem of learning from labeled and unlabeled data. Given a set of points, some of them are labeled, and the remaining points are unlabeled. The goal is to predict the labels of the unlabeled points. Any supervised learning algorithm can be applied to this problem, for instances, Support Vector Machines (SVMs). The problem of our interest is if we can implement a classifier which uses the unlabeled data information in some way and has higher accuracy than the classifiers which use the labeled data only. Here we propose a simple algorithm to utilize the unlabeled data. The basic idea is to construct the classifying function which sufficiently smooth with respect to the intrinsic global structure collectively revealed by known labeled and unlabeled points. The method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.",Dengyong Zhou and Olivier Bousquet and T Lal and Jason Weston and Bernhard Schölkopf,5,4202153267946397386,J. of Mach. Learn. Research,,,,Semi-supervised learning by maximizing smoothness,https://www.researchgate.net/profile/Olivier_Bousquet/publication/228526791_Semi-supervised_learning_by_maximizing_smoothness/links/004635219280f65d42000000/Semi-supervised-learning-by-maximizing-smoothness.pdf,,2004,/scholar?cites=4202153267946397386,DZ-fHPgAAAAJ:sSrBHYA8nusC
14617,,T Navin Lal and J Weston and D Zhou and O Bousquet and B Scholkopf,5,13842293954884855922,Advances in Neural Information Processing Systems (NIPS),,,,Learning with local and global consistency,http://scholar.google.com/scholar?cluster=13842293954884855922&hl=en&oi=scholarr,,2003,/scholar?cites=13842293954884855922,DZ-fHPgAAAAJ:9Ua_6ptbA6QC
14618,,Gunnar Ratsch,5,15151996380921387946,IEEE Transactions on Pattern Analysis and Machine Intelligence,9,1-16,,"Member, IEEE, Sebastian Mika, Bernhard Scholkopf, and Klaus-Robert Muller. Constructing Boosting Algorithms form SVMs: An Application to One-Class Classification",http://scholar.google.com/scholar?cluster=15151996380921387946&hl=en&oi=scholarr,24,2002,/scholar?cites=15151996380921387946,DZ-fHPgAAAAJ:g5h4crcM1N4C
14619,,S Bernhard and CC Burges and AJ Smola and C Massachusetts,5,8499925252476704472,,,255-268,"The MIT Press, C. Massachusetts, London England",Pairwise classification and support vector machines,http://scholar.google.com/scholar?cluster=8499925252476704472&hl=en&oi=scholarr,,1999,/scholar?cites=8499925252476704472,DZ-fHPgAAAAJ:3U6lgfQs5JwC
14620,,Robert C Williamson and Alexander J Smola and Bernhard Schölkopf,5,7393185379473089224,,,,"MIT Press, Cambridge, MA","Entropy numbers, operators and support vector kernels, Advances in kernel methods: support vector learning",http://scholar.google.com/scholar?cluster=7393185379473089224&hl=en&oi=scholarr,,1999,/scholar?cites=7393185379473089224,DZ-fHPgAAAAJ:97OsqvYTX2EC
14621,"Author: Schölkopf, B; Genre: Book Chapter; Published in Print: 1998; Title: Support-Vektor-Lernen.
",B Schölkopf,5,2330667359337701304,,,135-150,Teubner,Support-vektor-lernen,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1794076,,1998,/scholar?cites=2330667359337701304,DZ-fHPgAAAAJ:QYmifXMdJWgC
14622,"Methodological advances in metagenome assembly are rapidly increasing in the number of published metagenome assemblies. However, identifying misassemblies is challenging due to a lack of closely related reference genomes that can act as pseudo ground truth. Existing reference-free methods are no longer maintained, can make strong assumptions that may not hold across a diversity of research projects, and have not been validated on large-scale metagenome assemblies.We present DeepMAsED, a deep learning approach for identifying misassembled contigs without the need for reference genomes. Moreover, we provide an in silico pipeline for generating large-scale, realistic metagenome assemblies for comprehensive model training and testing. DeepMAsED accuracy substantially exceeds the state-of-the-art when applied to large and complex …",Olga Mineeva and Mateo Rojas-Carulla and Ruth E Ley and Bernhard Schölkopf and Nicholas D Youngblut,4,1310747797435502972,Bioinformatics,10,3011-3017,Oxford University Press,DeepMAsED: evaluating the quality of metagenomic assemblies,https://academic.oup.com/bioinformatics/article-abstract/36/10/3011/5756210,36,2020,/scholar?cites=1310747797435502972,DZ-fHPgAAAAJ:zrJuNqZCf_4C
14623,"Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (eg, only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines.",Amir-Hossein Karimi and Bodo Julius von Kügelgen and Bernhard Schölkopf and Isabel Valera,4,4986898048327715369,Advances in Neural Information Processing Systems,,,,Algorithmic recourse under imperfect causal knowledge: a probabilistic approach,http://proceedings.neurips.cc/paper/2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html,33,2020,/scholar?cites=4986898048327715369,DZ-fHPgAAAAJ:si2k7ziEMA4C
14624,"Contrast enhancement is an important preprocessing technique for improving the performance of downstream tasks in image processing and computer vision. Among the existing approaches based on nonlinear histogram transformations, contrast limited adaptive histogram equalization (CLAHE) is a popular choice for dealing with 2D images obtained in natural and scientific settings. The recent hardware upgrade in data acquisition systems results in significant increase in data complexity, including their sizes and dimensions. Measurements of densely sampled data higher than three dimensions, usually composed of 3D data as a function of external parameters, are becoming commonplace in various applications in the natural sciences and engineering. The initial understanding of these complex multidimensional datasets often requires human intervention through visual examination, which may be hampered by …",Vincent Stimper and Stefan Bauer and Ralph Ernstorfer and Bernhard Schölkopf and Rui Patrick Xian,4,815499739840320684,IEEE Access,,165437-165447,IEEE,Multidimensional contrast limited adaptive histogram equalization,https://ieeexplore.ieee.org/abstract/document/8895993/,7,2019,/scholar?cites=815499739840320684,DZ-fHPgAAAAJ:pPOkAXTFjuAC
14625,"A novel method for the acceleration of MRI acquisition is proposed that relies on the local modulation of magnetic fields. These local modulations provide additional spatial information for image reconstruction that is used to accelerate image acquisition.In experiments and simulations, eight local coils connected to current amplifiers were used for rapid local magnetic field variation. Acquired and simulated data were reconstructed to quantify reconstruction errors as a function of the acceleration factor and applied modulation frequency and strength.Experimental results demonstrate a possible acceleration factor of 2 to 4. Simulations demonstrate the challenges and limits of this method in terms of required magnetic field modulation strengths and frequencies. A normalized mean squared error of below 10% can be achieved for acceleration factors of up to 8 using modulation field …",Klaus Scheffler and Alexander Loktyushin and Jonas Bause and Ali Aghaeifar and Theodor Steffen and Bernhard Schölkopf,4,14707927063978667041,Magnetic resonance in medicine,3,877-885,,Spread‐spectrum magnetic resonance imaging,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27766,82,2019,/scholar?cites=14707927063978667041,DZ-fHPgAAAAJ:Js6xCJvs_BkC
14626,"We address the problem of inferring the causal direction between two variables by comparing the least-squares errors of the predictions in both possible directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable algorithm that only requires a regression in both possible causal directions and a comparison of the errors. The performance of the algorithm is compared with various related causal inference methods in different artificial and real-world data sets.",Patrick Blöbaum and Dominik Janzing and Takashi Washio and Shohei Shimizu and Bernhard Schölkopf,4,12218275229985887640,PeerJ Computer Science,,e169,PeerJ Inc.,Analysis of cause-effect inference by comparing regression errors,https://peerj.com/articles/cs-169/?utm_source=TrendMD&utm_campaign=PeerJ_TrendMD_1&utm_medium=TrendMD,5,2019,/scholar?cites=12218275229985887640,DZ-fHPgAAAAJ:ACkdcFGKRjAC
14627,"We propose a constraint-based causal feature selection method for identifying causes of a given target variable, selecting from a set of candidate variables, while there can also be hidden variables acting as common causes with the target. We prove that if we observe a cause for each candidate cause, then a single conditional independence test with one conditioning variable is sufficient to decide whether a candidate associated with the target is indeed causing it. We thus improve upon existing methods by significantly simplifying statistical testing and requiring a weaker version of causal faithfulness. Our main assumption is inspired by neuroscience paradigms where the activity of a single neuron is considered to be also caused by its own previous state. We demonstrate successful application of our method to simulated, as well as encephalographic data of twenty-one participants, recorded in Max Planck Institute for intelligent Systems. The detected causes of motor performance are in accordance with the latest consensus about the neurophysiological pathways, and can provide new insights into personalised brain stimulation.",Atalanti Mastakouri and Bernhard Schölkopf and Dominik Janzing,4,17706790894231104063,,,12553-12564,,Selecting causal brain features with a single conditional independence test per feature,http://papers.nips.cc/paper/9419-selecting-causal-brain-features-with-a-single-conditional-independence-test-per-feature,,2019,/scholar?cites=17706790894231104063,DZ-fHPgAAAAJ:KiiG_FXkR9MC
14628,"We address the problem of non-parametric multiple model comparison: given  candidate models, decide whether each candidate is as good as the best one (s) or worse than it. We propose two statistical tests, each controlling a different notion of decision errors. The first test, building on the post selection inference framework, provably controls the number of best models that are wrongly declared worse (false positive rate). The second test is based on multiple correction, and controls the proportion of the models declared worse but are in fact as good as the best (false discovery rate). We prove that under appropriate conditions the first test can yield a higher true positive rate than the second. Experimental results on toy and real (CelebA, Chicago Crime data) problems show that the two tests have high true positive rates with well-controlled error rates. By contrast, the naive approach of choosing the model with the lowest score without correction leads to more false positives.",Jen Ning Lim and Makoto Yamada and Bernhard Schölkopf and Wittawat Jitkrittum,4,8758698782174042861,,,2243-2253,,Kernel Stein Tests for Multiple Model Comparison,http://papers.nips.cc/paper/8496-kernel-stein-tests-for-multiple-model-comparison,,2019,/scholar?cites=8758698782174042861,DZ-fHPgAAAAJ:W6lDjji-Y3sC
14629,"We consider a bivariate time series  that is given by a simple linear autoregressive model. Assuming that the equations describing each variable as a linear combination of past values are considered structural equations, there is a clear meaning of how intervening on one particular  influences  at later times . In the present work, we describe conditions under which one can define a causal model between variables that are coarse-grained in time, thus admitting statements likesetting  to  changes  in a certain way'without referring to specific time instances. We show that particularly simple statements follow in the frequency domain, thus providing meaning to interventions on frequencies.",Dominik Janzing and Paul Rubenstein and Bernhard Schölkopf,4,6130659491956421559,arXiv preprint arXiv:1804.03911,,,,Structural causal models for macro-variables in time-series,https://arxiv.org/abs/1804.03911,,2018,/scholar?cites=6130659491956421559,DZ-fHPgAAAAJ:YeN430KGy3cC
14630,"We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders.",Paul K Rubenstein and Bernhard Schölkopf and Ilya Tolstikhin,4,5903790017355735864,,,,,Wasserstein auto-encoders: Latent dimensionality and random encoders,https://openreview.net/forum?id=r157GIJvz,,2018,/scholar?cites=5903790017355735864,DZ-fHPgAAAAJ:hNeVYFDBxzMC
14631,,Friedrich Solowjow and Arash Mehrjou and B Scholkopf and Sebastian Trimpe,4,2786669588900311671,57th IEEE Conference on Decision and Control (CDC).[Electronic resource],,,,Minimum Information Exchange in Distributed Systems,http://scholar.google.com/scholar?cluster=2786669588900311671&hl=en&oi=scholarr,,2018,/scholar?cites=2786669588900311671,DZ-fHPgAAAAJ:YzDdRQD-RXIC
14632,,Prateek Katiyar and Mathew R Divine and Ursula Kohlhofer and Leticia Quintanilla-Martinez and Bernhard Schölkopf and Bernd J Pichler and Jonathan A Disselhorst,4,12542514200807428148,Journal of Nuclear Medicine,4,651-657,Society of Nuclear Medicine,Spectral clustering predicts tumor tissue heterogeneity using dynamic 18F-FDG PET: a complement to the standard compartmental modeling approach,http://jnm.snmjournals.org/content/58/4/651.short,58,2017,/scholar?cites=12542514200807428148,DZ-fHPgAAAAJ:qdVtQq3Eg_YC
14633,"Light field photography captures rich structural information that may facilitate a number of traditional image processing and computer vision tasks. A crucial ingredient in such endeavors is accurate depth recovery. We present a novel framework that allows the recovery of a high quality continuous depth map from light field data. To this end we propose a generative model of a light field that is fully parametrized by its corresponding depth map. The model allows for the integration of powerful regularization techniques such as a non-local means prior, facilitating accurate depth map estimation. Comparisons with previous methods show that we are able to recover faithful depth maps with much finer details. In a number of challenging real-world examples we demonstrate both the effectiveness and robustness of our approach.",Mehdi SM Sajjadi and Rolf Köhler and Bernhard Schölkopf and Michael Hirsch,4,7787269034494770498,,,426-438,"Springer, Cham",Depth estimation through a generative model of light field synthesis,https://link.springer.com/chapter/10.1007/978-3-319-45886-1_35,,2016,/scholar?cites=7787269034494770498,DZ-fHPgAAAAJ:rDC_CG-fcEMC
14634,"The amount of digitally available but heterogeneous information about the world is remarkable, and new technologies such as self-driving cars, smart homes, or the internet of things may further increase it. In this paper we present preliminary ideas about certain aspects of the problem of how such heterogeneous information can be harnessed by autonomous agents. After discussing potentials and limitations of some existing approaches, we investigate how\emph {experiments} can help to obtain a better understanding of the problem. Specifically, we present a simple agent that integrates video data from a different agent, and implement and evaluate a version of it on the novel experimentation platform\emph {Malmo}. The focus of a second investigation is on how information about the hardware of different agents, the agents' sensory data, and\emph {causal} information can be utilized for knowledge transfer between agents and subsequently more data-efficient decision making. Finally, we discuss potential future steps wrt\theory and experimentation, and formulate open questions.",Philipp Geiger and Katja Hofmann and Bernhard Schölkopf,4,14021933540062421149,arXiv preprint arXiv:1606.04250,,,,Experimental and causal view on information integration in autonomous agents,https://arxiv.org/abs/1606.04250,,2016,/scholar?cites=14021933540062421149,DZ-fHPgAAAAJ:XBKgkpziyI4C
14635,"Introduction: Brain-computer interfaces (BCIs) are often based on the control of sensorimotor processes, yet sensorimotor processes are impaired in patients suffering from amyotrophic lateral sclerosis (ALS)[1]. Previously, we devised a novel paradigm that targets higher-level cognitive processes to transmit information from the user to the BCI [2]. The current work describes a refined version of this paradigm. We instructed five ALS patients (table 1) and eleven healthy subjects (6 female, mean age 28 years±7.5) to either activate selfreferential memories by thinking of a positive memory, or to focus on a mental subtraction task, while recording a high-density electroencephalogram (EEG). We argue that both memories [3] and mental calculations [4] are likely to modulate activity in the default mode network (DMN) without involving sensorimotor pathways.",MR Hohmann and T Fomina and V Jayaram and C Förster and J Just and M Synofzik and B Schölkopf and L Schöls and M Grosse-Wentrup,4,4839617513467439365,Proc. of the 6th Int. BCI Meeting,,,,An improved cognitive brain-computer interface for patients with amyotrophic lateral sclerosis,http://scholar.google.com/scholar?cluster=4839617513467439365&hl=en&oi=scholarr,,2016,/scholar?cites=4839617513467439365,DZ-fHPgAAAAJ:WIE79rvcIw4C
14636,"There has been a considerable progress recently in understanding and developing solutions to the problem of image quality deterioration due to patients’ motion in MR scanners. Retrospective methods can be applied to previously acquired motion corrupted data, however, such methods require complex-valued raw volumes as input. It is common practice, though, to preserve only spatial magnitudes of the medical scans, which makes the existing post-processing-based approaches inapplicable. In this work, we make first humble steps towards solving the problem of motion-related artifacts in magnitude-only scans. We propose a learning-based approach, which involves using large-scale convolutional neural networks to learn the transformation from motion-corrupted magnitude observations to the sharp images.",Alexander Loktyushin and Christian Schuler and Klaus Scheffler and Bernhard Schölkopf,4,13185754198062171149,,,3-12,"Springer, Cham",Retrospective motion correction of magnitude-input MR images,https://link.springer.com/chapter/10.1007/978-3-319-27929-9_1,,2015,/scholar?cites=13185754198062171149,DZ-fHPgAAAAJ:bX5SDva7FKsC
14637,"Electroencephalography often fails to assess both the level (i.e. arousal) and the content (i.e. awareness) of pathologically altered consciousness in patients without motor responsiveness. This might be related to a decline of awareness, to episodes of low arousal and disturbed sleep patterns, and/or to distorting and attenuating effects of the skull and intermediate tissue on the recorded brain signals. Novel approaches are required to overcome these limitations. We introduced epidural electrocorticography (ECoG) for monitoring of cortical physiology in a late-stage amytrophic lateral sclerosis patient in completely locked-in state. Despite long-term application for a period of six months, no implant-related complications occurred. Recordings from the left frontal cortex were sufficient to identify three arousal states. Spectral analysis of the intrinsic oscillatory activity enabled us to extract state-dependent dominant frequencies at < 4, ~ 7 and ~ 20 Hz, representing sleep-like periods, and phases of low and elevated arousal, respectively. In the absence of other biomarkers, ECoG proved to be a reliable tool for monitoring circadian rhythmicity, i.e. avoiding interference with the patient when he was sleeping and exploiting time windows of responsiveness. Moreover, the effects of interventions addressing the patient’s arousal, e.g. amantadine medication, could be evaluated objectively on the basis of physiological markers, even in the absence of behavioral parameters. Epidural ECoG constitutes a feasible trade-off between surgical risk and quality of recorded brain signals to gain information on the patient’s present level of arousal. This approach enables …",Suzanne Martens and Michael Bensch and Sebastian Halder and Jeremy Hill and Femke Nijboer and Ander Ramos-Murguialday and Bernhard Schoelkopf and Niels Birbaumer and Alireza Gharabaghi,4,10086482871492331734,Frontiers in human neuroscience,,861,Frontiers,Epidural electrocorticography for monitoring of arousal in locked-in state,https://www.frontiersin.org/articles/10.3389/fnhum.2014.00861/full,8,2014,/scholar?cites=10086482871492331734,DZ-fHPgAAAAJ:71d7Y1FijdoC
14638,"We consider the problem of function estimation in the case where the data distribution may shift between training and test time, and additional information about it may be available at test time. This relates to popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. This working paper discusses how these tasks could be tackled depending on the kind of changes of the distributions. It argues that knowledge of an underlying causal direction can facilitate several of these tasks.",Bernhard Schölkopf and Dominik Janzing and Jonas Peters and Kun Zhang,4,16242815930412304479,arXiv preprint arXiv:1112.2738,,,,Robust learning via cause-effect models,https://arxiv.org/abs/1112.2738,,2011,/scholar?cites=16242815930412304479,DZ-fHPgAAAAJ:Xz60mAmATU4C
14639,,S Hanneling and M Hirsch and S Sra and Be SchOlkopf,4,3579691126445361860,,,1-7,,Online blind deconvolution for astronomical imaging,http://scholar.google.com/scholar?cluster=3579691126445361860&hl=en&oi=scholarr,,2009,/scholar?cites=3579691126445361860,DZ-fHPgAAAAJ:9BrBD10KdIMC
14640,"We propose two kernel based methods for detecting the time direction in empirical time series. First we apply a Support Vector Machine on the finite-dimensional distributions of the time series (classification method) by embedding these distributions into a Reproducing Kernel Hilbert Space. For the ARMA method we fit the observed data with an autoregressive moving average process and test whether the regression residuals are statistically independent of the past values. Whenever the dependence in one direction is significantly weaker than in the other we infer the former to be the true one. Both approaches were able to detect the direction of the true generating model for simulated data sets. We also applied our tests to a large number of real world time series. The ARMA method made a decision for a significant fraction of them, in which it was mostly correct, while the classification method did not …",Jonas Peters and Dominik Janzing and Arthur Gretton and Bernhard Schölkopf,4,11696852275654068685,,,57-66,"Springer, Berlin, Heidelberg",Kernel methods for detecting the direction of time series,https://link.springer.com/chapter/10.1007/978-3-642-01044-6_5,,2009,/scholar?cites=11696852275654068685,DZ-fHPgAAAAJ:XiSMed-E-HIC
14641,"Systems and methods for object or pattern detection that use a nonlinear support vector (SV) machine are described. In the illustrated and described embodiment, objects or patterns comprising faces are detected. The decision surface is approximated in terms of a reduced set of expansion vectors. In order to determine the presence of a face, the kernelized inner product of the expansion vectors with the input pattern are sequentially evaluated and summed, such that if at any point the pattern can be rejected as not comprising a face, no more expansion vectors are used. The sequential application of the expansion vectors produces a substantial saving in computational time.",,4,3011166745360732251,,,,,Pattern detection using reduced set vectors,https://patents.google.com/patent/US7391908B2/en,,2008,/scholar?cites=3011166745360732251,DZ-fHPgAAAAJ:_Qo2XoVZTnwC
14642,"For quantitative PET information, correction of tissue photon attenuation is mandatory. Generally in conventional PET, the attenuation map is obtained from a transmission scan, which uses a rotating radionuclide source, or from the CT scan in a combined PET/CT scanner. In the case of PET/MRI scanners currently under development, insufficient space for the rotating source exists; the attenuation map can be calculated from the MR image instead. This task is challenging because MR intensities correlate with proton densities and tissue-relaxation properties, rather than with attenuation-related mass density. Methods: We used a combination of local pattern recognition and atlas registration, which captures global variation of anatomy, to predict pseudo-CT images from a given MR image. These pseudo-CT images were then used for attenuation correction, as the process would be performed in a PET/CT scanner. Results: For human brain scans, we show on a database of 17 MR/CT image pairs that our method reliably enables estimation of a pseudo-CT image from the MR image alone. On additional datasets of MRI/PET/CT triplets of human brain scans, we compare MRI-based attenuation correction with CT-based correction. Our approach enables PET quantification with a mean error of 3.2% for predefined regions of interest, which we found to be clinically not significant. However, our method is not specific to brain imaging, and we show promising initial results on 1 whole-body animal dataset. Conclusion: This method allows reliable MRI-based attenuation correction for human brain scans. Further work is necessary to validate the method for …",Matthias HOFMANNL and Florian STEINKE and Verena SCHEEL and Guillaume CHARPIAT and Jason FARQUHAR and Philip ASCHOFF and Michael BRADY and Bemhard SCHOLKOPF and Bernd J PICHLER,4,15612392445066944864,The Journal of nuclear medicine,11,1875-1883,Society of Nuclear Medicine,MRI-Based Attenuation Correction for PET,http://scholar.google.com/scholar?cluster=15612392445066944864&hl=en&oi=scholarr,49,2008,/scholar?cites=15612392445066944864,DZ-fHPgAAAAJ:Z4mSzhzztHAC
14643,"Systems and methods for object or pattern detection that use a nonlinear support vector (SV) machine are described. In the illustrated and described embodiment, objects or patterns comprising faces are detected. The decision surface is approximated in terms of a reduced set of expansion vectors. In order to determine the presence of a face, the kernelized inner product of the expansion vectors with the input pattern are sequentially evaluated and summed, such that if at any point the pattern can be rejected as not comprising a face, no more expansion vectors are used. The sequential application of the expansion vectors produces a substantial saving in computational time.",,4,8733885806562226827,,,,,Pattern detection,https://patents.google.com/patent/US7236626B2/en,,2007,/scholar?cites=8733885806562226827,DZ-fHPgAAAAJ:4JMBOYKVnBMC
14644,,Alex Smola and Arthur Gretton,4,12119489040178344212,A Hilbert Space Embedding for Distributions In Algorithmic Learning Theory,,,,"Le Song, and Bernhard Scholkopf",http://scholar.google.com/scholar?cluster=12119489040178344212&hl=en&oi=scholarr,,2007,/scholar?cites=12119489040178344212,DZ-fHPgAAAAJ:24NoylL6oHUC
14645,,Alex Smola and Arthur Gretton,4,12119489040178344212,A Hilbert Space Embedding for Distributions In Algorithmic Learning Theory,,,,"Le Song, and Bernhard Scholkopf",http://scholar.google.com/scholar?cluster=12119489040178344212&hl=en&oi=scholarr,,2007,/scholar?cites=12119489040178344212,DZ-fHPgAAAAJ:S8f_GPVCfywC
14646,"Autonomous robots that can adapt to novel situations has been a long standing vision of robotics, artificial intelligence, and cognitive sciences. Early approaches to this goal during the heydays of artificial intelligence research in the late 1980s, however, made it clear that an approach purely based on reasoning or human insights would not be able to model all the perceptuomotor tasks that a robot should fulfill. Instead, new hope was put in the growing wake of machine learning that promised fully adaptive control algorithms which learn both by observation and trial-and-error. However, to date, learning techniques have yet to fulfill this promise as only few methods manage to scale into the high-dimensional domains of manipulator robotics, or even the new upcoming trend of humanoid robotics, and usually scaling was only achieved in precisely pre-structured domains. In this paper, we investigate the …",Jan Peters and Stefan Schaal and Bernhard Schölkopf,4,15427523925636304819,,,138-144,"Springer, Berlin, Heidelberg",Towards machine learning of motor skills,https://link.springer.com/chapter/10.1007/978-3-540-74764-2_22,,2007,/scholar?cites=15427523925636304819,DZ-fHPgAAAAJ:1yQoGdGgb4wC
14647,"This chapter contains sections titled: Problem Settings, Problem of Generalization in Inductive and Transductive Inference, Structure of the VC Bounds and Transductive Inference, The Symmetrization Lemma and Transductive Inference, Bounds for Transductive Inference, The Structural Risk Minimization Principle for Induction and Transduction, Combinatorics in Transductive Inference, Measures of the Size of Equivalence Classes, Algorithms for Inductive and Transductive SVMs, Semi-Supervised Learning, Conclusion: Transductive Inference and the New Problems of Inference, Beyond Transduction: Selective Inference",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,4,12325379437809989921,,,453-472,MIT Press,Transductive Inference and Semi-Supervised Learning,https://ieeexplore.ieee.org/abstract/document/6280886/,,2006,/scholar?cites=12325379437809989921,DZ-fHPgAAAAJ:2--08GF19OoC
14648,"This chapter assesses the strengths and weaknesses of different semi-supervised learning (SSL) algorithms through inviting the authors of each chapter in this book to apply their algorithms to eight benchmark data sets. These data sets encompass both artificial and real-world problems. Details are provided on how the algorithms were applied, especially how hyperparameters were chosen given the few labeled points. Finally, the chapter concludes by presenting and discussing the empirical performance.",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,4,8409127925623788840,,,377-393,MIT Press,Analysis of Benchmarks,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3158949,,2006,/scholar?cites=8409127925623788840,DZ-fHPgAAAAJ:KqW5X_olkfQC
14649,"We consider the problem of fitting a linear operator induced equation to point sampled data. In order to do so we systematically exploit the duality between minimizing a regularization functional derived from an operator and kernel regression methods. Standard machine learning model selection algorithms can then be interpreted as a search of the equation best fitting given data points. For many kernels this operator induced equation is a linear differential equation. Thus, we link a continuous-time system identification task with common machine learning methods.The presented link opens up a wide variety of methods to be applied to this system identification problem. In a series of experiments we demonstrate an example algorithm working on non-uniformly spaced data, giving special focus to the problem of identifying one system from multiple data recordings.",Florian Steinke and Bernhard Schölkopf,4,9530451643528349817,IFAC Proceedings Volumes,1,1192-1197,Elsevier,Machine learning methods for estimating operator equations,https://www.sciencedirect.com/science/article/pii/S1474667015354288,39,2006,/scholar?cites=9530451643528349817,DZ-fHPgAAAAJ:fQNAKQ3IYiAC
14650,"The aim of research into brain-computer interfaces (BCI) is to allow a person to control a computer using signals from the brain, without the need for any muscular movement—for example, to allow a totally paralyzed patient to communicate. We report the results of an experiment on normal subjects, designed to develop a paradigm in which a user can make a binary choice. Other researchers have employed paradigms in which users imagine moving either their left or right hand [1], or in which they are trained over several weeks to control slow cortical potentials [2]. Here, we attempt to classify EEG signals that occur in response to two simultaneous auditory stimulus streams. To communicate a binary decision, the subject focuses attention on one of the two streams, left or right.",J Hill and T Lal and Michael Schröder and Thilo Hinterberger and Niels Birbaumer and Bernhard Schölkopf,4,10265978896732993784,Proc. 7th Tübingen Perception Conf,,102,,Selective attention to auditory stimuli: A brain-computer interface paradigm,https://www.researchgate.net/profile/N_Hill2/publication/203917956_Selective_Attention_to_Auditory_Stimuli_A_Brain-Computer_Interface_Paradigm/links/02e7e539621fa6d325000000.pdf,,2004,/scholar?cites=10265978896732993784,DZ-fHPgAAAAJ:eJXPG6dFmWUC
14651,"We give an exposition of the ideas of statistical learning theory, followed by a discussion of how a reinterpretation of the insights of learning theory could potentially also benefit our understanding of a certain notion of complexity. © 2003 Wiley Periodicals, Inc.",Bernhard Schölkopf,4,14849349523478590620,Complexity,4,87-94,"Wiley Subscription Services, Inc., A Wiley Company","Statistical learning theory, capacity, and complexity",https://onlinelibrary.wiley.com/doi/abs/10.1002/cplx.10094,8,2003,/scholar?cites=14849349523478590620,DZ-fHPgAAAAJ:ZuybSZzF8UAC
14652,"Author: Gretton, A et al.; Genre: Report; Published in Print: 2001; Title: Bound on the
Leave-One-Out Error for Density Support Estimation using nu-SVMs.
",A Gretton and R Herbrich and B Schoelkopf and AJ Smola and PJW Rayner,4,12877225813604360145,,,,,Bound on the Leave-One-Out Error for Density Support Estimation using nu-SVMs,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1793345,,2001,/scholar?cites=12877225813604360145,DZ-fHPgAAAAJ:HoDPlbN_d1QC
14653,,Dengyong Zhou and Olivier Bousquet,4,8949870721822233082,,,321-328,NIPS,"omas Navin Lal, Jason Weston, and Bernhard Schölkopf. 2003. Learning with local and global consistency",http://scholar.google.com/scholar?cluster=8949870721822233082&hl=en&oi=scholarr,16,,/scholar?cites=8949870721822233082,DZ-fHPgAAAAJ:rXl_JIDp3voC
14654,"Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.",Amir-Hossein Karimi and Gilles Barthe and Bernhard Schölkopf and Isabel Valera,3,16535741343718954415,arXiv preprint arXiv:2010.04050,,,,"A survey of algorithmic recourse: definitions, formulations, solutions, and prospects",https://arxiv.org/abs/2010.04050,,2020,/scholar?cites=16535741343718954415,DZ-fHPgAAAAJ:eqBW-PYUpt8C
14655,"Dynamic tasks like table tennis are relatively easy to learn for humans but pose significant challenges to robots. Such tasks require accurate control of fast movements and precise timing in the presence of imprecise state estimation of the flying ball and the robot. Reinforcement Learning (RL) has shown promise in learning of complex control tasks from data. However, applying step-based RL to dynamic tasks on real systems is safety-critical as RL requires exploring and failing safely for millions of time steps in high-speed regimes. In this paper, we demonstrate that safe learning of table tennis using model-free Reinforcement Learning can be achieved by using robot arms driven by pneumatic artificial muscles (PAMs). Softness and back-drivability properties of PAMs prevent the system from leaving the safe region of its state space. In this manner, RL empowers the robot to return and smash real balls with 5 m\s and 12m\s on average to a desired landing point. Our setup allows the agent to learn this safety-critical task (i) without safety constraints in the algorithm,(ii) while maximizing the speed of returned balls directly in the reward function (iii) using a stochastic policy that acts directly on the low-level controls of the real system and (iv) trains for thousands of trials (v) from scratch without any prior knowledge. Additionally, we present HYSR, a practical hybrid sim and real training that avoids playing real balls during training by randomly replaying recorded ball trajectories in simulation and applying actions to the real robot. This work is the first to (a) fail-safe learn of a safety-critical dynamic task using anthropomorphic robot arms,(b) learn a precision …",Dieter Büchler and Simon Guist and Roberto Calandra and Vincent Berenz and Bernhard Schölkopf and Jan Peters,3,627898812123284085,arXiv preprint arXiv:2006.05935,,,,Learning to Play Table Tennis From Scratch using Muscular Robots,https://arxiv.org/abs/2006.05935,,2020,/scholar?cites=627898812123284085,DZ-fHPgAAAAJ:LNKHhW0-MVwC
14656,"We point out an example of Simpson's paradox in COVID-19 case fatality rates (CFRs): comparing data from  cases from China with data from Italy reported on March 9th, we find that CFRs are lower in Italy for each age group, but higher overall. This phenomenon can be explained by a stark difference in case demographic between the two countries. Using this as a motivating example, we review basic concepts from mediation analysis and show how these can be used to quantify different direct and indirect effects when assuming a coarse-grained causal graph involving country, age, and mortality. As a case study, we then investigate how total, direct, and indirect (age-mediated) causal effects between China and Italy evolve over two months until May 7th 2020.",Julius von Kügelgen and Luigi Gresele and Bernhard Schölkopf,3,9971205931724049646,arXiv preprint arXiv:2005.07180,,,,Simpson's paradox in Covid-19 case fatality rates: a mediation analysis of age-related causal effects,https://arxiv.org/abs/2005.07180,,2020,/scholar?cites=9971205931724049646,DZ-fHPgAAAAJ:02yGKXRObHkC
14657,"Learning how to model complex scenes in a modular way with recombinable components is a pre-requisite for higher-order reasoning and acting in the physical world. However, current generative models lack the ability to capture the inherently compositional and layered nature of visual scenes. While recent work has made progress towards unsupervised learning of object-based scene representations, most models still maintain a global representation space (ie, objects are not explicitly separated), and cannot generate scenes with novel object arrangement and depth ordering. Here, we present an alternative approach which uses an inductive bias encouraging modularity by training an ensemble of generative models (experts). During training, experts compete for explaining parts of a scene, and thus specialise on different object classes, with objects being identified as parts that re-occur across multiple scenes. Our model allows for controllable sampling of individual objects and recombination of experts in physically plausible ways. In contrast to other methods, depth layering and occlusion are handled correctly, moving this approach closer to a causal generative scene model. Experiments on simple toy data qualitatively demonstrate the conceptual advantages of the proposed approach.",Julius von Kügelgen and Ivan Ustyuzhaninov and Peter Gehler and Matthias Bethge and Bernhard Schölkopf,3,18181271910707081975,arXiv preprint arXiv:2004.12906,,,,Towards causal generative scene models via competition of experts,https://arxiv.org/abs/2004.12906,,2020,/scholar?cites=18181271910707081975,DZ-fHPgAAAAJ:i-2NmMiFHv0C
14658,"Reinforcement learning with sparse rewards is still an open challenge. Classic methods rely on getting feedback via extrinsic rewards to train the agent, and in situations where this occurs very rarely the agent learns slowly or cannot learn at all. Similarly, if the agent receives also rewards that create suboptimal modes of the objective function, it will likely prematurely stop exploring. More recent methods add auxiliary intrinsic rewards to encourage exploration. However, auxiliary rewards lead to a non-stationary target for the Q-function. In this paper, we present a novel approach that (1) plans exploration actions far into the future by using a long-term visitation count, and (2) decouples exploration and exploitation by learning a separate function assessing the exploration value of the actions. Contrary to existing methods which use models of reward and dynamics, our approach is off-policy and model-free. We further propose new tabular environments for benchmarking exploration in reinforcement learning. Empirical results on classic and novel benchmarks show that the proposed approach outperforms existing methods in environments with sparse rewards, especially in the presence of rewards that create suboptimal modes of the objective function. Results also suggest that our approach scales gracefully with the size of the environment. Source code is available at this https URL",Simone Parisi and Davide Tateo and Maximilian Hensel and Carlo D'Eramo and Jan Peters and Joni Pajarinen,3,951940782694980921,arXiv preprint arXiv:2001.00119,,,,Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning,https://arxiv.org/abs/2001.00119,,2020,/scholar?cites=951940782694980921,DZ-fHPgAAAAJ:LTqI5EznaiYC
14659,"Learning expressive probabilistic models correctly describing the data is a ubiquitous problem in machine learning. A popular approach for solving it is mapping the observations into a representation space with a simple joint distribution, which can typically be written as a product of its marginals—thus drawing a connection with the field of nonlinear independent component analysis. Deep density models have been widely used for this task, but their maximum likelihood based training requires estimating the log-determinant of the Jacobian and is computationally expensive, thus imposing a trade-off between computation and expressive power. In this work, we propose a new approach for exact training of such neural networks. Based on relative gradients, we exploit the matrix structure of neural network parameters to compute updates efficiently even in high-dimensional spaces; the computational cost of the training is quadratic in the input size, in contrast with the cubic scaling of naive approaches. This allows fast training with objective functions involving the log-determinant of the Jacobian, without imposing constraints on its structure, in stark contrast to autoregressive normalizing flows.",Luigi Gresele and Giancarlo Fissore and Adrián Javaloy and Bernhard Schölkopf and Aapo Hyvarinen,3,6730260623059884783,Advances in Neural Information Processing Systems,,,,Relative gradient optimization of the Jacobian term in unsupervised deep learning,http://proceedings.neurips.cc/paper/2020/hash/c10f48884c9c7fdbd9a7959c59eebea8-Abstract.html,33,2020,/scholar?cites=6730260623059884783,DZ-fHPgAAAAJ:lCwh4MUTK3kC
14660,"Robot table tennis systems require a vision system that can track the ball position with low latency and high sampling rate. Altering the ball to simplify the tracking using, for instance, infrared coating changes the physics of the ball trajectory. As a result, table tennis systems use custom tracking systems to track the ball based on heuristic algorithms respecting the real-time constrains applied to RGB images captured with a set of cameras. However, these heuristic algorithms often report erroneous ball positions, and the table tennis policies typically need to incorporate additional heuristics to detect and possibly correct outliers. In this paper, we propose a vision system for object detection and tracking that focuses on reliability while providing real-time performance. Our assumption is that by using multiple cameras, we can find and discard the errors obtained in the object detection phase by checking for consistency with the positions reported by other cameras. We provide an open source implementation of the proposed tracking system to simplify future research in robot table tennis or related tracking applications with strong real-time requirements. We evaluate the proposed system thoroughly in simulation and in the real system, outperforming previous work. Furthermore, we show that the accuracy and robustness of the proposed system increases as more cameras are added. Finally, we evaluate the table tennis playing performance of an existing method in the real robot using the proposed vision system. We measure a slight increase in performance compared to a previous vision system even after removing all the heuristics previously present to …",Sebastian Gomez-Gonzalez and Yassine Nemmour and Bernhard Schölkopf and Jan Peters,3,7134047082202456340,Robotics,4,90,Multidisciplinary Digital Publishing Institute,Reliable Real-Time Ball Tracking for Robot Table Tennis,https://www.mdpi.com/2218-6581/8/4/90,8,2019,/scholar?cites=7134047082202456340,DZ-fHPgAAAAJ:lHDkRtKMBJAC
14661,"This work presents the concept of kernel mean embedding and kernel probabilistic programming in the context of stochastic systems. We propose formulations to represent, compare, and propagate uncertainties for fairly general stochastic dynamics in a distribution-free manner. The new tools enjoy sound theory rooted in functional analysis and wide applicability as demonstrated in distinct numerical examples. The implication of this new concept is a new mode of thinking about the statistical nature of uncertainty in dynamical systems.",Jia-Jie Zhu and Krikamol Muandet and Moritz Diehl and Bernhard Schölkopf,3,6829668130543455676,arXiv preprint arXiv:1911.11082,,,,"A new distribution-free concept for representing, comparing, and propagating uncertainty in dynamical systems with kernel probabilistic programming",https://arxiv.org/abs/1911.11082,,2019,/scholar?cites=6829668130543455676,DZ-fHPgAAAAJ:XKD4hAGUp1wC
14662,"The modulation transfer function (MTF) is widely used to characterise the performance of optical systems. Measuring it is costly and it is thus rarely available for a given lens specimen. Fortunately, images recorded through an optical system contain ample information about its MTF, only that it is confounded with the statistics of the images. This work presents a method to estimate the MTF of camera lens systems directly from photographs, without the need for expensive equipment. We use a custom grid display to accurately measure the point response of lenses to acquire ground truth training data. We then use the same lenses to record natural images and employ a supervised learning approach using a convolutional neural network to estimate the MTF on small image patches, aggregating the information into MTF charts over the entire field of view. It generalises to unseen lenses and can be applied for single …",Matthias Bauer and Valentin Volchkov and Michael Hirsch and Bernhard Schcölkopf,3,13265025512299658159,,,1-12,IEEE,Automatic estimation of modulation transfer functions,https://ieeexplore.ieee.org/abstract/document/8368467/,,2018,/scholar?cites=13265025512299658159,DZ-fHPgAAAAJ:3jcDufT8s9IC
14663,"Deep generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are important tools to capture and investigate the properties of complex empirical data. However, the complexity of their inner elements makes their functioning challenging to interpret and modify. In this respect, these architectures behave as black box models. In order to better understand the function of such network, we analyze the modularity of these system by quantifying the disentanglement of their intrinsic parameters. This concept relates to a notion of invariance to transformations of internal variables of the generative model, recently introduced in the field of causality. Our experiments on generation of human faces with VAEs supports that modularity between weights distributed over layers of generator architecture is achieved to some degree, and can be used to understand better the functioning of these architectures. Finally, we show that modularity can be enhanced during optimization.",Michel Besserve and Rémy Sun and Bernhard Schölkopf,3,17255860992615471164,,,,,Intrinsic disentanglement: an invariance view for deep generative models,http://is.tuebingen.mpg.de/uploads_file/attachment/attachment/445/47_Intrinsic_disentanglement_an_invariance_view_for_deep_generative_models.pdf,,2018,/scholar?cites=17255860992615471164,DZ-fHPgAAAAJ:y1Uj1W33RhcC
14664,"Purpose/Introduction: The field inhomogeneities produced by different susceptibilities in biological tissue and air cavities increase linearly with the applied static magnetic field. The most recent approach demonstrated advantage of using irregularly shaped coil elements in an array configuration [1, 2] instead of loop elements [3, 4] in close fitting B0 matrix shim. We introduce independent parameters describing a geometry and position of every single coil which are then optimized jointly. Subjects and Methods: For the simulations, a set of coils were placed on a cylinder (ø 360 mm), which is large enough to accommodate inside additional RF transmit and receive arrays. The optimization procedure was based on a field or frequency map that was acquired within the brain of a volunteer. One approach was to design individual setups, each one containing 6 coils optimized on a single slice. After having, for example, 2 setups tuned on 2 different slices we could accommodate them all on a cylinder having in total 12 channels. The second approach was to take 16 channels and configure them so as to optimize an accuracy over different slices simultaneously. The comparison of different setups was based on the standard deviation (in Hz) of the resulting magnetic field distributions. Two-slice optimization is just for proof of concept, in the future optimization will be performed on several slices or on the specified volume. Results: Figure 1 shows the slices after 2nd SH shim and after shimming with 6 channel optimized setups for a particular slice. Figure 2 shows the resulting shim field after simultaneous optimization of the setup on 2 slices. The final standard …",Irena Zivkovic and I Tolstikhin and Bernhard Schoelkopf and Klaus Scheffler,3,16589893008398536161,,,S36-S36,,"B0 matrix shim array design-optimization of the position, geometry and the number of segments of individual coil elements",https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2547452,,2016,/scholar?cites=16589893008398536161,DZ-fHPgAAAAJ:XHCur8y4e7YC
14665,"We present BundleMAP, a novel method for extracting features from diffusion MRI (dMRI), which can be used to detect disease with supervised classification. BundleMAP uses manifold learning to aggregate measurements over localized segments of nerve fiber bundles, which are natural anatomical units in this data. We obtain a fully integrated machine learning pipeline by combining this idea with mechanisms for outlier removal and feature selection. We demonstrate that it increases accuracy on a clinical dataset for which classification results have been reported previously, and that it pinpoints the anatomical locations relevant to the classification.",Mohammad Khatami and Tobias Schmidt-Wilcke and Pia C Sundgren and Amin Abbasloo and Bernhard Schölkopf and Thomas Schultz,3,13238381701330422091,,,52-60,"Springer, Cham",BundleMAP: anatomically localized features from dMRI for detection of disease,https://link.springer.com/chapter/10.1007/978-3-319-24888-2_7,,2015,/scholar?cites=13238381701330422091,DZ-fHPgAAAAJ:IWjwvGYIK2oC
14666,"The planet discoveries made using data from the Kepler mission have revolutionized the field of exoplanet statistics. Thousands of planets have been discovered with orbital periods ranging from hours to two years. Some of the most dynamically interesting planets (Jupiter analogs, for example) only show a single transit in the four-year baseline of the Kepler mission and, as a result, they have not yet been found. Upcoming transit surveys like K2, TESS, and PLATO all have shorter contiguous observation baselines. It is therefore crucial to develop robust techniques for the discovery of single transit events. We present a search procedure designed to find single transits using a supervised classification model. To search for a transit signal in a given month-long light curve from Kepler, we train a random forest classifier on tens of thousands of simulated transit signals injected into the light curve of the same star at other …",Daniel Foreman-Mackey and David W Hogg and Bernhard Schölkopf,3,8529254320788830189,IAUGA,,2258352,,The search for single exoplanet transits in the Kepler light curves,https://ui.adsabs.harvard.edu/abs/2015IAUGA..2258352F/abstract,29,2015,/scholar?cites=8529254320788830189,DZ-fHPgAAAAJ:XGCB9NUBGnMC
14667,"Even high-quality lenses suffer from optical aberrations, especially when used at full aperture. Furthermore, there are significant lens-to-lens deviations due to manufacturing tolerances, often rendering current software solutions like DxO, Lightroom, and PTLens insufficient as they don't adapt and only include generic lens blur models. We propose a method that enables the self-calibration of lenses from a natural image, or a set of such images. To this end we develop a machine learning framework that is able to exploit several recorded images and distills the available information into an accurate model of the considered lens.",Michael Hirsch and Bernhard Scholkopf,3,10332910630559937575,,,612-620,,Self-calibration of optical lenses,http://openaccess.thecvf.com/content_iccv_2015/html/Hirsch_Self-Calibration_of_Optical_ICCV_2015_paper.html,,2015,/scholar?cites=10332910630559937575,DZ-fHPgAAAAJ:RFBuFmO0de4C
14668,"We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, ie, do not lose information, provided the objects to be imaged satisfy a generic condition. A real world experiment shows that we can super-resolve beyond the Rayleigh limit.",Stefan Harmeling and Michael Hirsch and Bernhard Scholkopf,3,8420894278684086142,,,1083-1090,,"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution beyond the diffraction limit",http://openaccess.thecvf.com/content_cvpr_2013/html/Harmeling_On_a_Link_2013_CVPR_paper.html,,2013,/scholar?cites=8420894278684086142,DZ-fHPgAAAAJ:xGWFX6Gbr9MC
14669,"We present a methodology for incorporating prior knowledge on class probabilities into the registration process. By using knowledge from the imaging modality, pre-segmentations, and/or probabilistic atlases, we construct vectors of class probabilities for each image voxel. By defining new image similarity measures for distribution-valued images, we show how the class probability images can be nonrigidly registered in a variational framework. An experiment on nonrigid registration of MR and CT full-body scans illustrates that the proposed technique outperforms standard mutual information (MI) and normalized mutual information (NMI) based registration techniques when measured in terms of target registration error (TRE) of manually labeled fiducials.",Matthias Hofmann and Bernhard Schölkopf and Ilja Bezrukov and Nathan D Cahill,3,11354181538902475865,,,220-231,,Incorporating prior knowledge on class probabilities into local similarity measures for intermodality image registration,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1789144,,2009,/scholar?cites=11354181538902475865,DZ-fHPgAAAAJ:hMod-77fHWUC
14670,"Page 1. Applied Neuroscience Conference Nijmegen 2007 Impact of target-to-target interval
on classification performance in the P300 speller Suzanne Martens1 Jeremy Hill1 Jason
Farquhar1 Bernhard Schölkopf1 1 Empirical Inference Department Max Planck Institute for
Biological Cybernetics Tübingen, Germany suzanne.martens@tuebingen.mpg.de Page 2. 2 P300
speller in general – Brain-Computer Interface for spelling words – Possible means of
communication for paralyzed patients with intact visual system [Nijboer_unpubl] – Uses
discriminative properties of event-related potentials (ERPs) in response to target/attended (T)
and non-target/non-attended (N) stimuli. – Setup has hardly changed since it's introduction in
1988 [Farwell_1988] Page 3. 3 Mechanism of character prediction – Stimuli are flashing characters
organized in a matrix on a screen, flashing in a random/unpredictable order … 
",SMM Martens and Jeremy Hill and Jason Farquhar and Bernhard Schölkopf,3,12088029648386403660,,,,,Impact of target-to-target interval on classification performance in the P300 speller,https://pure.mpg.de/rest/items/item_1790509/component/file_3141057/content,,2007,/scholar?cites=12088029648386403660,DZ-fHPgAAAAJ:5awf1xo2G04C
14671,"Given a spatial filtering algorithm that has allowed us to identify task-relevant EEG sources, we present a simple approach for monitoring the activity of these sources while remaining relatively robust to changes in other (task-irrelevant) brain activity. The idea is to keep spatial patterns fixed rather than spatial filters, when transferring from training to test sessions or from one time window to another. We show that a fixed spatial pattern (FSP) approach, using a moving-window estimate of signal covariances, can be more robust to non-stationarity than a fixed spatial filter (FSF) approach.",N Jeremy Hill and Jason Farquhar and Thomas N Lal and Bernhard Schölkopf,3,17867515802734860405,Proceedings of the 3rd International Brain-Computer Interface Workshop and Training Course,,20-21,,Time-dependent demixing of task-relevant EEG signals,https://www.neurotechcenter.org/sites/default/files/misc/Time-dependent%20demixing%20of%20task-relevant%20EEG%20signals.pdf,,2006,/scholar?cites=17867515802734860405,DZ-fHPgAAAAJ:JlkRFsBlJToC
14672,"Given a spatial filtering algorithm that has allowed us to identify task-relevant EEG sources, we present a simple approach for monitoring the activity of these sources while remaining relatively robust to changes in other (task-irrelevant) brain activity. The idea is to keep spatial patterns fixed rather than spatial filters, when transferring from training to test sessions or from one time window to another. We show that a fixed spatial pattern (FSP) approach, using a moving-window estimate of signal covariances, can be more robust to non-stationarity than a fixed spatial filter (FSF) approach.",N Jeremy Hill and Jason Farquhar and Thomas Navin Lal and Bernhard Schölkopf,3,1797354805394684536,,,,,Time-dependent demixing of task-relevant EEG sources,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.8666,,2006,/scholar?cites=1797354805394684536,DZ-fHPgAAAAJ:ZSCPaVbTtlsC
14673,"Page 1. Introduction to Kernel Methods Bernhard Schölkopf Max-Planck-Institut für biologische
Kybernetik 72076 Tübingen, Germany bernhard.schoelkopf@tuebingen.mpg.de B. Schölkopf,
Erice, 31 October 2005 Page 2. Roadmap 1. Kernels 2. Support Vector classification 3. Further
kernel algorithms: kernel PCA, kernel dependency es- timation, implicit surface approximation,
morphing B. Schölkopf, Erice, 31 October 2005 Page 3. Learning and Similarity: some Informal
Thoughts • input/output sets X,Y • training set (x1 ,y1 ),...xm,ym) ∈X×Y • “generalization”: given
a previously unseen x ∈ X, find a suit- able y ∈ Y • (x, y) should be “similar” to (x1 ,y1 ),...xm,ym) •
how to measure similarity? – for outputs: loss function (eg, for Y = {±1}, zero-one loss) – for inputs:
kernel B. Schölkopf, Erice, 31 October 2005 Page 4. Similarity of Inputs • symmetric function k :
X×X → R (x, x ) ↦→ k(x, x ) • for example, if X = R … 
",Bernhard Schölkopf,3,6498410355937656546,"The Analysis of Patterns, Erice, Italy",,,,Introduction to Kernel Methods,http://www.ccs.neu.edu/home/vip/teach/MLcourse/6_SVM_kernels/materials/aop05_scholkopf_km.pdf,,2005,/scholar?cites=6498410355937656546,DZ-fHPgAAAAJ:5Ul4iDaHHb8C
14674,"This article presents a method for estimating a generative image model based on Kernel Principal Component Analysis (KPCA). In contrast to other patch-based modeling approaches such as PCA, ICA or sparse coding, KPCA is capable of capturing nonlinear interactions between the basis elements of the image. The original form of KPCA, however, can be only applied to strongly restricted image classes due to the limited number of training examples that can be processed. We therefore propose a new iterative method for performing KPCA, the Kernel Hebbian Algorithm. By kernelizing the Generalized Hebbian Algorithm, one can iteratively estimate the Kernel Principal Components with only linear order memory complexity. We demonstrate the generalization capabilities of the resulting image model in single-frame super-resolution and denoising applications.",Kwang In Kim and Matthias O Franz and Bernhard Schölkopf,3,17675052244905957222,IEEE Trans. on PAMI,,,,Image modeling based on kernel principal component analysis,https://www.researchgate.net/profile/Matthias_Franz2/publication/253101515_Image_Modeling_based_on_Kernel_Principal_Component_Analysis/links/540d70640cf2df04e7549c67/Image-Modeling-based-on-Kernel-Principal-Component-Analysis.pdf,,2005,/scholar?cites=17675052244905957222,DZ-fHPgAAAAJ:LPZeul_q3PIC
14675,"We propose a general regularization framework for transductive inference. The given data are thought of as a graph, where the edges encode the pairwise relationships among data. We develop discrete analysis and geometry on graphs, and then naturally adapt the classical regularization in the continuous case to the graph situation. A new and effective algorithm is derived from this general framework, as well as an approach we developed before.",Dengyong Zhou and Bernhard Schölkopf,3,11939426145738980385,,,,Max Planck Institute for Biological Cybernetics,Transductive Inference with Graphs,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792175,,2004,/scholar?cites=11939426145738980385,DZ-fHPgAAAAJ:WbkHhVStYXYC
14676,"This chapter contains sections titled: Introduction, Kernels, Suffix Trees and Matching Statistics, Weights and Kernels, Optimization and Prediction, Experimental Results, Conclusion",Bernhard Schölkopf and Koji Tsuda and Jean-Philippe Vert,3,12817913298886465420,,,113-130,MIT Press,Fast kernels for string and tree matching,https://ieeexplore.ieee.org/abstract/document/6282640/,,2004,/scholar?cites=12817913298886465420,DZ-fHPgAAAAJ:FbyvjEeiep8C
14677,"The Wiener series is one of the standard methods to systematically characterize the nonlinearity of a neural system. The classical estimation method of the expansion coefficients via cross-correlation suffers from severe problems that prevent its application to high-dimensional and strongly nonlinear systems. We propose a new estimation method based on regression in a reproducing kernel Hilbert space that overcomes these problems. Numerical experiments show performance advantages in terms of convergence, interpretability and system size that can be handled.",Matthias O Franz and Bernhard Schölkopf,3,9367725311117048413,,,,Max Planck Institute for Biological Cybernetics,Implicit Wiener Series: Part I: Cross-Correlation vs. Regression in Reproducing Kernel Hilbert Spaces,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792421,,2003,/scholar?cites=9367725311117048413,DZ-fHPgAAAAJ:xyvS_IvSCKsC
14678,,Pai-Hsuen Chen and Chih-Jen Lin and B Schölkopf,3,10105904792156917610,,,,,A tutorial,http://scholar.google.com/scholar?cluster=10105904792156917610&hl=en&oi=scholarr,,2003,/scholar?cites=10105904792156917610,DZ-fHPgAAAAJ:mWUU9EPYiGkC
14679,"We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function A; by a"" randomized kernel"" which behaves like k in expectation.",D Achlioptas F McSherry and B Schölkopf,3,6827534789351115410,Proc. Conf. Advances in Neural Information Processing Systems,,335-342,,Sampling Techniques for Kernel Methods,http://scholar.google.com/scholar?cluster=6827534789351115410&hl=en&oi=scholarr,14,2002,/scholar?cites=6827534789351115410,DZ-fHPgAAAAJ:KHHIWZcdS5cC
14680,,B Schlkopf and R Williamson and A Smola and J Shawe-Taylor and J Platt,3,9718917066060906756,Advances in Neural Information Processing Systems,,,,"Support vector method for novelty detection, Solla SA, Leen TK, Muller KR, editors",http://scholar.google.com/scholar?cluster=9718917066060906756&hl=en&oi=scholarr,12,2000,/scholar?cites=9718917066060906756,DZ-fHPgAAAAJ:CCY0Zhscfv0C
14681,"Autor: Schölkopf, B; Genre: Buchkapitel; Im Druck veröffentlicht: 1996; Titel: Künstliches Lernen.
",B Schölkopf,3,14109722570038283787,,,93-117,Röll,Künstliches Lernen,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1794409,,1996,/scholar?cites=14109722570038283787,DZ-fHPgAAAAJ:CEegLu37BmcC
14682,"Author: Vapnik, V et al.; Genre: Report; Published in Print: 1995; Title: A
New Method for Constructing Artificial Neural Networks.
",Vladimir Vapnik and Christopher JC Burges and Bernhard Schoelkopf,3,10247102589644155246,,,,"AT & T, Bell Laboratories",A new method for constructing artificial neural networks,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1794525,,1995,/scholar?cites=10247102589644155246,DZ-fHPgAAAAJ:VOx2b1Wkg3QC
14683,"In combined PET/MR, the high soft tissue contrast of Magnetic Resonance Imaging (MRI) and the functional information of Positron Emission Tomography (PET) are combined in one machine. Although this combination still poses many technological challenges, recent progress [1] indicates that it is only a matter of time before PET/MR machines enter the market. On the software side, attenuation correction, which accounts for radiation attenuation properties of the tissue, is mandatory for obtaining PET images that are sufficiently accurate for quantification. Usually the attenuation map is obtained from a transmission scan, which uses a rotating source. In the case of a PET/MR, there is insufficient space for the rotating source and the attenuation map needs to be determined in another way. Ideally, one would want to calculate it from the MR image. This is inherently difficult: The intensities in the MR image are proportional to the proton density; however, the PET attenuation value is not directly related to the proton density. For example bone compacta and air both do not contribute a MR signal whereas their attenuation values are maximally distinct. Thus, standard intensity based segmentation is bound to fail. In order to determine the attenuation value for a position we have to include additional knowledge. We show how such knowledge can be extracted from local image patches and the registration with an atlas.Attenuation correction directly influences the standard uptake values (SUV) and can therefore directly impact diagnostic decisions derived from the images. This means that any attenuation correction method needs to be validated carefully …",M Hofmann and F Steinke and V Scheel and GC Charpiat and JM Brady and B Schölkopf and BJ Pichler,3,6339389193448405136,Proc. IEEE Nuclear Science Symposium and Medical Imaging Conference,,,,MR-based PET Attenuation Correction–Method and Validation,https://pdfs.semanticscholar.org/4ff9/79b2b6a5aae67ffabf1325f98331a7a75fcf.pdf,30,,/scholar?cites=6339389193448405136,DZ-fHPgAAAAJ:JgaSC4GK3rcC
14684,,Alex Smola and Bernhard Schlkopf,3,3347525654530497107,,,,,Generalized Discriminan Analysis,http://scholar.google.com/scholar?cluster=3347525654530497107&hl=en&oi=scholarr,,,/scholar?cites=3347525654530497107,DZ-fHPgAAAAJ:O6Atq1G1BT4C
14685,"While many recent works have studied the problem of algorithmic fairness from the perspective of predictions, here we investigate the fairness of recourse actions recommended to individuals to recover from an unfavourable classification. To this end, we propose two new fairness criteria at the group and individual level which---unlike prior work on equalising the average distance from the decision boundary across protected groups---are based on a causal framework that explicitly models relationships between input features, thereby allowing to capture downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We then investigate how to enforce fair recourse in the training of the classifier. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions and structural changes to the system, as opposed to constraints on the classifier.",Julius von Kügelgen and Umang Bhatt and Amir-Hossein Karimi and Isabel Valera and Adrian Weller and Bernhard Schölkopf,2,15394017645110341871,arXiv preprint arXiv:2010.06529,,,,On the fairness of causal algorithmic recourse,https://arxiv.org/abs/2010.06529,,2020,/scholar?cites=15394017645110341871,DZ-fHPgAAAAJ:sG_0aNaVAsoC
14686,"Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks-inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, eg, robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (eg, only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.",Ossama Ahmed and Frederik Träuble and Anirudh Goyal and Alexander Neitz and Manuel Wüthrich and Yoshua Bengio and Bernhard Schölkopf and Stefan Bauer,2,13832770052357029885,arXiv preprint arXiv:2010.04296,,,,Causalworld: A robotic manipulation benchmark for causal structure and transfer learning,https://arxiv.org/abs/2010.04296,,2020,/scholar?cites=13832770052357029885,DZ-fHPgAAAAJ:n3Ahz6dV9U4C
14687,"In this paper, we investigate the principle thatgood explanations are hard to vary'in the context of deep learning. We show that averaging gradients across examples--akin to a logical OR of patterns--can favor memorization andpatchwork'solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers.",Giambattista Parascandolo and Alexander Neitz and Antonio Orvieto and Luigi Gresele and Bernhard Schölkopf,2,13244665491753112041,arXiv preprint arXiv:2009.00329,,,,Learning explanations that are hard to vary,https://arxiv.org/abs/2009.00329,,2020,/scholar?cites=13244665491753112041,DZ-fHPgAAAAJ:MlcLsmccrp0C
14688,"Online detection of instantaneous changes in the generative process of a data sequence generally focuses on retrospective inference of such change points without considering their future occurrences. We extend the Bayesian Online Change Point Detection algorithm to also infer the number of time steps until the next change point (ie, the residual time). This enables to handle observation models which depend on the total segment duration, which is useful to model data sequences with temporal scaling. The resulting inference algorithm for segment detection can be deployed in an online fashion, and we illustrate applications to synthetic and to two medical real-world data sets.",Diego Agudelo-España and Sebastian Gomez-Gonzalez and Stefan Bauer and Bernhard Schölkopf and Jan Peters,2,406686124128075282,,,320-329,PMLR,Bayesian online prediction of change points,http://proceedings.mlr.press/v124/agudelo-espana20a.html,,2020,/scholar?cites=406686124128075282,DZ-fHPgAAAAJ:lei5Nhix5ycC
14689,"A common assumption in generative models is that the generator immerses the latent space into a Euclidean ambient space. Instead, we consider the ambient space to be a Riemannian manifold, which allows for encoding domain knowledge through the associated Riemannian metric. Shortest paths can then be defined accordingly in the latent space to both follow the learned manifold and respect the ambient geometry. Through careful design of the ambient metric we can ensure that shortest paths are well-behaved even for deterministic generators that otherwise would exhibit a misleading bias. Experimentally we show that our approach improves interpretability of learned representations both using stochastic and deterministic generators.",Georgios Arvanitidis and Søren Hauberg and Bernhard Schölkopf,2,9129097555098147239,arXiv preprint arXiv:2008.00565,,,,Geometrically Enriched Latent Spaces,https://arxiv.org/abs/2008.00565,,2020,/scholar?cites=9129097555098147239,DZ-fHPgAAAAJ:vU07HMfDF0IC
14690,"Despite impressive progress in the last decade, it still remains an open challenge to build models that generalize well across multiple tasks and datasets. One path to achieve this is to learn meaningful and compact representations, in which different semantic aspects of data are structurally disentangled. The focus of disentanglement approaches has been on separating independent factors of variation despite the fact that real-world observations are often not structured into meaningful independent causal variables to begin with. In this work we bridge the gap to real-world scenarios by analyzing the behavior of most prominent methods and disentanglement scores on correlated data in a large scale empirical study (including 3900 models). We show that systematically induced correlations in the dataset are being learned and reflected in the latent representations, while widely used disentanglement scores fall short of capturing these latent correlations. Finally, we demonstrate how to disentangle these latent correlations using weak supervision, even if we constrain this supervision to be causally plausible. Our results thus support the argument to learn independent mechanisms rather than independent factors of variations.",Frederik Träuble and Elliot Creager and Niki Kilbertus and Anirudh Goyal and Francesco Locatello and Bernhard Schölkopf and Stefan Bauer,2,7668419770897952137,arXiv preprint arXiv:2006.07886,,,,Is independence all you need? on the generalization of representations learned from correlated data,https://arxiv.org/abs/2006.07886,,2020,/scholar?cites=7668419770897952137,DZ-fHPgAAAAJ:kA94_h2wilkC
14691,"The goal of the unsupervised learning of disentangled representations is to separate the independent explanatory factors of variation in the data without access to supervision. In this paper, we summarize the results of (Locatello et al. 2019b) and focus on their implications for practitioners. We discuss the theoretical result showing that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases and the practical challenges it entails. Finally, we comment on our experimental findings, highlighting the limitations of state-of-the-art approaches and directions for future research.",Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Bachem,2,2611340627350015108,Proceedings of the AAAI Conference on Artificial Intelligence,09,13681-13684,,A commentary on the unsupervised learning of disentangled representations,https://ojs.aaai.org/index.php/AAAI/article/view/7120,34,2020,/scholar?cites=2611340627350015108,DZ-fHPgAAAAJ:Kp4Oxjec8VUC
14692,"Modern implicit generative models such as generative adversarial networks (GANs) are generally known to suffer from issues such as instability, uninterpretability, and difficulty in assessing their performance. If we see these implicit models as dynamical systems, some of these issues are caused by being unable to control their behavior in a meaningful way during the course of training. In this work, we propose a theoretically grounded method to guide the training trajectories of GANs by augmenting the GAN loss function with a kernel-based regularization term that controls local and global discrepancies between the model and true distributions. This control signal allows us to inject prior knowledge into the model. We provide theoretical guarantees on the stability of the resulting dynamical system and demonstrate different aspects of it via a wide range of experiments.",Arash Mehrjou and Wittawat Jitkrittum and Krikamol Muandet and Bernhard Schölkopf,2,6085426334330409201,arXiv preprint arXiv:1910.14428,,,,Kernel-guided training of implicit generative models with stability guarantees,https://arxiv.org/abs/1910.14428,,2019,/scholar?cites=6085426334330409201,DZ-fHPgAAAAJ:mbGrCKGjD74C
14693,"We study the problem of causal discovery through targeted interventions. Starting from few observational measurements, we follow a Bayesian active learning approach to perform those experiments which, in expectation with respect to the current model, are maximally informative about the underlying causal structure. Unlike previous work, we consider the setting of continuous random variables with non-linear functional relationships, modelled with Gaussian process priors. To address the arising problem of choosing from an uncountable set of possible interventions, we propose to use Bayesian optimisation to efficiently maximise a Monte Carlo estimate of the expected information gain.",Julius von Kügelgen and Paul K Rubenstein and Bernhard Schölkopf and Adrian Weller,2,912968171698830441,arXiv preprint arXiv:1910.03962,,,,Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks,https://arxiv.org/abs/1910.03962,,2019,/scholar?cites=912968171698830441,DZ-fHPgAAAAJ:MivfCDya6-cC
14694,"Transcranial alternating current stimulation (tACS) is becoming an important method in the field of motor rehabilitation because of its ability to non-invasively influence ongoing brain oscillations at arbitrary frequencies. However, substantial variations in its effect across individuals are reported, making tACS a currently unreliable treatment tool. One reason for this variability is the lack of knowledge about the exact way tACS entrains and interacts with ongoing brain oscillations. The present crossover stimulation study on 20 healthy subjects contributes to the understanding of cross-frequency effects of gamma (70 Hz) tACS over the contralateral motor cortex by providing empirical evidence which is consistent with a role of low- (12 -20 Hz) and high- (20- 30 Hz) beta power as a mediator of gamma-tACS on motor performance.",Atalanti A Mastakouri and Bernhard Schölkopf and Moritz Grosse-Wentrup,2,6866825659618016065,,,5902-5908,IEEE,Beta Power May Meditate the Effect of Gamma-TACS on Motor Performance,https://ieeexplore.ieee.org/abstract/document/8856416/,,2019,/scholar?cites=6866825659618016065,DZ-fHPgAAAAJ:uL1jBpQtEOcC
14695,"We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders. The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.",Matthias R Hohmann and Michelle Hackl and Brian Wirth and Talha Zaman and Raffi Enficiaud and Moritz Grosse-Wentrup and Bernhard Schölkopf,2,2670403803902531026,,,1-6,,MYND: a platform for large-scale neuroscientific studies,https://dl.acm.org/doi/abs/10.1145/3290607.3313002,,2019,/scholar?cites=2670403803902531026,DZ-fHPgAAAAJ:MQKg-S6vjYgC
14696,"We propose a novel procedure which adds ""content-addressability"" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model.",Wittawat Jitkrittum and Patsorn Sangkloy and Muhammad Waleed Gondal and Amit Raj and James Hays and Bernhard Schölkopf,2,235365843120524307,,,3140-3151,PMLR,Kernel Mean Matching for Content Addressability of GANs,https://discovery.ucl.ac.uk/id/eprint/10091897/,97,2019,/scholar?cites=235365843120524307,DZ-fHPgAAAAJ:GOCRXhlsxu4C
14697,"Two popular examples of first-order optimization methods over linear spaces are coordinate descent and matching pursuit algorithms, with their randomized variants. While the former targets the optimization by moving along coordinates, the latter considers a generalized notion of directions. Exploiting the connection between the two algorithms, we present a unified analysis of both, providing affine invariant sublinear O (1/t) rates on smooth objectives and linear convergence on strongly convex objectives. As a byproduct of our affine invariant analysis of matching pursuit, our rates for steepest coordinate descent are the tightest known. Furthermore, we show the first accelerated convergence rate O (1/t2) for matching pursuit on convex objectives.",Francesco Locatello and Anant Raj and Sai Praneeth Karimireddy and Gunnar Rätsch and Bernhard Schölkopf and Sebastian U Stich and Martin Jaggi,2,13910225312349722569,arXiv preprint arXiv:1803.09539,,,,Revisiting First-Order Convex Optimization Over Linear Spaces,http://scholar.google.com/scholar?cluster=13910225312349722569&hl=en&oi=scholarr,,2018,/scholar?cites=13910225312349722569,DZ-fHPgAAAAJ:KV5dOBAvh8oC
14698,"While implicit generative models such as GANs have shown impressive results in high quality image reconstruction and manipulation using a combination of various losses, we consider a simpler approach leading to surprisingly strong results. We show that texture loss [1] alone allows the generation of perceptually high quality images. We provide a better understanding of texture constraining mechanism and develop a novel semantically guided texture constraining method for further improvement. Using a recently developed perceptual metric employing"" deep features"" and termed LPIPS [2], the method obtains state-of-theart results. Moreover, we show that a texture representation of those deep features better capture the perceptual quality of an image than the original deep features. Using texture information, off-the-shelf deep classification networks (without training) perform as well as the best performing (tuned and calibrated) LPIPS metrics.",Muhammad Waleed Gondal and Bernhard Scholkopf and Michael Hirsch,2,17411509948213874665,,,0-0,,The unreasonable effectiveness of texture transfer for single image super-resolution,http://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Gondal_The_Unreasonable_Effectiveness_of_Texture_Transfer_for_Single_Image_Super-resolution_ECCVW_2018_paper.html,,2018,/scholar?cites=17411509948213874665,DZ-fHPgAAAAJ:hLusy9HomMgC
14699,"Feature representations in both, biological neural networks in the primate ventral stream and artificial convolutional neural networks trained on object recognition, incresase in complexity and receptive field size with layer depth. Somewhat strikingly, empirical evidence indicates that this analogy extends to the specific representations learned in each layer. This suggests that biological and artificial neural networks share a fundamental organising principle. We shed light on this principle in the framework of optimal coding. Specifically, we first investigate which properties of a code render it robust to transmission over noisy channels and formally prove that for equientropic channels an upper bound on the expected minimum decoding error is attained for codes with maximum marginal entropy. We then show that the pairwise correlation of units in a deep layer of a neural network, that has been trained on an object recognition task, increases when perturbing the distribution of input images, ie, that the network exhibits properties of an optimally coding system. By analogy, this suggests that the layer-wise similarity of feature representations in biological and artificial neural networks is a result of optimal coding that enables robust transmission of object information over noisy channels. Because we find that in equientropic channels the upper bound on the expected minimum decoding error is independent of the class-conditional entropy, our work further provides a plausible explanation why optimal codes can be learned in unsupervised settings.",Sebastian Weichwald and Tatiana Fomina and Bernhard Schölkopf and Moritz Grosse-Wentrup,2,9665488108392538686,arXiv preprint ArXiv:1605.07094,,1-10,,Optimal coding in biological and artificial neural networks,https://www.researchgate.net/profile/Tatiana_Fomina3/publication/303449236_Optimal_Coding_in_Biological_and_Artificial_Neural_Networks/links/57b5a34608ae19a365fc3e5b/Optimal-Coding-in-Biological-and-Artificial-Neural-Networks.pdf,,2016,/scholar?cites=9665488108392538686,DZ-fHPgAAAAJ:eO2F4e7mqQkC
14700,"Two challenges in the theory and practice of cloud computing are:(1) smart control (allocation) of resources under exploration constraints, on time-varying systems, and (2) understanding and debugging of the performance of complex systems that involve eg virtualization. In this paper, we examine how these challenges can be approached using causal models. For challenge (1) we investigate how to infer and use causal models and selection diagrams to design and integrate experiments in a principled way, as well as to cope with partially varying systems. For challenge (2) we examine how to formalize performance attribution and debugging questions by counterfactual probabilities, and how to approximately answer them based on inferred (non-deterministic) graphical causal models.",Philipp Geiger and Lucian Carata and Bernhard Schölkopf,2,2129448219593509467,arXiv preprint arXiv,,,,Causal models for debugging and control in cloud computing,http://scholar.google.com/scholar?cluster=2129448219593509467&hl=en&oi=scholarr,1603,2016,/scholar?cites=2129448219593509467,DZ-fHPgAAAAJ:viP-qiFw8s0C
14701,"It is our great pleasure to present, combined in a single volume, the contributions from two closely related workshops that took place on September 22, 2013, in Nagoya, Japan, under the auspices of the 16th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2013. The MICCAI Workshop on Computational Diffusion MRI is already the fifth event in a successful series, following the exciting and well-attended workshops in 2008, 2010, 2011, and 2012. Despite, and partly because of, the rapid development the field has witnessed over the past few years, and the fact that diffusion MRI is now widely used both in scientific research and in the clinic, the field continues to face important computational challenges. For anyone interested in learning about Computational Diffusion MRI or already working in the field, the 17 original research papers collected in the first four parts …",Thomas Schultz and Gemma Nedjati-Gilani and Archana Venkataraman and Lauren O’Donnell and Eleftheria Panagiotaki,2,6242075240215431978,,,,Springer International Pu,Computational Diffusion MRI and Brain Connectivity,https://link.springer.com/content/pdf/10.1007/978-3-319-02475-2.pdf,,2016,/scholar?cites=6242075240215431978,DZ-fHPgAAAAJ:KqnX2w3egDsC
14702,"There are several issues with causal discovery from fMRI. First, the sampling frequency is so low that the time-delayed dependence between different regions is very small, making time-delayed causal relations weak and unreliable. Moreover, the complex correspondence between neural activity and the BOLD signal makes it difficult to formulate a causal model to represent the effect as a function of the cause. Second, the fMRI experiment may last for a relatively long time period, during which the causal influences are likely to change along with certain unmeasured states (eg, the attention) of the subject which can be written as a function of time, and ignoring the time-dependence will lead to spurious connections. Likewise, the causal influences may also vary as a function of the experimental condition (eg, health, disease, and behavior). In this paper we aim to develop a principled framework for robust and time-or condition-specific causal discovery, by addressing the above issues. Motivated by a simplified fMRI generating process, we show that the time-delayed conditional independence relationships at the proper causal frequency of neural activities are consistent with the instantaneous conditional independence relationships between brain regions in fMRI recordings. Then we propose an enhanced constraint-based method for robust discovery of the underlying causal skeletons, where we include time or condition as an additional variable in the system; it helps avoid spurious causal connections between brain regions and discover time-or conditionspecific regions. It also has additional benefit in causal direction determination. Experiments on …",Kun Zhang and Biwei Huang and Bernhard Schölkopf and Michel Besserve and Masataka Watanabe and Dajiang Zhu,2,12120101422712279484,Preprint at http://arxiv. org/abs/1509.08056,,,,Towards Robust and Specific Causal Discovery from fMRI,http://scholar.google.com/scholar?cluster=12120101422712279484&hl=en&oi=scholarr,,2015,/scholar?cites=12120101422712279484,DZ-fHPgAAAAJ:AimQIn9zGmEC
14703,"The Kalman filter is a well-established approach to get information on the time-dependent state of a system from noisy observations. It was developed in the context of the Apollo project to see the deviation of the true trajectory of a rocket from the desired trajectory. Afterwards it was applied to many different systems with small numbers of components of the respective state vector (typically about 10). In all cases the equation of motion for the state vector was known exactly. The fast dissipative magnetization dynamics is often investigated by x-ray magnetic circular dichroism movies (XMCD movies), which are often very noisy. In this situation the number of components of the state vector is extremely large (about 105), and the equation of motion for the dissipative magnetization dynamics (especially the values of the material parameters of this equation) is not well known. In the present paper it is shown by theoretical …",M Kopp and S Harmeling and G Schütz and B Schölkopf and M Fähnle,2,11190096432060507024,Ultramicroscopy,,115-122,North-Holland,Towards denoising XMCD movies of fast magnetization dynamics using extended Kalman filter,https://www.sciencedirect.com/science/article/pii/S0304399114001892,148,2015,/scholar?cites=11190096432060507024,DZ-fHPgAAAAJ:MSzX15-gZgkC
14704,"The goal of supervised learning is to infer a function f from a training set Dtr={(xtr1, ytr1),...,(xtrm, ytrm)}⊆ X× Y, where X and Y denote the domains of predictors X and target Y, respectively. The estimated f is expected to generalize well on the test set Dte={(xte1, yte1),...,(xten, yten)}⊆ X× Y, where ytei are unknown. Traditionally, the training set and test set are assumed to follow the same distribution. However, in many real world problems, the training data and test data have different distributions, ie, PtrXY= PteXY, 1 and the goal is to find a learning machine that performs well on the test domain. This problem is known as domain adaptation in machine learning. If the data distribution changes arbitrarily, training data would be of no use to make predictions on the test domain. To perform domain adaptation successfully, relevant knowledge in the training (or source) domain should be transferred to the test (or target) domain. For instance, the situation where PtrXY and PteXY only differ in the marginal distribution of the covariate (ie, PtrX= PteX, while PtrY| X= PteY| X) is termed covariate shift [25, 33, 10] or sample selection bias [37], and has been well studied. For surveys on domain adaptation for classification, see, eg,[13, 17, 1]. In particular, we address the situation where both the marginal distribution PX and the conditional distribution PY| X may change across the domains. Clearly, we need to make certain assumptions for the training domain to be adaptable to the test domain. We first consider the case where PX| Y is the same on both domains. As a consequence of Bayes’ rule, the changes in PX and PY| X are caused by the change in PY, the …",Kun Zhang and Bernhard Schölkopf and Krikamol Muandet and Zhikun Wang and Z Zhou and Claudio Persello,2,2904942662374097560,,,427,CRC Press,Single-source domain adaptation with target and conditional shift,http://books.google.com/books?hl=en&lr=&id=25TaBAAAQBAJ&oi=fnd&pg=PA427&dq=info:mIKv2_VtUCgJ:scholar.google.com&ots=g0iL5zeGC0&sig=XznTn_i_cuVmYkuq3Gqj4ye4cYE,,2014,/scholar?cites=2904942662374097560,DZ-fHPgAAAAJ:i8eIfGGcn98C
14705,"Kepler's immense photometric precision to date was maintained through satellite stability and precise pointing. In this white paper, we argue that image modeling--fitting the Kepler-downlinked raw pixel data--can vastly improve the precision of Kepler in pointing-degraded two-wheel mode. We argue that a non-trivial modeling effort may permit continuance of photometry at 10-ppm-level precision. We demonstrate some baby steps towards precise models in both data-driven (flexible) and physics-driven (interpretably parameterized) modes. We demonstrate that the expected drift or jitter in positions in the two-weel era will help with constraining calibration parameters. In particular, we show that we can infer the device flat-field at higher than pixel resolution; that is, we can infer pixel-to-pixel variations in intra-pixel sensitivity. These results are relevant to almost any scientific goal for the repurposed mission; image modeling ought to be a part of any two-wheel repurpose for the satellite. We make other recommendations for Kepler operations, but fundamentally advocate that the project stick with its core mission of finding and characterizing Earth analogs.[abridged]",David W Hogg and Ruth Angus and Tom Barclay and Rebekah Dawson and Rob Fergus and Dan Foreman-Mackey and Stefan Harmeling and Michael Hirsch and Dustin Lang and Benjamin T Montet and David Schiminovich and Bernhard Schölkopf,2,15211318111926405826,arXiv preprint arXiv:1309.0653,,,,Maximizing Kepler science return per telemetered pixel: Detailed models of the focal plane in the two-wheel era,https://arxiv.org/abs/1309.0653,,2013,/scholar?cites=15211318111926405826,DZ-fHPgAAAAJ:8RAEygVn5_EC
14706,"Current concepts of combined PET/MR tomographs do not allow for separate CT-like transmission sources. Therefore, corresponding PET attenuation coefficients must be calculated from the available MR images. MR-based attenuation correction (MR-AC) is far more challenging than the well-established algorithms for CT-based attenuation correction (CT-AC) since MR image voxel values correlate with the hydrogen nuclei density in tissues and tissue relaxation properties rather than with electron density-related mass attenuation coefficients on CT (Figure 11.1). Therefore, a direct transformation from available MR images to CT-like attenuation values is challenging[3]. Although pre-clinical PET/MR prototype systems [37] have been around since the mid 1990s MR-AC is still a work in progress. While early pre-clinical PET/MR design concepts did not include means for AC [17],[5] a relatively simple 2-class attenuation correction scheme was suggested for the first clinical prototype [36]. The lack of viable MR-AC methods today can be explained by the fact that attenuation is less critical in small animals compared to pediatric and adult patients, and, therefore, the issue of AC was of minor importance in pre-clinical PET and PET/MR.In this chapter we review methods to derive PET attenuation maps from available MR images in clinical PET/MR imaging scenarios. We will discuss potential pitfalls of MR-AC as well as a number of possible advantages of MR-AC that in certain cases could render it beneficial over CT-AC. Table 11.1 summarizes the main approaches to MR-AC for clinical imaging scenarios. Interestingly, several studies of MR-AC …",Matthias Hofmann and F Steinke and I Bezrukov and A Kolb and P Aschoff and M Lichy and M Erb and T Nägele and M Brady and B Schölkopf and B Pichler,2,16622125963949379365,Correction Techniques in Emission Tomography,,217,CRC Press,MR-based attenuation correction for PET/MR,http://books.google.com/books?hl=en&lr=&id=vqIczxYeXNcC&oi=fnd&pg=PA217&dq=info:JQ8AFX-nreYJ:scholar.google.com&ots=IYpVYDiO1r&sig=MSFCsw62QZ6_cAOPufBG2SqC0Xk,,2012,/scholar?cites=16622125963949379365,DZ-fHPgAAAAJ:WqliGbK-hY8C
14707,"Functional correlates of Rhythms in the gamma band (30-100Hz) are observed in the mammalian brain with a large variety of functional correlates. Nevertheless, their functional role is still debated. One way to disentangle this issue is to go beyond usual correlation analysis and apply causality measures that quantify the directed interactions between the gamma rhythms and other aspects of neural activity. These measures can be further compared with other aspects of neurophysicological signals to find markers of neural interactions. In a recent study, we analyzed extracellular recordings in the primary visual cortex of 4 anesthetized macaques during the presentation of movie stimuli using a causality measure named Transfer Entropy. We found causal interactions between high frequency gamma rhythms (60-100Hz) recorded in different electrodes, involving in particular their phase, and between the gamma phase and spiking activity quantified by the instantaneous envelope of the MUA band (1-3kHz). Here, we further investigate in the same dataset the meaning of these phase-MUA and phase-phase causal interactions by studying the distribution of phases at multiple recording sites at lags around the occurrence of spiking events. First, we found a sharpening of the gamma phase distribution in one electrode when spikes are occurring in other recording site. This phenomena appeared as a form of phase-spike synchronization and was quantified by an information theoretic measure. We found this measure correlates significantly with phase-MUA causal interactions. Additionally, we quantified in a similar way the interplay between spiking and …",M Besserve and Y Murayama and B Schölkopf and NK Logothetis and S Panzeri,2,17497384970747771724,,,,,High frequency phase-spike synchronization of extracellular signals modulates causal interactions in monkey primary visual cortex,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1788443,,2010,/scholar?cites=17497384970747771724,DZ-fHPgAAAAJ:uLbwQdceFCQC
14708,"We compare two image bases with respect to their capabilities for image modeling and steganalysis. The first basis consists of wavelets, the second is a Laplacian pyramid. Both bases are used to decompose the image into subbands where the local dependency structure is modeled with a linear Bayesian estimator. Similar to existing approaches, the image model is used to predict coefficient values from their neighborhoods, and the final classification step uses statistical descriptors of the residual. Our findings are counter-intuitive on first sight: Although Laplacian pyramids have better image modeling capabilities than wavelets, steganalysis based on wavelets is much more successful. We present a number of experiments that suggest possible explanations for this result.",Valentin Schwamberger and Pham Hai Dang Le and Bernhard Schölkopf and Matthias O Franz,2,17800053689459491869,,,133-144,"Springer, Berlin, Heidelberg",The influence of the image basis on modeling and steganalysis performance,https://link.springer.com/chapter/10.1007/978-3-642-16435-4_11,,2010,/scholar?cites=17800053689459491869,DZ-fHPgAAAAJ:Y0pCki6q_DkC
14709,"Maximizing some form of Poisson likelihood (either with or without penalization) is central to image reconstruction algorithms in emission tomography. In this paper we introduce NMML, a non-monotonic algorithm for maximum likelihood PET image reconstruction. NMML offers a simple and flexible procedure that also easily incorporates standard convex regularization for doing penalized likelihood estimation. A vast number image reconstruction algorithms have been developed for PET, and new ones continue to be designed. Among these, methods based on the expectation maximization (EM) and ordered-subsets (OS) framework seem to have enjoyed the greatest popularity. Our method NMML differs fundamentally from methods based on EM: i) it does not depend on the concept of optimization transfer (or surrogate functions); and ii) it is a rapidly converging nonmonotonic descent procedure. The greatest …",Suvrit Sra and Dongmin Kim and Inderjit Dhillon and Bernhard Schölkopf,2,14314346269174198458,,,2500-2502,IEEE,A new non-monotonic algorithm for PET image reconstruction,https://ieeexplore.ieee.org/abstract/document/5402060/,,2009,/scholar?cites=14314346269174198458,DZ-fHPgAAAAJ:69ZgNCALVd0C
14710,,Bharath K Sriperumbudur and Arthur Gretton and Kenji Fukumizu and Gert RG Lanckriet and Bernhard Schölkopf,2,898723693730431061,CoRR,,,,A Note on Integral Probability Metrics and ϕ-Divergences,http://scholar.google.com/scholar?cluster=898723693730431061&hl=en&oi=scholarr,,2009,/scholar?cites=898723693730431061,DZ-fHPgAAAAJ:gjFswcl-TRQC
14711,,Jeremy Hill and Jason Farquhar and Moritz Grosse-Wentrup and Suzanne Martens and Bernhard Schölkopf,2,2027897521723720446,"Report 2006-2009, MPI for Biological Cybernetics",,,,Development of Brain-Computer Interface Systems,,,2009,/scholar?cites=2027897521723720446,DZ-fHPgAAAAJ:_Re3VWB3Y0AC
14712,"Author: Hofmann, M et al.; Genre: Meeting Abstract; Published in Print: 2008-10; Title:
MR-Based PET Attenuation Correction: Initial Results for Whole Body.
",M Hofmann and F Steinke and P Aschoff and M Lichy and M Brady and B Schölkopf and BJ Pichler,2,8544688309314536876,,,,,MR-Based PET Attenuation Correction: Initial Results for Whole Body,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1789625,,2008,/scholar?cites=8544688309314536876,DZ-fHPgAAAAJ:654Jc7Ppz2kC
14713,"Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut être utilisé dans le cadre d’une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya señalado antes, el contenido de este registro bibliográfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS",William FREEMAN and Pietro PERONA and Bernhard SCHÖLKOPF,2,1680531502222040743,International journal of computer vision,1-3,,,Special Issue on Machine Learning for Vision,https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=20248434,77,2008,/scholar?cites=1680531502222040743,DZ-fHPgAAAAJ:18X6PRg5UAMC
14714,,M Hofmann and F Steinke and V Scheel and G Charpiat and J Farquhar and P Aschoff and M Brady and B Schlkopf and BJ Pichler,2,11735874128756035066,Journal of Nuclear Medicine,11,1875-1883,,MR-based attenuation correction for PET/MR: a novel approach combining atlas registration and recognition of local patterns,http://scholar.google.com/scholar?cluster=11735874128756035066&hl=en&oi=scholarr,49,2008,/scholar?cites=11735874128756035066,DZ-fHPgAAAAJ:x0AULgBdw-EC
14715,"This technical report is merely an extended version of the appendix of Steinke et. al."" Manifold-valued Thin-Plate Splines with Applications in Computer Graphics""(2008) with complete proofs, which had to be omitted due to space restrictions. This technical report requires a basic knowledge of differential geometry. However, apart from that requirement the technical report is self-contained.",Matthias Hein and Florian Steinke and Bernhard Schölkopf,2,3118219918304599399,,,,Max Planck Institute for Biological Cybernetics,Energy functionals for manifold-valued mappings and their properties,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790098,,2008,/scholar?cites=3118219918304599399,DZ-fHPgAAAAJ:B3FOqHPlNUQC
14716,"One can define a classification rule in H based on the closest mean, ie, using a hyperplane with normal vector µ (X)− µ (Y)[4]. This begs the question: when is this normal vector zero (in which case it does not define a hyperplane)? For polynomial kernels k (x, x)=(〈 x, x〉+ 1) d, this amounts to all empirical moments up to order d vanishing. For strictly positive definite kernels, the means coincide only if X= Y, rendering µ injective:",B Schölkopf and BK Sriperumbudur and A Gretton and K Fukumizu,2,13445689615678195448,,,,Mathematisches Forschungsinstitut,"RKHS Representation of Measures Applied to Homogeneity, Independence, and Fourier Optics",https://pure.mpg.de/rest/items/item_1789728/component/file_3035514/content,,2008,/scholar?cites=13445689615678195448,DZ-fHPgAAAAJ:M05iB0D1s5AC
14717,"The neural processing of visual motion is of essential importance for course control. A basic model suggesting a possible mechanism of how such a computation could be implemented in the fly visual system is the so called"" correlation-type motion detector"" proposed by Reichardt and Hassenstein in the 1950s. The basic requirement to reconstruct the neural circuit underlying this computation is the availability of electron microscopic 3D data sets of whole ensembles of neurons constituting the fly visual ganglia. We apply a new technique,"" Serial Block Face Scanning Electron Microscopy""(SBFSEM), that allows for an automatic sectioning and imaging of biological tissue with a scanning electron microscope [Denk, Horstman (2004) Serial block face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure. PLOS Biology 2: 1900-1909]. Image Stacks generated with this technology have a resolution sufficient to distinguish different cellular compartments, especially synaptic structures. Consequently detailed anatomical knowledge of complete neuronal circuits can be obtained. Such an image stack contains several thousands of images and is recorded with a minimal voxel size of 25nm in x and y and 30nm in z direction. Consequently a tissue block of 1mm³ (volume of the Calliphora vicina brain) produces several hundreds terabyte of data. Therefore new concepts for managing large data sets and for automated 3D reconstruction algorithms need to be developed. We developed an automated image segmentation and 3D reconstruction software, which allows a precise contour tracing of cell membranes and …",N Maack and C Kapfer and JH Macke and B Schölkopf and W Denk and A Borst,2,6167302199225832961,,,1195,,3D reconstruction of neural circuits from serial EM images,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790541,,2007,/scholar?cites=6167302199225832961,DZ-fHPgAAAAJ:CUa0UMr99BoC
14718,"We develop a Bayesian “sum-of-trees” model, named BART, where each tree is constrained by a prior to be a weak learner. Fitting and inference are accomplished via an iterative backfitting MCMC algorithm. This model is motivated by ensemble methods in general, and boosting algorithms in particular. Like boosting, each weak learner (i.e., each weak tree) contributes a small amount to the overall model. However, our procedure is defined by a statistical model: a prior and a likelihood, while boosting is defined by an algorithm. This model-based approach enables a full and accurate assessment of uncertainty in model predictions, while remaining highly competitive in terms of predictive accuracy.",Bernhard Schölkopf and John Platt and Thomas Hofmann,2,11035928075146685572,,,265-272,MIT Press,Bayesian Ensemble Learning,https://ieeexplore.ieee.org/abstract/document/6287361/,,2007,/scholar?cites=11035928075146685572,DZ-fHPgAAAAJ:tdKF98QEuhYC
14719,"Lemma ([Dudley, 2002]) Let (X, d) be a separable metric space, and let P, Q be two Borel probability measures defined on X. Then P= Q if and only if EP [f (x)]= EQ [f (x)],∀ f∈ C (X), where C (X) is the space of bounded continuous functions on X.",Bharath K Sriperumbudur and Arthur Gretton and Kenji Fukumizu and Bernhard Schölkopf,2,5366407807011067193,"Representations and Inference on Probability Distributions Workshop, NIPS",,,,The effect of kernel choice of RKHS based statistical tests,http://personal.psu.edu/bks18/MMD-workshop.pdf,,2007,/scholar?cites=5366407807011067193,DZ-fHPgAAAAJ:Ye8fVhOjtu8C
14720,"This invention relates generally to a novel CaM multi functional protein kinase, which has been named Pregnancy Up-Regulated, Nonubiquitous CaM Kinase (PNCK), and to the nucleotide sequence encoding it. The kinase is tempo rally expressed during postnatal mammary development in a spatially heterogeneous manner in certain Subsets of cells, and overexpressed in a Subset of primary breast cancers. The invention further relates to an analysis of a correlation between carcinogenesis and postnatal development, particu larly mammary development, especially associated with parity; as well as to methods of using the kinase, or gene encoding it, as markers, prognostic tools, Screening tools and therapies, in vitro and in vivo that are based upon that correlation.",,2,4228522918080210241,,,,,"Pregnancy up-regulated, nonubiquitous CaM kinase",https://patents.google.com/patent/US7041495B2/en,,2006,/scholar?cites=4228522918080210241,DZ-fHPgAAAAJ:ogNjDCfJQuQC
14721,"The computation of classical higher-order statistics such as higher-order moments or spectra is difficult for images due to the huge number of terms to be estimated and interpreted. We propose an alternative approach in which multiplicative pixel interactions are described by a series of Wiener functionals. Since the functionals are estimated implicitly via polynomial kernels, the combinatorial explosion associated with the classical higher-order statistics is avoided. In addition, the kernel framework allows for estimating infinite series expansions and for the regularized estimation of the Wiener series. First results show that image structures such as lines or corners can be predicted correctly, and that pixel interactions up to the order of five play an important role in natural images.",Matthias O Franz and Bernhard Schölkopf,2,10578897835121158073,,,106-107,,Implicit Volterra and Wiener series for higher-order image analysis,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1791112,,2006,/scholar?cites=10578897835121158073,DZ-fHPgAAAAJ:Cjb_BMXg7WwC
14722,"1. Given an image or image sequence, set up a weighted graph G=(V, E) and set the weight on the edge connection two nodes to be a measure of the similarity between the two nodes. 2. Solve for eigenvectors with the smallest eigenvalues. 3. Use the eigenvector with the second smallest eigenvalue to bipartition the graph. 4. Decide if the current partition should be subdivided and recursively repartition the segmented parts if necessary.",M Wu and B Schölkopf,2,1999412290055498991,Proceedings of Annual Conference on Neural Information Processing Systems,,1529-1536,,Normalized cuts and image segmentation,https://www.stat.washington.edu/wxs/Stat593-s03/Student-presentations/NormalizedCuts.pdf,,2006,/scholar?cites=1999412290055498991,DZ-fHPgAAAAJ:thyth0UfN58C
14723,"The main aim of this study is to unravel the mechanisms responsible for classification and memorization of visual stimuli in the human brain using methods from machine learning. We perform a visual gender discrimination task of images from a human face database. Man and machine are studied using an embedded feedback loop methodology as explained below. In a first classification experiment the human subjects are asked to classify faces from a subset of the database according to their gender. Their responses, ie estimated gender, reaction time and confidence rating are recorded. The gender estimated by the subject together with a low-dimensional representation obtained using Principal Component Analysis of the presented stimuli then form the subject’s personal dataset. On this dataset, various linear separating hyperplane algorithms obtained from machine learning, Support Vector Machines, Relevance Vector Machines, mean-of-class prototype learners and K-means clustering combined with a nearest-neigbour classifier, are trained. The separating hyperplane (SH) is then computed for each of these algorithms using a possibly sparse subset of the subject’s dataset: the representations. We observe that stimuli far from the SH are classified more accurately, faster and with higher confidence than those near to the SH. A new dataset, consisting of stimuli drawn from the representations, from the dataset seen by the subject and from a subset of the database which has not been presented to the subject is created on-line. In a subsequent memory experiment the subject is asked to classify the stimuli from this dataset as seen or unseen in …",ABA Graf and FA Wichmann and HH Bülthoff and B Schölkopf,2,7008936891916305796,,,72,Cold Spring Harbor Laboratory,Classification and memory behaviour of man revisited by machine,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792020,,2004,/scholar?cites=7008936891916305796,DZ-fHPgAAAAJ:ITz7TVZQ0sgC
14724,"For IEEE to continue sending you helpful information on our products and services, please consent 
to our updated Privacy Policy … I have read and accepted the IEEE Privacy Policy … A 
not-for-profit organization, IEEE is the world's largest technical professional organization dedicated 
to advancing technology for the benefit of humanity. © Copyright 2020 IEEE - All rights 
reserved. Use of this web site signifies your agreement to the terms and conditions.  ",Bernhard Schölkopf and Koji Tsuda and Jean-Philippe Vert,2,16779735625660829289,,,275-275,MIT Press,Advanced Application of Support Vector Machines,https://ieeexplore.ieee.org/abstract/document/6282629/,,2004,/scholar?cites=16779735625660829289,DZ-fHPgAAAAJ:mEGMijfW7KEC
14725,"This chapter contains sections titled: Introduction, Biological Background, Methods, Results and Discussion, Conclusion, Appendix: Data Generation",Bernhard Schölkopf and Koji Tsuda and Jean-Philippe Vert,2,9696737725600794740,,,277-298,MIT Press,Accurate splice site detection for Caenorhabditis elegans,https://ieeexplore.ieee.org/abstract/document/6282646/,,2004,/scholar?cites=9696737725600794740,DZ-fHPgAAAAJ:Uk-XF1zu03IC
14726,"This chapter contains sections titled: Common Issues in Classifying Gene Expression Profiles, A Review of Feature Selection Methods for Kernel Machines, The Joint Classifier and Feature Optimization Algorithm, Experimental Studies Comparing the Methods, Discussion, Availability of Software, Appendix: Derivation for Q-Function and E-Step",Bernhard Schölkopf and Koji Tsuda and Jean-Philippe Vert,2,5414133618132098447,,,299-317,MIT Press,Gene expression analysis: joint feature selection and classifier design,https://ieeexplore.ieee.org/abstract/document/6282645/,,2004,/scholar?cites=5414133618132098447,DZ-fHPgAAAAJ:tGwqb3G7jokC
14727,,Bernhard Schölkopf and Koji Tsuda,2,11803081691181073621,,,,MIT press,"Vert, Jean-Philippe. Kernel methods in computational biology",http://scholar.google.com/scholar?cluster=11803081691181073621&hl=en&oi=scholarr,,2004,/scholar?cites=11803081691181073621,DZ-fHPgAAAAJ:AdCmT_kuKwQC
14728,"Die Lerntheorie befasst sich mit der Extraktion von Gesetzmäßigkeiten aus Beobachtungen. Das Grundproblem hierbei ist die, Generalisierung:’die extrahierten Gesetzmäßigkeiten sollen nicht nur die bereits vorliegenden Beobachtungen (die, Trainingsmenge’) korrekt erklären, sondern auch für neue Beobachtungen zutreffend sein. Dieses Problem der Induktion berührt Grundsatzfragen nicht nur der Statistik, sondern der empirischen Wissenschaften im Allgemeinen. Dazu gehören die Repräsentation von Daten und von Vorwissen, sowie die Komplexität oder Kapazität von Erklärungen bzw. Modellen. Wenn anhand eines Modells geringer Komplexität (in einer geeigneten Formalisierung dieses Begriffs) eine gegebene Menge von empirischen Beobachtungen hinreichend genau erklärt werden kann, dann garantiert die statistische Lerntheorie, dass mit hoher Wahrscheinlichkeit auch zukünftige Beobachtungen mit dem Modell konsistent sein werden.",Bernhard Schölkopf,2,2438130355383350896,Jahrbuch der Max-Planck-Gesellschaft,,377-382,Max-Planck-Gesellschaft zur Förderung der Wissenschaften,Statistische Lerntheorie und Empirische Inferenz,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792193,2004,2004,/scholar?cites=2438130355383350896,DZ-fHPgAAAAJ:fEOibwPWpKIC
14729,,Xie Xiaofeng and Zhang Wenjun and Yang Zhilian,2,16021361713091800786,Control &Determine,2,1129-134,,Particle swarm optimization,http://scholar.google.com/scholar?cluster=16021361713091800786&hl=en&oi=scholarr,18,2003,/scholar?cites=16021361713091800786,DZ-fHPgAAAAJ:BDYw5ed5dIcC
14730,,A Piccolboni and C Schindelhauer,2,851803771240298624,,,208-223,"Springer, Berlin, Heidelberg","Discrete prediction games with arbitrary feedback and loss. Computational Learning Theory Lecture Notes in Computer Science, Vol. 2111",http://scholar.google.com/scholar?cluster=851803771240298624&hl=en&oi=scholarr,,2001,/scholar?cites=851803771240298624,DZ-fHPgAAAAJ:NBmyyahqHj0C
14731,,Bernhard Schölkopf,2,11565921629674046580,Proc. of Neural,,,,Tutorial: SVM and Kernel methods,http://scholar.google.com/scholar?cluster=11565921629674046580&hl=en&oi=scholarr,,2001,/scholar?cites=11565921629674046580,DZ-fHPgAAAAJ:9YU4pG19l3EC
14732,"In support vector (SV) regression, a parameter /spl nu/ controls the number of support vectors and the number of points that come to lie outside of the so-called /spl epsi/-insensitive tube. For various noise models and SV parameter settings, we experimentally determine the values of /spl nu/ that lead to the lowest generalization error. We find good agreement with the values that had previously been predicted by a theoretical argument based on the asymptotic efficiency of a simplified model of SV regression.",Athanassia Chalimourda and B Scholkopf and Alexander J Smola,2,14695958984471277693,,,199-204,IEEE,Choosing/spl nu/in support vector regression with different noise models-theory and experiments,https://ieeexplore.ieee.org/abstract/document/861457/,5,2000,/scholar?cites=14695958984471277693,DZ-fHPgAAAAJ:eO3_k5sD8BwC
14733,"Effective methods of capacity control via uniform convergence bounds for function expansions have been largely limited to Support Vector machines, where good bounds are obtainable by the entropy number approach. We extend these methods to systems with expansions in terms of arbitrary (parametrized) basis functions and a wide range of regularization methods covering the whole range of general linear additive models. This is achieved by a data dependent analysis of the eigenvalues of the corresponding design matrix.",Alex J Smola and John Shawe-Taylor and Bernhard Schölkopf and Robert C Williamson,2,2216545840835694818,,,342-348,,The entropy regularization information criterion,http://papers.nips.cc/paper/1677-the-entropy-regularization-information-criterion.pdf,,2000,/scholar?cites=2216545840835694818,DZ-fHPgAAAAJ:wbdj-CoPYUoC
14734,,B Scholkopf and H Mallot,2,14496066036339334650,Arbeitsgruppe Biilthoff,,,,View-based cognitive mapping and path-finding,http://scholar.google.com/scholar?cluster=14496066036339334650&hl=en&oi=scholarr,7,1994,/scholar?cites=14496066036339334650,DZ-fHPgAAAAJ:nRcIc8bqtusC
14735,,C Burges Schlkopf and A Smola,2,14826950014118737764,,,,"Cambridge, MA: MIT Press",Advances in Kernel Methods-Support Vector Learning (pp. 327-352),http://scholar.google.com/scholar?cluster=14826950014118737764&hl=en&oi=scholarr,,,/scholar?cites=14826950014118737764,DZ-fHPgAAAAJ:-tmR84mPQEwC
14736,,B Schlkopf and A Smola,2,11657980737869398321,"Neural Computation, as a Kernel Eigenvalue Problem1996",,,,Nonlinear component analysis,http://scholar.google.com/scholar?cluster=11657980737869398321&hl=en&oi=scholarr,,,/scholar?cites=11657980737869398321,DZ-fHPgAAAAJ:19eLPh0qeHwC
14737,,A Smola and TT Friess and B Schlkopf,2,8685338488477903805,"MJ Kearns, SA Solla, and DA Cohn",,585-591,,Semiparametric support vector and 570 linear programming machines,http://scholar.google.com/scholar?cluster=8685338488477903805&hl=en&oi=scholarr,571,,/scholar?cites=8685338488477903805,DZ-fHPgAAAAJ:hE9Q4kiZolAC
14738,,D Zhou,2,7518993473338653049,18th Annual Conf. on Neural Information Processing System,,237-244,,"Bousquet 0, Lal TN, Weston J, and Schlkopf B. 2003. Learning with Local and Global Consistency [C]",http://scholar.google.com/scholar?cluster=7518993473338653049&hl=en&oi=scholarr,16,,/scholar?cites=7518993473338653049,DZ-fHPgAAAAJ:dSOt4g9qevQC
14739,,Zhou Dengyong and Huang Jiayuan,2,14778575440291183129,Proceedings of 20th Annual Conference on Neural Information Processing SysterrLs,,1601-1608,,"Bernhard Scholkopf Learning with Hypergraphs: Clustering, Classification, and Embedding",http://scholar.google.com/scholar?cluster=14778575440291183129&hl=en&oi=scholarr,2,,/scholar?cites=14778575440291183129,DZ-fHPgAAAAJ:XEKcsfOMjpkC
14740,,Sebastian Mika and Bernhard Schölkopf and AJ Smola and K Müller and Matthias Scholz and Gunnar Rätsch,2,9895100806629417872,DOWNLOAD.(Cited on page 67.),,,,Kernel PCA and De-Noising in Feature Spaces; 1998,http://scholar.google.com/scholar?cluster=9895100806629417872&hl=en&oi=scholarr,,,/scholar?cites=9895100806629417872,DZ-fHPgAAAAJ:XLtXma4zydAC
14741,,Joris Mooij and Dominik Janzing and Bernhard Schölkopf,2,2713543529456823088,URL http://www. causality. inf. ethz. ch/repository. php,,,,"Distinguishing between cause and effect, 2008",http://scholar.google.com/scholar?cluster=2713543529456823088&hl=en&oi=scholarr,,,/scholar?cites=2713543529456823088,DZ-fHPgAAAAJ:ItCcSVbosjYC
14742,,Bernhard Scholkopf and A Srnola,2,16721030774310205407,,,,"Cambridge, MA: MIT Press",Learning will: kezrnols,http://scholar.google.com/scholar?cluster=16721030774310205407&hl=en&oi=scholarr,,,/scholar?cites=16721030774310205407,DZ-fHPgAAAAJ:K2M-mhe8R-QC
14743,,Robert C Williamson and Alex J Smola and Bernhard Scholkopf,2,6715871355788425080,,,,,"Entropy Numbers, Operators and Support Vector Kernels,"" submitted to Euro-COLT'99. See also\Generalization Performance of Regularization Networks and Support Vector Machines …",http://scholar.google.com/scholar?cluster=6715871355788425080&hl=en&oi=scholarr,100,,/scholar?cites=6715871355788425080,DZ-fHPgAAAAJ:MB4u-SWKN0MC
14744,"While the success of semi-supervised learning (SSL) is still not fully understood, Schölkopf et al.(2012) have established a link to the principle of independent causal mechanisms. They conclude that SSL should be impossible when predicting a target variable from its causes, but possible when predicting it from its effects. Since both these cases are restrictive, we extend their work by considering classification using cause and effect features at the same time, such as predicting a disease from both risk factors and symptoms. While standard SSL exploits information contained in the marginal distribution of all inputs (to improve the estimate of the conditional distribution of the target given in-puts), we argue that in our more general setting we should use information in the conditional distribution of effect features given causal features. We explore how this insight generalises the previous understanding, and how it relates to and can be exploited algorithmically for SSL.",Julius Kügelgen and Alexander Mey and Marco Loog and Bernhard Schölkopf,1,8450573069998714944,,,1-10,PMLR,"Semi-supervised learning, causality, and the conditional cluster assumption",http://proceedings.mlr.press/v124/kugelgen20a.html,,2020,/scholar?cites=8450573069998714944,DZ-fHPgAAAAJ:OzO231GTI64C
14745,"Dexterous object manipulation remains an open problem in robotics, despite the rapid progress in machine learning during the past decade. We argue that a hindrance is the high cost of experimentation on real systems, in terms of both time and money. We address this problem by proposing an open-source robotic platform which can safely operate without human supervision. The hardware is inexpensive (about\SI {5000}[\$]{}) yet highly dynamic, robust, and capable of complex interaction with external objects. The software operates at 1-kilohertz and performs safety checks to prevent the hardware from breaking. The easy-to-use front-end (in C++ and Python) is suitable for real-time control as well as deep reinforcement learning. In addition, the software framework is largely robot-agnostic and can hence be used independently of the hardware proposed herein. Finally, we illustrate the potential of the proposed platform through a number of experiments, including real-time optimal control, deep reinforcement learning from scratch, throwing, and writing.",Manuel Wüthrich and Felix Widmaier and Felix Grimminger and Joel Akpo and Shruti Joshi and Vaibhav Agrawal and Bilal Hammoud and Majid Khadiv and Miroslav Bogdanovic and Vincent Berenz and Julian Viereck and Maximilien Naveau and Ludovic Righetti and Bernhard Schölkopf and Stefan Bauer,1,3731018994806532289,arXiv preprint arXiv:2008.03596,,,,Trifinger: An open-source robot for learning dexterity,https://arxiv.org/abs/2008.03596,,2020,/scholar?cites=3731018994806532289,DZ-fHPgAAAAJ:BAmsVx-ZrPgC
14746,"Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalize well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. We accomplish this by abstracting the modeled dynamical system as a collection of autonomous but sparsely interacting sub-systems. The sub-systems interact according to a topology that is learned, but also informed by the spatial structure of the underlying real-world system. This results in a class of models that are well suited for modeling the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modeling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalization to novel tasks without additional training, even when compared against strong baselines that perform equally well or better on the training distribution.",Nasim Rahaman and Anirudh Goyal and Muhammad Waleed Gondal and Manuel Wuthrich and Stefan Bauer and Yash Sharma and Yoshua Bengio and Bernhard Schölkopf,1,12555610987950068833,arXiv preprint arXiv:2007.06533,,,,S2RMs: Spatially Structured Recurrent Modules,https://arxiv.org/abs/2007.06533,,2020,/scholar?cites=12555610987950068833,DZ-fHPgAAAAJ:l0Ye9Vd5hjsC
14747,"We study the problem of structuring a learned representation to significantly improve performance without supervision. Unlike most methods which focus on using side information like weak supervision or defining new regularization objectives, we focus on improving the learned representation by structuring the architecture of the model. We propose a self-attention based architecture to make the encoder explicitly associate parts of the representation with parts of the input observation. Meanwhile, our structural decoder architecture encourages a hierarchical structure in the latent space, akin to structural causal models, and learns a natural ordering of the latent mechanisms. We demonstrate how these models learn a representation which improves results in a variety of downstream tasks including generation, disentanglement, and transfer using several challenging and natural image datasets.",Felix Leeb and Yashas Annadani and Stefan Bauer and Bernhard Schölkopf,1,12444536670100923878,arXiv preprint arXiv:2006.07796,,,,Structural autoencoders improve representations for generation and transfer,https://arxiv.org/abs/2006.07796,,2020,/scholar?cites=12444536670100923878,DZ-fHPgAAAAJ:MPsQPTIloacC
14748,"Learning controllers merely based on a performance metric has been proven effective in many physical and non-physical tasks in both control theory and reinforcement learning. However, in practice, the controller must guarantee some notion of safety to ensure that it does not harm either the agent or the environment. Stability is a crucial notion of safety, whose violation can certainly cause unsafe behaviors. Lyapunov functions are effective tools to assess stability in nonlinear dynamical systems. In this paper, we combine an improving Lyapunov function with automatic controller synthesis to obtain control policies with large safe regions. We propose a two-player collaborative algorithm that alternates between estimating a Lyapunov function and deriving a controller that gradually enlarges the stability region of the closed-loop system. We provide theoretical results on the class of systems that can be treated with the proposed algorithm and empirically evaluate the effectiveness of our method using an exemplary dynamical system.",Arash Mehrjou and Mohammad Ghavamzadeh and Bernhard Schölkopf,1,8404477422029679367,arXiv preprint arXiv:2006.03947,,,,Automatic Policy Synthesis to Improve the Safety of Nonlinear Dynamical Systems,https://arxiv.org/abs/2006.03947,,2020,/scholar?cites=8404477422029679367,DZ-fHPgAAAAJ:AoZQGyKLABkC
14749,"In the context of model-based reinforcement learning and control, a large number of methods for learning system dynamics have been proposed in recent years. The purpose of these learned models is to synthesize new control policies. An important open question is how robust current dynamics-learning methods are to shifts in the data distribution due to changes in the control policy. We present a real-robot dataset which allows to systematically investigate this question. This dataset contains trajectories of a 3 degrees-of-freedom (DOF) robot being controlled by a diverse set of policies. For comparison, we also provide a simulated version of the dataset. Finally, we benchmark a few widely-used dynamics-learning methods using the proposed dataset. Our results show that the iid test error of a learned model is not necessarily a good indicator of its accuracy under control policies different from the one which …",Diego Agudelo-Espana and Andrii Zadaianchuk and Philippe Wenk and Aditya Garg and Joel Akpo and Felix Grimminger and Julian Viereck and Maximilien Naveau and Ludovic Righetti and Georg Martius and Andreas Krause and Bernhard Schölkopf and Stefan Bauer and Manuel Wüthrich,1,18062964276783627697,,,8151-8157,IEEE,A real-robot dataset for assessing transferability of learned dynamics models,https://ieeexplore.ieee.org/abstract/document/9197392/,,2020,/scholar?cites=18062964276783627697,DZ-fHPgAAAAJ:JChhaDHAsWcC
14750,"The electronic band structure (BS) of solid state materials imprints the multidimensional and multi-valued functional relations between energy and momenta of periodically confined electrons. Photoemission spectroscopy is a powerful tool for its comprehensive characterization. A common task in photoemission band mapping is to recover the underlying quasiparticle dispersion, which we call band structure reconstruction. Traditional methods often focus on specific regions of interests yet require extensive human oversight. To cope with the growing size and scale of photoemission data, we develop a generic machine-learning approach leveraging the information within electronic structure calculations for this task. We demonstrate its capability by reconstructing all fourteen valence bands of tungsten diselenide and validate the accuracy on various synthetic data. The reconstruction uncovers previously inaccessible momentum-space structural information on both global and local scales in conjunction with theory, while realizing a path towards integrating band mapping data into materials science databases.",Rui Patrick Xian and Vincent Stimper and Marios Zacharias and Shuo Dong and Maciej Dendzik and Samuel Beaulieu and Bernhard Schölkopf and Martin Wolf and Laurenz Rettig and Christian Carbogno and Stefan Bauer and Ralph Ernstorfer,1,15444997941983200773,arXiv preprint arXiv:2005.10210,,,,A machine learning route between band mapping and band structure,https://arxiv.org/abs/2005.10210,,2020,/scholar?cites=15444997941983200773,DZ-fHPgAAAAJ:5oqvLvXezWAC
14751,"We study the identification of direct and indirect causes on time series and provide necessary and sufficient conditions in the presence of latent variables. Our theoretical results and estimation algorithms require two conditional independence tests for each observed candidate time series to determine whether or not it is a cause of an observed target time series. We provide experimental results in simulations, where the ground truth is known, as well as in real data. Our results show that our method leads to essentially no false positives and relatively low false negative rates, even in confounded environments with non-unique lag effects, outperforming the common method of Granger causality.",Atalanti A Mastakouri and Bernhard Schölkopf and Dominik Janzing,1,742046268077293502,arXiv preprint arXiv:2005.08543,,,,Necessary and sufficient conditions for causal feature selection in time series with latent common causes,https://arxiv.org/abs/2005.08543,,2020,/scholar?cites=742046268077293502,DZ-fHPgAAAAJ:RXVP-JhfB5kC
14752,"We study the problem usually referred to as group testing in the context of COVID-19. Given  samples taken from patients, how should we select mixtures of samples to be tested, so as to maximize information and minimize the number of tests? We consider both adaptive and non-adaptive strategies, and take a Bayesian approach with a prior both for infection of patients and test errors. We start by proposing a mathematically principled objective, grounded in information theory. We then optimize non-adaptive optimization strategies using genetic algorithms, and leverage the mathematical framework of adaptive sub-modularity to obtain theoretical guarantees for the greedy-adaptive method.",Louis Abraham and Gary Bécigneul and Bernhard Schölkopf,1,16771398844561613284,arXiv preprint arXiv:2005.06413,,,,Crackovid: Optimizing Group Testing,https://arxiv.org/abs/2005.06413,,2020,/scholar?cites=16771398844561613284,DZ-fHPgAAAAJ:zmAw5nq0afAC
14753,"In order to anticipate rare and impactful events, we propose to quantify the worst-case risk under distributional ambiguity using a recent development in kernel methods--the kernel mean embedding. Specifically, we formulate the generalized moment problem whose ambiguity set (ie, the moment constraint) is described by constraints in the associated reproducing kernel Hilbert space in a nonparametric manner. We then present the tractable optimization formulation and its theoretical justification. As a concrete application, we numerically test the proposed method in characterizing the worst-case constraint violation probability in the context of a constrained stochastic control system.",Jia-Jie Zhu and Wittawat Jitkrittum and Moritz Diehl and Bernhard Schölkopf,1,4155148018682474805,arXiv preprint arXiv:2004.00166,,,,Worst-Case Risk Quantification under Distributional Ambiguity using Kernel Mean Embedding in Moment Problem,https://arxiv.org/abs/2004.00166,,2020,/scholar?cites=4155148018682474805,DZ-fHPgAAAAJ:Iz9D5oSlWWsC
14754,"We apply kernel mean embedding methods to sample-based stochastic optimization and control. Specifically, we use the reduced-set expansion method as a way to discard sampled scenarios. The effect of such constraint removal is improved optimality and decreased conservativeness. This is achieved by solving a distributional-distance-regularized optimization problem. We demonstrated this optimization formulation is well-motivated in theory, computationally tractable, and effective in numerical algorithms.",Jia-Jie Zhu and Bernhard Schölkopf and Moritz Diehl,1,3318561831415439795,arXiv preprint arXiv:2001.10398,,,,A Kernel Mean Embedding Approach to Reducing Conservativeness in Stochastic Programming and Control,https://arxiv.org/abs/2001.10398,,2020,/scholar?cites=3318561831415439795,DZ-fHPgAAAAJ:31YcTh_BSL0C
14755,"Riemannian geometry-based methods have shown to be effective in many sorts of Brain-Computer Interface (BCI) applications, but are only capable of measuring the power of the measured signal. This paper proposes a set of novel features derived via the Hilbert transform and applies them to the generalized Riemannian manifold, the Hermitian manifold, to see whether the classification accuracy benefits from this treatment. To validate these features, we benchmark them with the Mother of All BCI Benchmarks framework, a recently introduced tool to make BCI methods research more reproducible. The results indicate that in some settings the analytic covariance matrix can improve BCI performance.",Jiachen Xu and Vinay Jayaram and Bernhard Schölkopf and Moritz Grosse-Wentrup,1,8669106047592074775,,,965-968,IEEE,Feature extraction from the Hermitian manifold for Brain-Computer Interfaces,https://ieeexplore.ieee.org/abstract/document/8717011/,,2019,/scholar?cites=8669106047592074775,DZ-fHPgAAAAJ:amtGwOyh6YUC
14756,"Online detection of instantaneous changes in the generative process of a data sequence generally focuses on retrospective inference of such change points without considering their future occurrences. We extend the Bayesian Online Change Point Detection algorithm to also infer the number of time steps until the next change point (ie, the residual time). This enables us to handle observation models which depend on the total segment duration, which is useful to model data sequences with temporal scaling. In addition, we extend the model by removing the iid assumption on the observation model parameters. The resulting inference algorithm for segment detection can be deployed in an online fashion, and we illustrate applications to synthetic and to two medical real-world data sets.",Diego Agudelo-España and Sebastian Gomez-Gonzalez and Stefan Bauer and Bernhard Schölkopf and Jan Peters,1,13380886534879037274,arXiv preprint arXiv:1902.04524,,,,Bayesian Online Detection and Prediction of Change Points,https://arxiv.org/abs/1902.04524,,2019,/scholar?cites=13380886534879037274,DZ-fHPgAAAAJ:hG410n-4ytkC
14757,"Understanding the principles of causal inference in the visual system has a long history at least since the seminal studies by Albert Michotte. Many cognitive and machine learning scientists believe that intelligent behavior requires agents to possess causal models of the world. Recent ML algorithms exploit the dependence structure of additive noise terms for inferring causal structures from observational data, eg to detect the direction of time series; the arrow of time. This raises the question whether the subtle asymmetries between the time directions can also be perceived by humans. Here we show that human observers can indeed discriminate forward and backward autoregressive motion with non-Gaussian additive independent noise, ie they appear sensitive to subtle asymmetries between the time directions. We employ a so-called frozen noise paradigm enabling us to compare human performance with four different algorithms on a trial-by-trial basis: A causal inference algorithm exploiting the dependence structure of additive noise terms, a neurally inspired network, a Bayesian ideal observer model as well as a simple heuristic. Our results suggest that all human observers use similar cues or strategies to solve the arrow of time motion discrimination task, but the human algorithm is significantly different from the three machine algorithms we compared it to. In fact, our simple heuristic appears most similar to our human observers.",Kristof Meding and Dominik Janzing and Bernhard Schölkopf and Felix A Wichmann,1,2690486525333910605,,,2306-2317,,Perceiving the arrow of time in autoregressive motion,http://papers.nips.cc/paper/8502-perceiving-the-arrow-of-time-in-autoregressive-motion,,2019,/scholar?cites=2690486525333910605,DZ-fHPgAAAAJ:iOUJLkeIIkEC
14758,"Motivation/background: Methodological advances in metagenome assembly are rapidly increasing in the number of published metagenome assemblies. However, identifying misassemblies is challenging due to a lack of closely related reference genomes that can act as pseudo ground truth. Existing reference-free methods are no longer maintained, can make strong assumptions that may not hold across a diversity of research projects, and have not been validated on large scale metagenome assemblies. Results: We present DeepMAsED, a deep learning approach for identifying misassembled contigs without the need for reference genomes. Moreover, we provide an in silico pipeline for generating large-scale, realistic metagenome assemblies for comprehensive model training and testing. DeepMAsED accuracy substantially exceeds the state-of-the-art when applied to large and complex metagenome assemblies. Our model estimates close to a 5% contig misassembly rate in two recent large-scale metagenome assembly publications. Conclusions: DeepMAsED accurately identifies misassemblies in metagenome-assembled contigs from a broad diversity of Bacteria and Archaea without the need for reference genomes or strong modelling assumptions. Running DeepMAsED is straight-forward, as well as is model re-training with our dataset generation pipeline. Therefore, DeepMAsED is a flexible misassembly classifier that can be applied to a wide range of metagenome assembly projects. Availability: DeepMAsED is available from GitHub at https://github.com/leylabmpi/DeepMAsED",Mateo Rojas-Carulla and Ruth E Ley and Bernhard Schölkopf and Nicholas D Youngblut,1,14499901596577022585,BioRxiv,,763813,Cold Spring Harbor Laboratory,DeepMAsED: Evaluating the quality of metagenomic assemblies,https://www.biorxiv.org/content/10.1101/763813v1.abstract,,2019,/scholar?cites=14499901596577022585,DZ-fHPgAAAAJ:C62MIdXOJf4C
14759,"An efficient representation of observed data has many benefits in various domains of engineering and science. Representing static data sets, such as images, is a living branch in machine learning and eases downstream tasks, such as classification, regression, or decision making. However, the representation of dynamical systems has received less attention. In this work, we develop a method to represent a dynamical system efficiently as a combination of a state and a local model, which fulfills a criterion inspired by the minimum description length (MDL) principle. The MDL principle is used in machine learning and statistics to quantify the trade-off between the ability to explain seen data and the model complexity. Networked control systems are a prominent example, where such a representation is beneficial. When many agents share a network, information exchange is costly and should thus happen only when …",Friedrich Solowjow and Arash Mehrjou and Bernhard Schölkopf and Sebastian Trimpe,1,15442587883390953292,,,6073-6079,IEEE,Efficient Encoding of Dynamical Systems through Local Approximations,https://ieeexplore.ieee.org/abstract/document/8619232/,,2018,/scholar?cites=15442587883390953292,DZ-fHPgAAAAJ:xqVt0NE8Gh4C
14760,"Phase artifacts due to B0 inhomogeneity can severely degrade the quality of MR images. The artifacts are particularly prominent in long‐TE scans and usually appear as ghosting and blur. We propose a retrospective phase correction method based on autofocusing. The proposed method uses raw data acquired with standard imaging sequences, and does not rely on navigators or external measures of field inhomogeneity.We formulate and solve the optimization problem, where we seek the latent phase offsets that are associated with an optimal value of the image quality measure that is evaluated in the spatial domain. As a quality measure we use entropy computed on spatial image gradients. We propose two types of objective function, both compatible with parallel imaging and accelerated image acquisition.We evaluate the method on both synthetic and real data. In real data case …",Alexander Loktyushin and Philipp Ehses and Bernhard Schölkopf and Klaus Scheffler,1,7169351058626933066,Magnetic Resonance in Medicine,3,958-968,,Autofocusing‐based phase correction,https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27092,80,2018,/scholar?cites=7169351058626933066,DZ-fHPgAAAAJ:jblQE3dAagYC
14761,"We introduce groupICA, a novel independent component analysis (ICA) algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) structure in data and hence provides a justified alternative to the use of ICA on data blindly pooled across groups. In addition to our theoretical framework, we explain its causal interpretation and motivation, provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data and run experiments on publicly available EEG datasets demonstrating the applicability to real-world scenarios. We provide a scikit-learn compatible pip-installable Python package groupICA as well as R and Matlab implementations accompanied by a documentation and an audible example at https://sweichwald. de/groupICA.",Niklas Pfister and Sebastian Weichwald and Peter Bühlmann and Bernhard Schölkopf,1,9111278890256142373,stat,,4,,groupICA: Independent component analysis for grouped data,https://ps.is.tue.mpg.de/uploads_file/attachment/attachment/430/PfiWeiBuhSch18.pdf,1050,2018,/scholar?cites=9111278890256142373,DZ-fHPgAAAAJ:tDl1h8iI-dgC
14762,"Encoding a sequence of observations is an essential task with many applications. The encoding can become highly efficient when the observations are generated by a dynamical system. A dynamical system imposes regularities on the observations that can be leveraged to achieve a more efficient code. We propose a method to encode a given or learned dynamical system. Apart from its application for encoding a sequence of observations, we propose to use the compression achieved by this encoding as a criterion for model selection. Given a dataset, different learning algorithms result in different models. But not all learned models are equally good. We show that the proposed encoding approach can be used to choose the learned model which is closer to the true underlying dynamics. We provide experiments for both encoding and model selection, and theoretical results that shed light on why the approach works.",Arash Mehrjou and Friedrich Solowjow and Sebastian Trimpe and Bernhard Schölkopf,1,9203467863306408212,arXiv preprint arXiv:1805.10615,,,,A Local Information Criterion for Dynamical Systems,https://arxiv.org/abs/1805.10615,,2018,/scholar?cites=9203467863306408212,DZ-fHPgAAAAJ:FSqkM6QGvPQC
14763,"Author: Schölkopf, B; Genre: Newspaper Article; Published in Print: 2018-03-16;
Keywords: Abt. Schölkopf; Title: Kybernetische Revolution.
",Bernhard Schölkopf,1,812616804698099440,Süddeutsche Zeitung,,,,Kybernetische Revolution,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3066542,,2018,/scholar?cites=812616804698099440,DZ-fHPgAAAAJ:k-ciFsyH9-YC
14764,"Difference imaging or image subtraction is a method that measures differential photometry by matching the pointing and point-spread function (PSF) between image frames. It is used for the detection of time-variable phenomena. Here we present a new category of method---CPM Difference Imaging, in which differences are not measured between matched images but instead between image frames and a data-driven predictive model that has been designed only to predict the pointing, PSF, and detector effects but not astronomical variability. In CPM Difference Imaging each pixel is modelled by the Causal Pixel Model (CPM) originally built for modeling Kepler data, in which pixel values are predicted by a linear combination of other pixels at the same epoch but far enough away such that these pixels are causally disconnected, astrophysically. It does not require that the user have any explicit model or description of the pointing or point-spread function of any of the images. Its principal drawback is that---in its current form---it requires an imaging campaign with many epochs and fairly stable telescope pointing. The method is applied to simulated data and also the K2 Campaign 9 microlensing data. We show that CPM Difference Imaging can detect variable objects and produce precise differentiate photometry in a crowded field. CPM Difference Imaging is capable of producing image differences at nearly photon-noise precision.",Dun Wang and David W Hogg and Daniel Foreman-Mackey and Bernhard Schölkopf,1,2013591277962905042,arXiv preprint arXiv:1710.02428,,,,A pixel-level model for event discovery in time-domain imaging,https://arxiv.org/abs/1710.02428,,2017,/scholar?cites=2013591277962905042,DZ-fHPgAAAAJ:jf_cyAkbRPUC
14765,"Cognitive brain-computer interfaces (BCIs) are an auspicious alternative to BCIs based on motor tasks for severely paralyzed patients, eg, those in late-stages of amyotrophic lateral sclerosis. These patients, however, are often not able to volitionally control their eye lids: Undeliberate eye opening and closing affects modulation of theta-and alpha-rhythms, which impairs decoding performance in cognitive BCIs. Here, we demonstrate on EEG data recorded from nine healthy subjects that a cognitive BCI based on task-induced modulation of the frequency of the parietal alpha-rhythm is more robust to eye lid movements than a BCI based on amplitude modulation. Specifically, we instructed subjects to either open or close their eyes while performing cognitive tasks, and show that closing their eyes decreases decoding performance relative to the eyesopen condition for amplitude modulation but not for frequency modulation features. This insight has important consequences for the design of cognitive BCIs for severely paralyzed patients.",Marius Goerner and Bernhard Schölkopf and Moritz Grosse-Wentrup,1,14148273857404034793,,,,,Closing One's eyes Affects amplitude modulation but not frequency modulation in a Cognitive BCI.,https://www.researchgate.net/profile/Marius_Goerner/publication/331234163_CLOSING_ONE'S_EYES_AFFECTS_AMPLITUDE_MODULATION_BUT_NOT_FREQUENCY_MODULATION_IN_A_COGNITIVE_BCI/links/5c6db2064585156b570d3560/CLOSING-ONES-EYES-AFFECTS-AMPLITUDE-MODULATION-BUT-NOT-FREQUENCY-MODULATION-IN-A-COGNITIVE-BCI.pdf,,2017,/scholar?cites=14148273857404034793,DZ-fHPgAAAAJ:fMXnj547Ig4C
14766,"A method for correcting B o fluctuation-induced ghosting artifacts in long-TE gradient-echo scan images, comprising the steps of: acquiring an image (u); determining phase offsets (Φ); and applying the phase offsets (Φ) to the image (u); such that an entropy of the spatial intensity variations in the corrected image (u) decreases.",,1,6217756839688815232,,,,,Autofocusing-based correction of B0 fluctuation-induced ghosting,https://patents.google.com/patent/US20170212202A1/en,,2017,/scholar?cites=6217756839688815232,DZ-fHPgAAAAJ:hNW1902727sC
14767,"The expansion of brain-computer interfaces (BCIs) to outside the research laboratory has historically been hampered by their difficulty of use. Well-functioning BCIs often require many channels, which can be difficult to properly prepare and require expert support. Low-channel setups, however, can lead to poor or unreliable classification of intent. Here we introduce a novel method for extracting more information from a single EEG channel and test it on a ten subject motor imagery dataset. Instead of looking at bandpower or phase synchrony, we test the average frequency within each trial to see if there are task-dependent changes in the spectral locations of neural frequency peaks. We show that using this feature in combination with standard bandpower features is significantly better than bandpower features alone across subjects, both for standard electrodes and electrodes that include a Laplacian filter.",Vinay Jayaram and Bernhard Schölkopf and Moritz Grosse-Wentrup,1,12850634457308604222,,,321-324,IEEE,Frequency peak features for low-channel classification in motor imagery paradigms,https://ieeexplore.ieee.org/abstract/document/8008355/,,2017,/scholar?cites=12850634457308604222,DZ-fHPgAAAAJ:_Ib3u7pdcXQC
14768,"The Dagstuhl Seminar on 16481"" New Directions for Learning with Kernels and Gaussian Processes"" brought together two principal theoretical camps of the machine learning community at a crucial time for the field. Kernel methods and Gaussian process models together form a significant part of the discipline's foundations, but their prominence is waning while more elaborate but poorly understood hierarchical models are ascendant. In a lively, amiable seminar, the participants re-discovered common conceptual ground (and some continued points of disagreement) and productively discussed how theoretical rigour can stay relevant during a hectic phase for the subject.",Arthur Gretton and Philipp Hennig and Carl Edward Rasmussen and Bernhard Schölkopf,1,13046685112449987903,,11,,Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,New directions for learning with kernels and gaussian processes (dagstuhl seminar 16481),https://drops.dagstuhl.de/opus/volltexte/2017/7106/,6,2017,/scholar?cites=13046685112449987903,DZ-fHPgAAAAJ:bZ-w8ItpEJYC
14769,"Dual control, the simultaneous identification and control of dynamic systems, is an idea that has been around for several decades without being widely used in applications, due to the fundamental intractability of the optimal solution. Available algorithms are either complex and computationally demanding, or reduce to a simple change of the cost function, which can lead to poor average performance. In addition, classic dual control schemes do not deal with constraints and economic cost structures present in many applications. In this paper, we aim at facilitating the use of dual control algorithms based on a series expansion of the cost function for such systems. In particular, this is realized by employing reference tracking of the optimal mean trajectory together with soft constraints. A key feature of the proposed formulation is that it maintains the value of information in the cost, making dual control tractable with all dual …",Edgar D Klenske and Philipp Hennig and Bernhard Scholkopf and Melanie N Zeilinger,1,11106920956840518681,,,800-806,IEEE,Approximate dual control maintaining the value of information with an application to building control,https://ieeexplore.ieee.org/abstract/document/7810387/,,2016,/scholar?cites=11106920956840518681,DZ-fHPgAAAAJ:ybo9GWVbjlMC
14770,"Cloud computing involves complex technical and economical systems and interactions. This brings about various challenges, two of which are:(1) debugging and control of computing systems, based on heterogeneous data, and (2) prediction of performance and price of “spot” resources, allocated via auctions. In this paper, we first establish two theoretical results on approximate causal inference. We then use the first one, approximate counterfactuals, along with established causal methodology, to outline a general framework to address (1). To address (2), we show how the second one, approximate integration of causal knowledge, can in principle provide a tool for cloud clients to trade off privacy against predictability of cloud costs. We report experiments on simulated and real data.",Philipp Geiger and Lucian Carata and Bernhard Schölkopf,1,9200828985684334960,arXiv preprint arXiv,,,,Causal inference for cloud computing,http://scholar.google.com/scholar?cluster=9200828985684334960&hl=en&oi=scholarr,1603,2016,/scholar?cites=9200828985684334960,DZ-fHPgAAAAJ:u_MDJdmB8EcC
14771,"Brain-Computer Interfaces (BCIs) often rely on low-level cognitive processes known to be impaired in late stages of amyotrophic lateral sclerosis (ALS). We propose a BCI for ALS patients based on self-regulation of neuronal oscillations in the superior parietal lobule, which is less affected by ALS than motor and sensory cortices. We describe a case of self-regulation of band power in gamma range (55-85 Hz) based on feedback from the parietal cortex by an ALS patient, resulting in a mean offline two-class decoding accuracy of 79.2% across four sessions. Despite a good offline decoding accuracy, a source localisation analysis revealed that gamma-power modulation was not spatially localized, suggesting confounding by non-cortical artifacts. Theta-power in contrast, showed a strong localized response in the precuneus. As such, this may be an alternative possibility of using self-regulation of neuronal oscillations …",Tatiana Fomina and Bernhard Schölkopf and Moritz Grosse-Wentrup,1,17019860133565006079,,,77-80,IEEE,Towards cognitive brain-computer interfaces for patients with amyotrophic lateral sclerosis,https://ieeexplore.ieee.org/abstract/document/7332703/,,2015,/scholar?cites=17019860133565006079,DZ-fHPgAAAAJ:xrERVq8-JpsC
14772,"From training data from several related domains (or tasks), methods of domain adaptation try to combine knowledge to improve performance. This paper discusses an approach to domain adaptation which is inspired by a causal interpretation of the multi-task problem. We assume that a covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant with respect to shifts in those predictors (covariates). We propose to learn the corresponding conditional expectation in the training domains and use it for estimation in the target domain. We further introduce a method which allows for automatic inference of the above subset in regression and classification. We study the performance of this approach in an adversarial setting, in the case where no additional examples are available in the test domain. If a labeled sample is available, we provide a method for using both the transferred invariant conditional and task specific information. We present results on synthetic data sets and a sentiment analysis problem.",Mateo Rojas-Carulla and Bernhard Schölkopf and Richard Turner and Jonas Peters,1,6944769838016057547,stat,,19,,A Causal Perspective on Domain Adaptation,https://www.researchgate.net/profile/Richard_Turner18/publication/280243342_A_Causal_Perspective_on_Domain_Adaptation/links/5742bfcd08aea45ee84a6365/A-Causal-Perspective-on-Domain-Adaptation.pdf,1050,2015,/scholar?cites=6944769838016057547,DZ-fHPgAAAAJ:LGPsv5YoZhkC
14773,"Information spreads across social and technological networks, but often the network structures are hidden and we only observe the traces left by the diffusion processes, called cascades. It is known that, under a popular continuous-time diffusion model, as long as the model parameters satisfy a natural incoherence condition, it is possible to recover the correct network structure with high probability if we observe O (d3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. However, the incoherence condition depends, in a nontrivial way, on the source (node) distribution of the cascades, which is typically unknown. Our open problem is whether it is possible to design an active algorithm which samples the source locations in a sequential manner and achieves the same or even better sample complexity, eg, o (d3 i log N), than previous work.",Manuel Gomez-Rodriguez and Le Song and Bernhard Schoelkopf,1,13481461849184261075,,,1276-1279,,Open problem: Finding good cascade sampling processes for the network inference problem,http://www.jmlr.org/proceedings/papers/v35/gomezrodriguez14.pdf,,2014,/scholar?cites=13481461849184261075,DZ-fHPgAAAAJ:UarirCmVI0EC
14774,"We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images. The method makes use of pixelrank information in the individual input images to improve a “consensus” pixel rank in the combined image. Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons. We apply the method to images of the night sky (of unknown provenance) discovered on the Web. The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually. More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system.",Dustin Lang and David Hogg and Bernhard Schölkopf,1,7508921832513140007,,,549-557,,Towards building a Crowd-Sourced Sky Map,http://www.jmlr.org/proceedings/papers/v33/lang14.pdf,,2014,/scholar?cites=7508921832513140007,DZ-fHPgAAAAJ:PZE8UkGerEcC
14775,"The application of brain-computer interfaces (BCI) shows promising results in stroke rehabilitation, but the underlying neural substrates and processes of successful BCI-based neurorehabilitation remain unclear. The goal of our present work was to identify the brain areas associated with successful visuomotor integration and motor learning (VMIL), and investigate their connection with successful sensorimotor-rhythm (SMR)-modulation commonly used in stroke rehabilitation related BCI systems. Our hypothesis was that neural processes associated with VMIL are linked to characteristics of the SMR, and thus share a common neural basis. Preliminary results indicate that the areas used to predict the current state of VMIL overlap with, but are not confined to, those areas used for SMR-based BCI training in stroke rehabilitation. This supports our hypothesis that VMIL and successful SMR modulation used in …",Timm Meyer and Jan Peters and Thorsten O Zander and Doris Brötz and Surjo R Soekadar and Bernhard Schölkopf and Moritz Grosse-Wentrup,1,14033795342235315171,,,617-621,"Springer, Berlin, Heidelberg",Investigating the Neural Basis of Brain-Computer Interface (BCI)-based Stroke Rehabilitation,https://link.springer.com/chapter/10.1007/978-3-642-34546-3_100,,2013,/scholar?cites=14033795342235315171,DZ-fHPgAAAAJ:Og1tA8FjbJAC
14776,"Predicting a CT image or a map of the linear attenuation coefficients from the information provided by magnetic resonance imaging (MRI) is a challenging task. This problem is of significant importance for combined positron emission tomography (PET)/MRI scanners, as quantitative PET image reconstruction requires an attenuation map. In PET/CT this attenuation map is derived from the CT scan or from a rotating source, however, current PET/MR systems can not directly measure attenuation images - and indeed it is desirable to save the patient from the additional radiation exposure. Recent approaches tackle this problem by using MR sequences with ultra-short echo times (UTE). At the price of lower effective image resolution, the UTE image yields signal from bone and therefore provides valuable information for calculating the attenuation map. We propose a novel approach to this problem based on nonnegative …",Michael Hirsch and Matthias Hofmann and Frederic Mantlik and Bernd J Pichler and Bernhard Schölkopf and Michael Habeck,1,15063013018170680677,,,2953-2956,IEEE,A blind deconvolution approach for pseudo CT prediction from MR image pairs,https://ieeexplore.ieee.org/abstract/document/6467519/,,2012,/scholar?cites=15063013018170680677,DZ-fHPgAAAAJ:YPNY0knpFBYC
14777,"An automatic particle picking algorithm for processing electron micrographs of a large molecular complex, the 26S proteasome, is described. The algorithm makes use of a coherence enhancing diffusion filter to denoise the data, and a random forest classifier for removing false positives. It does not make use of a 3D reference model, but uses a training set of manually picked particles instead. False positive and false negative rates of around 25 to 30 are achieved on a testing set. The algorithm was developed for a specific particle, but contains steps that should be useful for developing automatic picking algorithms for other particles.",Paul Joubert and Stephan Nickell and Florian Beck and Michael Habeck and Michael Hirsch and Bernhard Schölkopf,1,12862423082001944847,,,1-6,,Automatic particle picking using diffusion filtering and random forest classification,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1788023,,2011,/scholar?cites=12862423082001944847,DZ-fHPgAAAAJ:C33y2ycGS3YC
14778,"Cross-spectral density (CSD), is widely used to find linear dependency between two real or complex valued time series. We define a non-linear extension of this measure by mapping the time series into two Reproducing Kernel Hilbert Spaces. The dependency is quantified by the Hilbert Schmidt norm of a cross-spectral density operator between these two spaces. We prove that, by choosing a characteristic kernel for the mapping, this quantity detects any pairwise dependency between the time series. Then we provide a fast estimator for the Hilbert-Schmidt norm based on the Fast Fourier Trans form. We demonstrate the interest of this approach to quantify non-linear dependencies between frequency bands of simulated signals and intra-cortical neural recordings.",Michel Besserve and Dominik Janzing and Nikos K Logothetis and Bernhard Schölkopf,1,10071091803154081715,,,2080-2083,IEEE,Finding dependencies between frequencies with the kernel cross-spectral density,https://ieeexplore.ieee.org/abstract/document/5946735/,,2011,/scholar?cites=10071091803154081715,DZ-fHPgAAAAJ:65Yg0jNCQDAC
14779,"Combined PET/MR provides at the same time molecular and functional imaging as well as excellent soft tissue contrast. It does not allow one to directly measure the attenuation properties of scanned tissues, despite the fact that accurate attenuation maps are necessary for quantitative PET imaging. Several methods have therefore been proposed for MR-based attenuation correction (MR-AC). So far, they have only been evaluated on data acquired from separate MR and PET scanners. We evaluated several MR-AC methods on data from 10 patients acquired on a combined BrainPET/MR scanner. This allowed the consideration of specific PET/MR issues, such as the RF coil that attenuates and scatters 511 keV gammas. We evaluated simple MR thresholding methods as well as atlas and machine learning-based MR-AC. CT-based AC served as gold standard reference. To comprehensively evaluate the MR-AC accuracy, we used RoIs from 2 anatomic brain atlases with different levels of detail. Visual inspection of the PET images indicated that even the basic FLASH threshold MR-AC may be sufficient for several applications. Using a UTE sequence for bone prediction in MR-based thresholding occasionally led to false prediction of bone tissue inside the brain, causing a significant overestimation of PET activity. Although it yielded a lower mean underestimation of activity, it exhibited the highest variance of all methods. The atlas averaging approach had a smaller mean error, but showed high maximum overestimation on the RoIs of the more detailed atlas. The Nave Bayes and Atlas-Patch MR-AC yielded the smallest variance, and the Atlas-Patch …",F Mantlik and M Hofmann and I Bezrukov and A Kolb and T Beyer and M Reimold and BJ Pichler and B Schölkopf,1,3274743551679877894,,,,,Comparative quantitative evaluation of MR-Based attenuation correction methods in combined brain PET/MR,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1788455,,2010,/scholar?cites=3274743551679877894,DZ-fHPgAAAAJ:oE_QS-WwsdAC
14780,"Brain-Computer Interfaces based on electrocorticography (ECoG) or electroencephalography (EEG), in combination with robot-assisted active physical therapy, may support traditional rehabilitation procedures for patients with severe motor impairment due to cerebrovascular brain damage caused by stroke. In this short report, we briefly review the state-of-the art in this exciting new field, give an overview of the work carried out at the Max Planck Institute for Biological Cybernetics and the University of Tübingen, and discuss challenges that need to be addressed in order to move from basic research to clinical studies.",M Gomez Rodriguez and J Peters and J Hill and B Schölkopf and M Grosse-Wentrup,1,8219669980612997346,,,59-63,,Combining real-time brain-computer interfacing and robot control for stroke rehabilitation,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1788429,,2010,/scholar?cites=8219669980612997346,DZ-fHPgAAAAJ:e5wmG9Sq2KIC
14781,"Cryo-electron microscopy (cryo-EM) plays an increasingly prominent role in structure elucidation of macromolecular assemblies. Advances in experimental instrumentation and computational power have spawned numerous cryo-EM studies of large biomolecular complexes resulting in the reconstruction of three-dimensional density maps at intermediate and low resolution. In this resolution range, identification and interpretation of structural elements and modeling of biomolecular structure with atomic detail becomes problematic. In this paper, we present a novel algorithm that enhances the resolution of intermediate- and low-resolution density maps. Our underlying assumption is to model the low-resolution density map as a blurred and possibly noise-corrupted version of an unknown high-resolution map that we seek to recover by deconvolution. By exploiting the nonnegativity of both the high-resolution …",Michael Hirsch and Bernhard Schölkopf and Michael Habeck,1,13754596803100160344,,,174-188,"Springer, Berlin, Heidelberg",A new algorithm for improving the resolution of Cryo-EM density maps,https://link.springer.com/chapter/10.1007/978-3-642-12683-3_12,,2010,/scholar?cites=13754596803100160344,DZ-fHPgAAAAJ:SeFeTyx0c_EC
14782,"This chapter presents a novel computer graphics approach suitable for the 3D tracking of facial movements. This kernel-based approach for constructing 3D facial animations also provides information needed for realistic and controllable facial animation and dynamic analyses of face space. This approach, which provides a fully automatic, markerless tracking system, creates a platform where an intersection takes place between psychology and other research areas such as computer graphics and machine learning. The chapter highlights the need to develop a system in the future that can improve the mesh regularization terms in a face-specific manner by using feedback and findings from previous tracking systems.",C Walder and M Breidt and HH Bülthoff and B Schölkopf and C Curio,1,7558009740372190955,,,,MIT Press,Markerless tracking of dynamic 3D scans of faces,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1788407,,2010,/scholar?cites=7558009740372190955,DZ-fHPgAAAAJ:hC7cP41nSMkC
14783,"We develop an incremental generalized expectation maximization (GEM) framework to model the multiframe blind deconvolution problem. A simplistic version of this problem was recently studied by Harmeling etal~ citeharmeling09}. We solve a more realistic version of this problem which includes the following major features:(i) super-resolution ability emph {despite noise and unknown blurring;(ii) saturation-correction, ie, handling of overexposed pixels that can otherwise confound the image processing; and (iii) simultaneous handling of color channels. These features are seamlessly integrated into our incremental GEM framework to yield simple but efficient multiframe blind deconvolution algorithms. We present technical details concerning critical steps of our algorithms, especially to highlight how all operations can be written using matrix-vector multiplications. We apply our algorithm to real-world images from astronomy and super resolution tasks. Our experimental results show that our methods yield improve d resolution and deconvolution at the same time.",Stefan Harmeling and Suvrit Sra and Michael Hirsch and Bernhard Schölkopf,1,1878061652578761221,,,,Max Planck Institute for Biological Cybernetics,"An Incremental GEM Framework for Multiframe Blind Deconvolution, Super-Resolution, and Saturation Correction",https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1789028,,2009,/scholar?cites=1878061652578761221,DZ-fHPgAAAAJ:ZfRJV9d4-WMC
14784,"For simple visual patterns under the experimenter's control we impose which information, or features, an observer can use to solve a given perceptual task. For natural vision tasks, however, there are typically a multitude of potential features in a given visual scene which the visual system may be exploiting when analyzing it: edges, corners, contours, etc. Here we describe a novel non-linear system identification technique based on modern machine learning methods that allows the critical features an observer uses to be inferred directly from the observer's data. The method neither requires stimuli to be embedded in noise nor is it limited to linear perceptive fields (classification images). We demonstrate our technique by deriving the critical image features observers fixate in natural scenes (bottom-up visual saliency). Unlike previous studies where the relevant structure is determined manually—eg by selecting …",Felix A Wichmann and Wolf Kienzle and Bernhard Schölkopf and Matthias Franz,1,12468256280792996422,Journal of Vision,8,32-32,The Association for Research in Vision and Ophthalmology,Non-linear system identification: Visual saliency inferred from eye-movement data,https://jov.arvojournals.org/article.aspx?articleid=2135725,9,2009,/scholar?cites=12468256280792996422,DZ-fHPgAAAAJ:NaGl4SEjCO4C
14785,We introduce the use of graphical models in the decoding process of brain-computer interface (BCI) visual speller data. The standard decoding implicitly assumes a simple graphical model which does not incorporate overlap and refractory effects of the brain signals. We propose a more realistic graphical model that does incorporate these effects. The decoding that follows from the graphical model involves the use of multiple classifiers. Our approach is tested on real visual speller data. The results show that the proposed method slightly outperforms the standard decoding method.,Suzanna Martens and Jason Farquhar and Jeremy Hill and Bernhard Scholkopf,1,12250432307136765412,,,470-473,IEEE,Graphical models for decoding in BCI visual speller systems,https://ieeexplore.ieee.org/abstract/document/5109335/,,2009,/scholar?cites=12250432307136765412,DZ-fHPgAAAAJ:QsaTk4IG4EwC
14786,"MAP reconstructions for Cartesian undersampled data for 64 of 256 acquired lines. Four different k-space schemes, each consisting of the central 32 lines plus 32 lines positioned by different scenarios are compared to the fully sampled image (center). The given error is the squared difference between the fully sampled and the undersampled images. The optimized k-space scheme shows significantly better recovery of detail than images reconstructed from other undersampled data.",Matthias Seeger and Hannes Nickisch and Rolf Pohmann and Bernhard Schölkopf,1,16707890064330981286,reconstruction,,2,,Optimization of k-Space Trajectories by Bayesian Experimental Design,https://www.researchgate.net/profile/Hannes_Nickisch/publication/267405590_Optimization_of_k-Space_Trajectories_by_Bayesian_Experimental_Design/links/54f9ac720cf29a9fbd7c451b.pdf,50,2009,/scholar?cites=16707890064330981286,DZ-fHPgAAAAJ:d1gkVwhDpl0C
14787,"A system and method (12) for determining a property map (82) for an object, in particular for a living being based on at least one first image (84) in particular a nuclear magnetic resonance image, an MR image, of the object. According to the method (12), a structure for reference pairs is defined in a first step (96), each reference pair (16-26), having at least two entries (62). The first entry represents a property value, in particular, an attenuation value. The second entry (62) preferably represents a group of related image points (67), in particular, extracted from MR images (28) comprising an image point of interest corresponding to the property value. A number of training pairs (16-26) are prepared in a further step (98) of the method (12). A structure for the training pairs (16-26) corresponds to the structure of the reference pairs and the entries for the training pairs (16-26) are known. A relationship between the first entries and the further entries (62-66) of the training pairs (16-26) is established by machine learning in a further step (100) of the method (12) in order to be able to predict a corresponding property value (88) for any point (90) of the first image (84).",B Pichler and M Hofmann and B Schölkopf and F Steinke,1,18275185398650904072,,,,,Method for determining a property map for an object in particular for a living being based on at least one first image in particular a nuclear magnetic resonance image,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3168011,,2008,/scholar?cites=18275185398650904072,DZ-fHPgAAAAJ:HrvnDIceeq0C
14788,"In which way do the local image statistics at the center of gaze differ from those at randomly chosen image locations? In 1999, Reinagel and Zador [1] showed that RMS contrast is significantly increased around fixated locations in natural images. Since then, numerous additional hypotheses have been proposed, based on edge content, entropy, self-information, higher-order statistics, or sophisticated models such as that of Itti and Koch [2]. While these models are rather different in terms of the used image features, they hardly differ in terms of their predictive power. This complicates the question of which bottom-up mechanism actually drives human eye movements. To shed some light on this problem, we analyze the nonlinear receptive fields of an eye movement model which is purely data-driven. It consists of a nonparametric radial basis function network, fitted to human eye movement data. To avoid a bias towards specific image features such as edges or corners, we deliberately chose raw pixel values as the input to our model, not the outputs of some filter bank. The learned model is analyzed by computing its optimal stimuli. It turns our that there are two maximally excitatory stimuli, both of which have center-surround structure, and two maximally inhibitory stimuli which are basically flat. We argue that these can be seen as nonlinear receptive fields of the underlying system. In particular, we show that a small radial basis function network with the optimal stimuli as centers predicts unseen eye movements as precisely as the full model. The fact that center-surround filters emerge from a simple optimality criterion—without any prior assumption that …",Wolf Kienzle and Felix A Wichmann and Bernhard Schölkopf and Matthias O Franz,1,14581715567722412640,,,207,,Center-surround filters emerge from optimizing predictivity in a free-viewing task,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790619,,2007,/scholar?cites=14581715567722412640,DZ-fHPgAAAAJ:VL0QpB8kHFEC
14789,"Using extensions of linear algebra concepts to Reproducing Kernel Hilbert Spaces (RKHS), we define a unifying framework for random walk kernels on graphs. Reduction to a Sylvester equation allows us to compute many of these kernels in O(n 3 ) worst-case time. This includes kernels whose previous worst-case time complexity was O(n 6 ), such as the geometric kernels of Gårtner et al. [1] and the marginal graph kernels of Kashima et al. [2]. Our algebra in RKHS allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods, yielding sub-cubic computational complexity when combined with conjugate gradient solvers or fixed-point iterations. Experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches.",Bernhard Schölkopf and John Platt and Thomas Hofmann,1,4211706458744885898,,,,MIT Press,Fast computation of graph kernels,https://ieeexplore.ieee.org/abstract/document/6287399/,,2007,/scholar?cites=4211706458744885898,DZ-fHPgAAAAJ:qJTPUp8CQbAC
14790,"Identification of stimulus-response functions is a central problem in systems neuroscience and related areas. Prominent examples are the estimation of receptive fields and classification images [1]. In most cases, the relationship between a high-dimensional input and the system output is modeled by a linear (first-order) or quadratic (second-order) model. Models with third or higher order dependencies are seldom used, since both parameter estimation and model interpretation can become very difficult. Recently, Wu and Gallant [3] proposed the use of kernel methods, which have become a standard tool in machine learning during the past decade [2]. Kernel methods can capture relationships of any order, while solving the parameter estmation problem efficiently. In short, the stimuli are mapped into a high-dimensional feature space, where a standard linear method, such as linear regression or Fisher discriminant, is applied. The kernel function allows for doing this implicitly, with all computations carried out in stimulus space. As a consequence, the resulting model is nonlinear, but many desirable properties of linear methods are retained. For example, the estimation problem has no local minima, which is in contrast to other nonlinear approaches, such as neural networks [4]. Unfortunately, although kernel methods excel at modeling complex functions, the question of how to interpret the resulting models remains. In particular, it is not clear how receptive fields should be defined in",Wolf Kienzle and Jakob H Macke and Felix A Wichmann and Bernhard Schölkopf and Matthias O Franz,1,823783329956932750,,,,,Nonlinear receptive field analysis: Making kernel methods interpretable,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.8576,,2007,/scholar?cites=823783329956932750,DZ-fHPgAAAAJ:1qzjygNMrQYC
14791,"This chapter contains sections titled: The Semi-Supervised Learning Problem, Paradigms for Semi-Supervised Learning, Examples, Conclusions",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,1,18351052668983633820,,,15-31,MIT Press,A taxonomy for Semi-supervised learning methods,https://ieeexplore.ieee.org/abstract/document/6280907/,,2006,/scholar?cites=18351052668983633820,DZ-fHPgAAAAJ:0m9iHBJCm-UC
14792,"This chapter contains sections titled: Introduction, Metric Structure of Supervised Learning, Model Selection, Regularization, Classification, Conclusion, Acknowledgments",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,1,16229997395308100576,,,421-451,MIT Press,Metric-Based Approaches for Semi-Supervised Regression and Classification,https://ieeexplore.ieee.org/abstract/document/6280887/,,2006,/scholar?cites=16229997395308100576,DZ-fHPgAAAAJ:0HLgouInvOQC
14793,"This chapter contains sections titled: Introduction, A Generative Model for Text, Experimental Results with Basic EM, Using a More Expressive Generative Model, Overcoming the Challenges of Local Maxima, Conclusions and Summary",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,1,2339248757465306396,,,33-55,MIT Press,Semi-Supervised Text Classification Using EM,https://ieeexplore.ieee.org/abstract/document/6280906/,,2006,/scholar?cites=2339248757465306396,DZ-fHPgAAAAJ:8XpgKc9YVYQC
14794,"This chapter contains sections titled: Introduction, Label Propagation on a Similarity Graph, Quadratic Cost Criterion, From Transduction to Induction, Incorporating Class Prior Knowledge, Curse of Dimensionality for Semi-Supervised Learning, Discussion, Acknowledgments",Olivier Chapelle and Bernhard Schölkopf and Alexander Zien,1,18401891895878271343,,,193-216,MIT Press,Label Propagation and Quadratic Criterion,https://ieeexplore.ieee.org/abstract/document/6280898/,,2006,/scholar?cites=18401891895878271343,DZ-fHPgAAAAJ:_KHj4d_VctIC
14795,,Olivier Chapelle and Bernhard Scholkopf and Alexander Zien,1,8673795428373993833,,,,"MIT Press, Cambridge, MA","Semi-Supervised Learning, chapter Analysis of Benchmarks",http://scholar.google.com/scholar?cluster=8673795428373993833&hl=en&oi=scholarr,5,2006,/scholar?cites=8673795428373993833,DZ-fHPgAAAAJ:Y-CEfswgrBAC
14796,"We present easy-to-use alternatives to the often-used two-stage Common Spatial Pattern+ classifier approach for spatial filtering and classification of Event-Related Desychnronization signals in BCI. We report two algorithms that aim to optimize the spatial filters according to a criterion more directly related to the ability of the algorithms to generalize to unseen data. Both are based upon the idea of treating the spatial filter coefficients as hyperparameters of a kernel or covariance function. We then optimize these hyper-parameters directly along side the normal classifier parameters with respect to our chosen learning objective function. The two objectives considered are margin maximization as used in Support-Vector Machines and the evidence maximization framework used in Gaussian Processes. Our experiments assessed generalization error as a function of the number of training points used, on 9 BCI competition data sets and 5 offline motor imagery data sets measured in Tubingen. Both our approaches sho w consistent improvements relative to the commonly used CSP+ linear classifier combination. Strikingly, the improvement is most significant in the higher noise cases, when either few trails are used for training, or with the most poorly performing subjects. This a reversal of the usual"" rich get richer"" effect in the development of CSP extensions, which tend to perform best when the signal is strong enough to accurately find their additional parameters. This makes our approach particularly suitable for clinical application where high levels of noise are to be expected.",Jason Farquhar and Jeremy Hill and Bernhard Schölkopf,1,6500674414620379603,,,,,Optimizing Spatial Filters for BCI: Margin-and Evidence-Maximization Approaches,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1790726,,2006,/scholar?cites=6500674414620379603,DZ-fHPgAAAAJ:AXPGKjj_ei8C
14797,"Like many other researchers, we have previously found that blind source separation using Independent Components Analysis (ICA) can significantly improve classification performance in single-trial brain signal classification. The following figure illustrates the data presented by Hill et al.(2005)[Advances in Neural Information Processing Systems 17, 569–576], in which auditory ERPs were classified in a binary (left-vs-right) attention-based BCI.",N Jeremy Hill and Michael Schröder and Thomas Navin Lal and Bernhard Schölkopf,1,16227927504634313011,,,,,Comparative evaluation of independent components analysis algorithms for isolating target-relevant information in brain-signal classification,https://pure.mpg.de/rest/items/item_1791449/component/file_3177890/content,,2005,/scholar?cites=16227927504634313011,DZ-fHPgAAAAJ:dQ2og3OwTAUC
14798,"Author: Birbaumer, N et al.; Genre: Meeting Abstract; Published in Print: 2005-02; Title: Invasive
and Non-Invasive Brain-Computer-Interfaces for Communication in Locked-in Syndrome.
",N Birbaumer and G Widmann and C Elger and M Schröder and T Hinterberger and TN Lal and B Schölkopf and M Tatagiba and D Freudenstein,1,11062524423169974839,,,239,,Invasive and non-invasive brain-computer-interfaces for communication in locked-in syndrome,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3178533,,2005,/scholar?cites=11062524423169974839,DZ-fHPgAAAAJ:qyG9-fBpYwcC
14799,"We are delighted to present the proceedings of DAGM 2004, and wish to-press our gratitude to the many people whose e? orts made the success of the conference possible. We received 146 contributions of which we were able to-cept 22 as oral presentations and 48 as posters. Each paper received 3 reviews, upon which decisions were based. We are grateful for the dedicated work of the 38 members of the program committee and the numerous referees. The careful review process led to the exciting program which we are able to present in this volume. Among the highlights of the meeting were the talks of our four invited spe-ers, renowned experts in areas spanning learning in theory, in vision and in robotics:–William T. Freeman, Arti? cial Intelligence Laboratory, MIT: Sharing F-tures for Multi-class Object Detection–PietroPerona, Caltech: TowardsUnsupervisedLearningofObjectCategories–StefanSchaal, DepartmentofComputerScience, UniversityofSouthernC-ifornia: Real-Time Statistical Learning for Humanoid Robotics–Vladimir Vapnik, NEC Research Institute: Empirical Inference WearegratefulforeconomicsupportfromHondaResearchInstituteEurope, ABW GmbH, Transtec AG, DaimlerChrysler, and Stemmer Imaging GmbH, which enabled us to? nance best paper prizes and a limited number of travel grants. Many thanks to our local support Sabrina Nielebock and Dagmar Maier, who dealt with the unimaginably diverse range of practical tasks involved in planning a DAGM symposium. Thanks to Richard van de Stadt for providing excellent software and support for handling the reviewing process. A special thanks goes to Jeremy Hill, who …",Carl Edward Rasmussen and Heinrich H Bülthoff and Bernhard Schölkopf and Martin A Giese,1,1490087180750699139,,,,Springer Science & Business Media,"Pattern Recognition: 26th DAGM Symposium, August 30-September 1, 2004, Proceedings",http://books.google.com/books?hl=en&lr=&id=g_UWvE55-LcC&oi=fnd&pg=PA1&dq=info:gzoCZWXarRQJ:scholar.google.com&ots=RqYTF7mVIY&sig=wGk8ljmQ2jt4tuJllBN2G1Y0Bbo,3175,2004,/scholar?cites=1490087180750699139,DZ-fHPgAAAAJ:c_xDhezhKKUC
14800,"Interactive Images are a natural extension of three recent developments: digital photography, interactive web pages, and browsable video. An interactive image is a multi-dimensional image, displayed two dimensions at a time (like a standard digital image), but with which a user can interact to browse through the other dimensions. One might consider a standard video sequence viewed with a video player as a simple interactive image with time as the third dimension.",Kentaro Toyama and Bernhard Schoelkopf,1,14005160271487966103,Microsoft Research MSR-TR-2003-64,,,,Interactive Images,https://www.researchgate.net/profile/Kentaro_Toyama/publication/228832330_Interactive_Images/links/5411df3f0cf2788c4b354e82.pdf,,2003,/scholar?cites=14005160271487966103,DZ-fHPgAAAAJ:rO6llkc54NcC
14801,"The talk will start with a short tutorial on kernel methods in machine learning. Following this, we will describe how some recent methods for nonlinear dimensionality reduction can be viewed as kernel methods.",Bernard Schölkopf,1,5570775225576907084,,,13-14,,Kernel methods and dimensionality reduction,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792650,,2003,/scholar?cites=5570775225576907084,DZ-fHPgAAAAJ:0Kh4an1R61UC
14802,"We attempt to reach a better understanding of classication in humans using both psychophysical and machine learning techniques. In our psychophysical paradigm the stimuli presented to the human subjects are modied using machine learning algorithms according to their responses. Frontal views of human faces taken from a processed version of the MPI face database are employed for a gender classication task. The processing assures that all heads have same mean intensity, same pixel-surface area and are centered. This processing stage is followed by a smoothing of the database in order to eliminate, as much as possible, scanning artifacts. Principal Component Analysis is used to obtain a low-dimensional representation of the faces in the database. A subject is asked to classify the faces and experimental parameters such as class (ie female/male), condence ratings and reaction times are recorded. A mean classication error of 14.5 is measured and, on average, 0.5 males are classied as females and 21.3 females as males. The mean reaction time for the correctly classied faces is 1229+-252 [ms] whereas the incorrectly classied faces have a mean reaction time of 1769+-304 [ms] showing that the reaction times increase with the subject's classi-cation error. Reaction times are also shown to decrease with increasing condence, both for the correct and incorrect classications. Classication errors, reaction times and condence ratings are then correlated to concepts of machine learning such as separating hyperplane obtained when considering Support Vector Machines, Relevance Vector Machines, boosted Prototype and K-means Learners …",ArnULF BA GrAF and FA Wichmann and HH Bülthoff and B Schölkopf,1,15494303624956872858,,,149,Knirsch,Study of human classification using psychophysics and machine learning,https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792526,,2003,/scholar?cites=15494303624956872858,DZ-fHPgAAAAJ:PffFD9Wx8kQC
14803,"Author: Schölkopf, B et al.; Genre: Proceedings; Published in Print: 2003; Title:
Learning theory and Kernel machines: 16th Annual Conference on Learning
Theory and 7th Kernel Workshop (COLT/Kernel 2003).
",B Schölkopf and MK Warmuth,1,5987808075406143333,,,,Springer,Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop (COLT/Kernel 2003),https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1792233,,2003,/scholar?cites=5987808075406143333,DZ-fHPgAAAAJ:-aZU7N-im6MC
14804,"This chapter contains sections titled: Tricks for Constructing Kernels, String Kernels, Locality-Improved Kernels, Natural Kernels, Summary, Problems",Bernhard Schölkopf and Alexander J Smola,1,3635726041019715809,,,407-426,MIT Press,Designing kernels,https://ieeexplore.ieee.org/abstract/document/6282673/,,2001,/scholar?cites=3635726041019715809,DZ-fHPgAAAAJ:hxOKpwOdqr8C
14805,"This chapter contains sections titled: Data Representation and Similarity, A Simple Pattern Recognition Algorithm, Some Insights From Statistical Learning Theory, Hyperplane Classifiers, Support Vector Classification, Support Vector Regression, Kernel Principal Component Analysis, Empirical Results and Implementations",Bernhard Schölkopf and Alexander J Smola,1,13293634979829693516,,,1-22,MIT Press,A Tutorial Introduction,https://ieeexplore.ieee.org/abstract/document/6282677/,,2001,/scholar?cites=13293634979829693516,DZ-fHPgAAAAJ:Y0pCZpjrPW0C
14806,"This chapter contains sections titled: Linear Regression with Insensitive Loss function, Dual Problems, -SV Regression, Convex Combinations and 1-Norms, Parametric Insensitivity Models, Applications, Summary, Problems",Bernhard Schölkopf and Alexander J Smola,1,17645539756235968951,,,,MIT Press,Regression Estimation,https://ieeexplore.ieee.org/abstract/document/6282660/,,2001,/scholar?cites=17645539756235968951,DZ-fHPgAAAAJ:NUbW2F9HgtoC
14807,"• Data: input-output pairs (xi, yi)∈ R× R• Regularity:(x1, y1),...(xm, ym) drawn from P (x, y)• Learning: choose a function f: R→ R such that the error, averaged over P, is minimized.• Problem: P is unknown, so the average cannot be computed—need an “induction principle”",Bernhard Schölkopf and Statistical Learning,1,15965255995401024881,,,405-406,MIT Press,Kernel Methods,http://mlss11.bordeaux.inria.fr/docs/MLSS11Bordeaux_Scholkopf.pdf,,2001,/scholar?cites=15965255995401024881,DZ-fHPgAAAAJ:YMqph-EB6eoC
14808,"For IEEE to continue sending you helpful information on our products and services, please consent 
to our updated Privacy Policy … I have read and accepted the IEEE Privacy Policy … A 
not-for-profit organization, IEEE is the world's largest technical professional organization dedicated 
to advancing technology for the benefit of humanity. © Copyright 2019 IEEE - All rights 
reserved. Use of this web site signifies your agreement to the terms and conditions.  ",Alexander J Smola and Peter Bartlett and Bernhard Schölkopf and Dale Schuurmans,1,14755888453136559788,,,133-133,MIT Press,Kernel machines,https://ieeexplore.ieee.org/abstract/document/6274974/,,2000,/scholar?cites=14755888453136559788,DZ-fHPgAAAAJ:2Kfv4wVwXM4C
